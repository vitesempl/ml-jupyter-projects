[ЗАСТАВКА] Добрый день! Меня зовут Воронцов Константин. Я профессор кафедры «Интеллектуальные системы» Московского физико-технического института. Этот модуль будет посвящен вероятностному тематическому моделированию – направление, которое развивается в машинном обучении очень активно последние 15 лет. Постоянно появляются новые постановки задач, новые прикладные задачи и новые методы. В этом видео мы начнем с целей и задач тематического моделирования. Это область статистического анализа текстов, и методы призваны ответить на вопрос: чему посвящена большая коллекция текстовых документов? Какие в ней есть темы? И для того чтобы как-то формализовать это, строить какие-то методы, прежде всего необходимо ответить на вопрос: а что такое тема? Интуитивно мы понимаем под темой какую-то специальную терминологию определенной предметной области. Если более точно, то это набор слов или словосочетаний (мы будем их для общности называть «терминами»), которые совместно часто встречаются в документах. Интуиция здесь такая: мы с вами легко отличаем документ по математике от документа по биологии, даже если не разбираемся в этих предметных областях, но термины для нас узнаваемые. Более того, даже если мы случайным образом переставим слова или термины в документе, мы все равно по-прежнему сможем понять он из математики или из биологии по тому, какие слова там встречаются. Ну, если формализовывать эти соображения, переходя к языку теории вероятности и статистики, мы конечно же будем говорить о частотах встречаемости слов во всей коллекции, ну и в каждой теме. И каждая тема характеризуется своим словарем и своими частотами слов. Каждый документ мы будем считать, что тоже может относиться не обязательно к одной теме, а к нескольким темам, как говорят – вероятностной смеси тем. И вот эти вот соображения мы далее формализуем. А тематическая модель должна автоматически выявлять темы, которые находятся в документах. Темы называются латентными, потому что мы их не видим, мы только можем предполагать о том, что автор, когда писал тот или иной документ, думал о той или иной теме, но мы видим только слова, которые появились из этих тем. Давайте немножечко поговорим о целях тематического моделирования. Это хороший статистический инструмент для автоматического анализа текстов, который позволяет решать многие задачи. Ну, например, задачи жанровой классификации документов или категоризации документов. Когда у нас есть поток текстов или большая коллекция документов, нам их нужно разложить по папкам, по тематикам, и конечно же нам могут помогать частоты слов, специальных терминов, и обычно так и делается, но тематическое моделирование отличается вот этой вот особенностью, что оно позволяет документу относиться одновременно ко многим темам, то есть относить его сразу к нескольким рубрикам. Еще класс задач связан с аннотированием документов, когда по документу нужно выдернуть наиболее важные фразы и составить краткое резуме или реферат. Эта работа тоже должна делаться автоматически, а тематическое моделирование может помочь тем, что, выделив тематику документа, к каким темам он относится, дальше можно выделить наиболее тематичные фразы в этих документах и сделать такую выборку фраз из документов, чтобы они покрывали все те же темы, которые имеются в этом документе. Задачи суммаризации очень похожи, но отличаются тем, что вместо того чтобы делать краткое резюме одного документа, здесь мы делаем уже саммари большого числа документов, для какой-то части коллекции, для какой-то рубрики и так далее. Есть еще задача тематической сегментации и здесь тоже помогает тематическое моделирование. Сегментация, например, нужна для того, когда мы занимаемся поиском и хотим найти, а какой именно фрагмент длинного документа был посвящен той или иной теме и здесь нам, конечно, нужно длинный документ уметь разбивать на тематические однородные фрагменты или сегменты. Во всех этих задачах есть общая идея решения, что вот это самое распределение тем в каждом документе, оно становится фактически признаковым описанием этого документа, сжатым признаковым описанием. Часто в анализе текстов используют модели, векторные модели, когда каждое слово, каждый термин дает свой признак и частота этого слова есть признак, но здесь беда в том, что признаков получается слишком много – столько, сколько терминов в словаре, а тематическая модель позволяет перейти к сжатому признаковому описанию, например, можно задать 100 тем или 500 тем и иметь вот такие вот фиксированные размерности признаков описания документов. Следующий большой пункт задач связан с новыми видами информационного поиска, которые нужны людям, особенно людям, которые постоянно занимаются самообразованием, пополняют свои знания. Им нужны инструменты, которые позволят искать новое знание в новых предметных областях и быстро разбираться с той или иной новой предметной областью. Как структурировать эти общие представления о том, как область устроена, как она делится на подтемы? Вот для этих целей тоже очень хорошо использовать тематическое моделирование. О таком виде поиска говорят, что это разведочный поиск или семантический поиск, название пока что еще не устоялось, потому что это новые задачи. И здесь очень важно визуализировать тематическую структуру коллекции. Важно представлять как темы развивались во времени, если к каждому документу привязана метка времени. Для пользователей таких инструментов поиска очень важно иметь средства мониторинга новых поступлений документов в электронную коллекцию, чтобы не приходилось искать тематически важные документы каждый раз заново, а чтобы они автоматически приходили как только где-то они появляются в Интернете или в какой-то внешней электронной библиотеке. И, конечно, рекомендательные сервисы для тех пользователей, которые пользуются вот такими поисковиками, большими электронными библиотеками. Если хранить информацию о том, какие документы читал/ скачивал каждый пользователь, какие пользователи на него похожи, то мы уже можем строить рекомендательную систему, где завязаны сразу многие сущности: и документы, и слова, и пользователи. И слова распространяют смысл и семантику, и в документах она проявляется, и в пользователях тоже. Это все возможности строить рекомендательные системы для рекомендаций текстового контента. Ну и, если кратко перечислить приложения, то это, конечно, поиск научной информации, поиск в потентных базах, поиск в том огромном объеме статей и публикаций, которые пополняются ежегодно по разным оценкам примерно на 10 миллионов документов. Дальше, это автоматизация деятельности экспертных советов, когда поступает огромное количество заявок на инновационные проекты, на научные гранты, необходимо быстро подобрать экспертов и распределить пришедшую коллекцию документов на экспертизы. Агрегирование новостных потоков – это задача, с которой сталкиваются многие компании, которые занимаются и социологическими исследованиями; и аналитические компании; и информационные агентства. Необходимо приходящий поток новостей из разных источников разбирать как-то по темам, убирать дубликаты и отслеживать каждую тему во времени. Чтобы автоматизировать эту деятельность и иметь возможно делать это меньшим количеством сотрудников, тоже вот и средства тематического поиска и визуализации и моделирования – они очень хорошо подходят. Есть приложения, их появляется все больше и больше, которые связаны не с текстовой аналитикой, а с тем, что примерно такие же методы можно применять и для совершенно другого типа данных, например аннотирование и поиск изображений. Если у нас имеются уже процедуры, которые умеют выделять на изображениях те или иные элементы, мы можем считать изображения такими документами, в которых вот эти вот элементы, они либо есть, либо нет, либо они повторяются там какое-то число раз, то есть, это своего рода слова, поэтому элементы изображения становятся для нас такого рода словами, и если изображение сопровождается текстом, то у нас появляется такое вот представление изображения с одной стороны своими графическими элементами, с другой стороны – текстами. Тексты могут распространять семантику и на элементы изображения и на изображение. И построение тематических моделей внутри таких вот многомодальных коллекций позволяет выполнять многие интересные функции: искать тексты по изображениям, изображения по текстам, аннотировать изображения и так далее. Сейчас это распространяется и на видеопоследовательности и на другие задачи, в которых, ну вот как в задачах биоинформатики, вроде бы там тоже тексты, это ДНК и белки, они представимы в виде символьных последовательностей, однако появляются все больше и больше задачи, где исходный сигнал был вещественный, он как-то дискретизировался, выявляются какие-то события Типа того, что значение сигнала превосходит какой-то порог, значит ненормально. И после дискретизации исходный вещественный сигнал опять-таки может быть представлен в виде символьной последовательности и проанализирован вот методами вычислительной лингвистики, машинного обучения, тематического моделирования. Такие приложения появляются всё чаще и чаще. Примеры того, что же всё-таки такое темы, как они выглядят. Вот здесь представлен пример решения задачи, когда тематизировалась «Википедия», причём модель строилась сразу и на русской, и на английской «Википедии». И вот видно, что модель сама, без помощи человека, эксперта смогла собрать темы на обоих языках. И они оказались, во-первых, интерпретируемые (вот есть темы про университеты, темы про футбол), а во-вторых, видно, что и русские, и английские слова, они примерно соответствуют друг другу. Строилось здесь 400 тем. Но вдруг я подобрал две хорошие темы? Нет, они все хорошие. Более того, когда эту модель оценил эксперт, оказалось, что плохих тем всего было 4 из 400. То есть на таких достаточно больших коллекциях тематическая модель способна сама вытянуть вполне интерпретируемые темы. Ещё один пример. Оказывается, что интерпретируемость тем, понятность для человека резко возрастает, если в качестве терминов использовать не отдельные слова, а словосочетания. Например, мы строили тематическую модель для коллекции научных статей. Это статьи конференций «Математические методы распознавания образов» и «Интеллектуализация обработки информации» на русском языке. Так вот, если темы состояли из отдельных слов, понять тему можно, она интерпретируема. Но понять, что это была за научная школа, какие авторы, глядя просто на список слов в теме, не получается. А вот если строить биграммную модель, где входами словаря являются пары слов, то там оказывается, что смотришь на тему и, уже разбираясь в тематике конференции, можно даже догадаться, какие авторы стояли за этой темой. Ещё один пример построения тематической модели, теперь уже в области социология — это поддержка социологических исследований в области межэтничных отношений. Задача заключалась в том, чтобы выделить во всём потоке текстов из социальной сети только те тексты, которые относятся к обсуждению межнациональных, межэтнических отношений. И это уже задача фактически разведочного поиска, когда запросом выступают вообще все слова, которые так или иначе называют какие-то национальности. Такие слова называются этнонимами. И их было создано, заведено заранее несколько сотен, здесь потрудились специалисты. Но дальше задача тематического моделирования заключалась в том, чтобы выделить в коллекции все тексты, которые относятся вот к обсуждению подобного рода тем. И поделить вот этот выделенный контент (его не так уж много в социальной сети, меньше 1 %), а вот его нужно тщательно оттематизировать и понять, какие есть темы. Ну вот примеры тем, и опять-таки без всякого участия экспертов, без ручной разметки, а только вот большим словарём этничных слов, которые были заданы заранее, удалось выявить вот такие вот темы. Например, про русских видно, что тема либо связана с историей России, либо она связана с текущей политикой. И вот про сирийцев тема связана с войной в Сирии, тема про таджиков и узбеков связана с трудовой миграцией. А вот про канадцев в нашей социальной сети обсуждают только то, как они играют в хоккей. А про норвежцев обсуждают в основном проблематику ювенальной юстиции. А про китайцев в основном производство и деловые отношения. Ну вот такие примеры построенных тематических моделей, которые показывают, что выявленная тематика позволяет лучше понимать структуру коллекции. Это средство анализа больших текстовых коллекций. И главная задача тематической модели — приняв на входе коллекции текстов, на выходе мы должны получить распределение слов в каждой теме и распределение тем в каждом документе. Существует огромное количество, большое разнообразие разных моделей тематических, которые позволяют учитывать дополнительные данные, внешнелингвистические ресурсы, учитывают разную дополнительную информацию, которая кроме текста может быть. И некоторых из них мы коснёмся в следующих видео.

[ЗАСТАВКА] В этом видео мы поговорим о формальной постановке задачи тематического моделирования и рассмотрим, что такое вероятностная порождающая модель текста. Сначала, конечно же, прежде чем применять формальные методы, тексты необходимо подготовить, и процедуры подготовки текстов очень разные и зависят от того, откуда к нам этот текст пришел. Например, если коллекция была получена из PDF файлов, то там могут находиться всякие артефакты, там могут оставаться какие-то участки форматирования, там могут оставаться переносы слов, и слова будут разрываться в этих местах, могут оставаться колонтитулы, остатки графиков, таблиц и так далее, где будут какие-то непонятные сочетания символов. Значит, все это должно вычищаться из исходных текстов. Если тексты приходят к нам из социальной сети, там возникает другая проблема, или скажем из twitter, это проблема очень коротких сообщений, и применяются разные методики для того, чтобы сливать короткие тексты в один более длинный, более удобный для тематического моделирования, и это, конечно, зависит от цели исследования. Например, мы могли бы сливать все тексты, все посты данного автора за данный день или вообще все посты данного автора независимо от периода времени, ну и так далее. Методики применяются разные, для того чтобы работать с слишком короткими текстами. Ну, допустим, все эти барьеры мы прошли и теперь мы имеем текст как последовательность слов, без всякой грязи, но все равно у нас остаются проблемы с словоформами. И мы должны применить какие-то средства, стемминг или лемматизацию. Стемминг лучше подходит для английских текстов, лемматизация — для русских. Но и это еще не все. Потому что если мы хотим работать, например, с научными текстами, где есть хорошо устоявшаяся терминология, но, как правило, это не отдельные слова, а словосочетания, мы должны там еще сделать выделение терминов, и для этого тоже есть отдельные методы. Некоторые из них реализуются отдельно от тематического моделирования — это методы предобработки, а некоторые можно прямо встроить в тематические модели, но о таких сложных моделях мы с вами говорить не будем. Ну и наконец, удаляются слишком частые слова, так называемые стоп-слова, которые есть везде, и, очевидно, они не помогают нам определять тематику текста, а так же слишком редкие слова, чтобы просто сократить словарь и получить какие-то выигрыши от сокращения словаря и увеличения скорости обработки данных. Слишком редкие слова, их трудно относить к тем или иным темам просто потому, что тема — это некое статистическое явление, это набор слов, которые совместно часто встречаются в определенных текстах, ну если слово вообще встречается редко, то ни о какой статистике здесь речи быть не может, и, как правило, слова, которые встретились реже примерно десяти раз, просто удаляются из коллекции. Базовые вероятностные предположения. Мы предполагаем, что текст — это последовательность слов, которые генерируются из некоторого вероятностного распределения. Причем базовые тематические модели, самые простые, основываются на предположении о мешке слов, что мы можем произвольным образом переставить местами слова в документе, тем не менее мы сможем определить его тематику. Под словами здесь имеются в виду, конечно, и словосочетания, если мы смогли их выделить. Еще предположение, что употребление каждого слова в каждом документе связано с некоторой темой, на самом деле коллекция документов — это последовательность таких вот троек (документ, слово и тема), но в этой последовательности мы темы не видим, они для нас скрытые, латентные. И мы как раз используем тематическую модель, для того чтобы попытаться угадать: а к какой теме относилось каждое слово в каждом документе? Ну и наконец, последнее нетривиальное вероятностное допущение, так называемая гипотеза условной независимости, которая предполагает, что слова в документах генерируются именно темой, а не самим документом. То есть без этого предположения у нас бы не получились те легко оцениваемые тематические модели, которые мы будем дальше строить. Иногда еще вводят предположение разреженности, что каждый документ состоит из небольшого числа тем, а каждая тема состоит из относительно небольшого числа слов — некое лексическое ядро темы должно быть, которое существенно отличает эту тему от остальных. И вот таких вот слов ядерных, их, наверное, несколько десятков или несколько сотен, но это не существенная доля всего словаря. Вероятностная порождающая модель текста основана вот на таком вот двухэтапном процессе порождения. Для каждых слово-позиций мы сначала определяем, к какой теме будет относиться это слово из распределения тем в документе. Вот условное распределение тем в документе — это важный параметр модели, и нам его надо будет оценивать. Но как только для слово-позиции определилась тема, дальше мы берем распределение слов в данной теме и уже из него генерируем конкретное слово и записываем в данную слово-позицию. Вот так вот слово за словом у нас появляется текст. Ну и напоминаю, что, конечно же, здесь гипотеза «мешка слов» работает, то есть таким образом сгенерированный текст, где каждая слово-позиция появилась независимо от предыдущих, конечно же, не будет осмысленным текстом, он не будет читаться человеком, но можно говорить о том, что с точностью до произвольной перестановки слов этот текст вполне мог бы нести в себе какую-то тематику, а именно выявлением тематики мы и занимаемся. То есть тематическое моделирование, вообще говоря, не претендует на понимание компьютером смысла текста, а претендует всего лишь на то, чтобы сделать вот такую вот кластеризацию текста — понять, какие темы в нем есть, и дальше искать тексты по близости тематической, по темам. Итак, формальная постановка задачи. Нам дана коллекция документов. Что это означает? Зафиксирован словарь, состав терминов, слов или словосочетаний, из элементов этого словаря складываются документы, мы знаем длину каждого документа и знаем сколько раз каждый термин встретился в каждом документе — это наши исходные данные. Что мы хотим найти? Мы хотим найти представление условной вероятности появления слов в документе в виде вероятностной тематической модели. В виде вот такой вот суммы. Фактически это сумма условных... произведение условных вероятностей есть не что иное, как формула полной вероятности, и мы хотим в итоге определить вот такие вот семейства распределения для каждой темы вероятности терминов и для каждого документа вероятности тем — это условное распределение. Вот в этом и заключается вероятностная порождающая модель. Порождающая модель описывает, как по вот этим распределениям φ и θ получить саму коллекцию, а мы собираемся решать обратную задачу — мы по наблюдаемой коллекции, по словам, хотим понять, а какими распределениями φ и θ она могла бы быть получена. Фактически эту задачу можно также трактовать и как задачу матричного разложения, потому что условное распределение φ можно записать в виде матрицы, где каждый столбец соответствует теме, и в каждом столбце находятся неотрицательные и нормированные значения, то есть суммы значений по каждому столбцу равны единице, потому что столбец — это дискретное распределение вероятностей. Ну и точно так же вторая матрица θ, где каждый столбец — это дискретное распределение вероятностей на множестве тем для каждого документа, значит тоже есть ограничение неотрицательности и нормировки на каждый столбец. А произведения этих двух матриц φ и θ как раз должны нам давать частотные оценки условных вероятностей слов в документах. И то, что эти матрицы дают, это мы можем легко посчитать, потому что эти частотные оценки вычисляются по имеющимся у нас частотам терминов в документах и по длинам документов. То есть, таким образом, матрица слева нам дана, известна, а нужно найти те две матрицы справа — φ и θ. И, собственно, это и есть задача схематического моделирования, понимаемая, как задача матричного разложения, или еще говорят стохастического матричного разложения. Такие матрицы называются стохастическими, потому что их столбцы — это дискретные вероятностные распределения. Отнюдь не потому, что их элементы как-то там случайно генерируются. Итак, чтобы решать такую вот задачу, мы можем воспользоваться принципом максимума правдоподобия. У нас есть вероятностная модель порождения данных — это условные вероятности слов в документах. У нас есть сами данные — это наблюдаемые частоты слов в документах, и мы хотим построить такую модель, которая хорошо описывает наблюдаемые частоты. Пользуемся принципом максимума правдоподобия, получаем вот такую вот оптимизационную задачу максимизации некоторого функционала, но при ограничениях типа равенства, потому что каждый столбец нормирован (и в матрице φ, и в матрице θ), и ограничения неравенства, потому что все эти элементы (φ и θ) — это условные вероятности, они, конечно же, не отрицательны. Интересная особенность этой задачи, задачи матричного разложения, заключается в том, что ее решение не единственное, и это создает, с одной стороны, некоторую проблему, потому что если мы нашли какое-то решение φ и θ, мы можем взять произвольную, (не совсем произвольную, но в широком множестве) матрицу S размера темы на темы, умножить φ справа на нее, а θ умножить слева на обратную, и мы получим ровно то же самое решение, но матрицы φ и θ останутся другими теперь у нас, и получается, что слишком много решений, бесконечное множество решений. И любой метод, который итерационный, из какого-то начального приближения будет искать нам матрицы φ и θ, но при изменении этого начального приближения он найдет нам другие φ и θ. Это, с одной стороны, не хорошо, получается неустойчивость, невоспроизводимость той математической модели, которую мы получаем, но, с другой стороны, это и хорошо, потому что появляется свобода выбора каких-то дополнительных ограничений, которые можно наложить на матрицы φ и θ, и такие ограничения принято называть в теории некорректно поставленных задач (задач, у которых не единственное решение), ну вот их принято называть регуляризаторами. Это некоторый функционал, который мы должны придумать сами, исходя из тех или иных содержательных соображений данной задачи, и он просто добавляется к тому функционалу, логарифму правдоподобия, который мы до сих пор максимизировали. И вот оказывается, что с этой добавочкой задача тоже неплохо решается, довольно просто, и открывается огромный спектр возможностей строить тематические модели с заданными свойствами, задавая те или иные регуляризаторы. Итак, краткое резюме. Во-первых, вероятностная тематическая модель — это порождающая модель или модель порождения коллекции текстов, которая описывает каждый документ вероятностной смесью тем, а каждую тему описывает распределением на множестве терминов. При этом задействуется два основных вероятностных допущения. Во-первых, гипотеза «мешка слов»: что слова в документе можно произвольным образом переставить и для наших приложений ничего не изменится. Второе, это гипотеза условной независимости: что слова в документе генерируются независимо от документа и генерируются они из темы. Ну и наконец, на эту задачу можно смотреть с нескольких углов зрения. Во-первых, это такая «мягкая» двойная кластеризация или, как говорят, би-кластеризация и документов, и терминов. Кластерами здесь являются темы, и мы одновременно и документы разделяем по кластерам, и темы разносим по кластерам, но кластеризация «мягкая», потому что документ может одновременно относиться ко многим кластерам, и то же самое относится и к термину. С другой стороны, с другой точки зрения, это задача стохастического матричного разложения, что дает понимание, что это некорректно поставленная задача, ее решение не единственное, и поэтому нужно использовать всякого рода регуляризаторы, о которых мы поговорим в следующем видео.

[ЗАСТАВКА] В этом видео мы с вами разберемся, как строятся вероятностные тематические модели и как устроен основной алгоритм для их построения — EM-алгоритм. Напомню, что в конце предыдущего видео мы пришли к постановке задачи, которая по наблюдающимся словам в документах строит тематическую модель. Постановка заключалась в максимизации логарифма правдоподобия, к которому добавлен некоторый регуляризатор R — произвольная функция. Но для того чтобы эту задачу было удобно решать, от этой функции требуется только то, чтобы она была гладкой, тогда можно к этой задаче применять условие Каруша — Куна — Таккера. Естественно, выкладки я сейчас опускаю и просто показываю результат. Это не решение, выраженное в аналитическом виде, к сожалению, это всего лишь система уравнений, которую придется решать, но решать ее достаточно удобно очень простым численным методом, который называется метод простых итераций. И он же по совместительству здесь у нас оказывается EM-алгоритмом. Это система уравнений, которая состоит из двух типов уравнений. Первое, это так называемый E-шаг EM-алгоритма — это вычисление условных распределений тем для каждого слова в каждом документе. И то, что записано в первой строке этой системы уравнений, по сути дела, не что иное, как просто формула Байеса, по которой мы можем выразить вот эти наши вспомогательные переменные P, t, d, w в этой системе через параметры модели Φ и θ. А вот формулы M-шага, они, наоборот, они выражают основные параметры модели Φ и θ через вспомогательные переменные P, t, d, w. И если откинуть регуляризатор, считать, что R = 0, то окажется, что вот те формулы, которые здесь записаны — это просто частотные оценки условных вероятностей слов в темах и тем в документах. То есть если бы мы знали тему каждого слова, мы могли бы просто написать, очень легко посчитать частотные оценки вероятностей условных слов в темах и тем в документах. Но поскольку мы не можем знать, на самом деле, тему каждого слова в каждом документе, мы ее оцениваем вот так вот по фактически формуле Байеса и получаем итерационный процесс, где основные вспомогательные переменные оцениваются друг по другу поочередно. Но и обращаю внимание на то, что оператор нормировки здесь переводит произвольный вектор xt в вектор, координаты которого неотрицательны и нормированы, то есть сумма их в точности равна 1. То есть любой произвольный вещественный вектор переводится в вектор той же размерности, который может играть роль дискретного распределения. И вот через эту операцию нормирования векторов очень удобно оказалось записать вот все уравнения этой системы уравнений. Есть два очень известных частных случая этой системы — ведь мы можем задавать регуляризатор в широких пределах. Так вот, если он совсем не задан — R = 0, то мы получаем очень известный метод, который в 1999 году был придуман и опубликован Томасом Хофманом. Метод называется «вероятностный латентный семантический анализ», или PLSA. Четыре года спустя появился следующий метод, который был предложен Дэвидом Блеем, Эндрю Энджи и Майклом Джорданом — три очень известных ученых. Их статья по латентному размещению Дирихле — это, наверное, самая цитируемая работа в тематическом моделировании. Здесь я не буду очень подробно рассказывать все те соображения, и почему именно распределение Дирихле здесь появляется, и все те соображения, которые были использованы авторами этого метода. Но с точки зрения регуляризации взгляд на этот метод довольно прост — это вот такой вот регуляризатор, в котором появляются параметры β и α. Авторы этого метода рассматривали его с точки зрения байесовского обучения, и там делалось два вероятностных предположения. Первое — что столбцы матриц Φ и θ могут быть не какими угодно, они порождаются из распределения Дирихле. Распределение Дирихле — это такое распределение, которое умеет порождать векторы заданной размерности, которые являются нормированными неотрицательными, то есть могут быть вероятностными распределениями. Вот такая вот двухступенчатая порождающая модель: сначала из Дирихле порождаются столбы матриц Φ и θ, а потом из них уже, как мы в предыдущих видео обсуждали, порождается текст. И второе предположение делается, что если мы воспользуемся принципом максимума апостериорной вероятности, то есть учтем эти априорные распределения в принципе максимума совместного правдоподобия данных и модели Φ и θ, которые тоже генерируются вероятностным образом, то вот тогда мы сможем получить новую постановку задачи, которую можно трактовать как регуляризованное логарифмированное правдоподобие. Вот интересны свойства распределения Дирихле — почему-то все-таки они именно оказались удобными в качестве априорных распределений. Дело в том, что распределение Дирихле позволяет генерировать как разреженные, так и такие вот сильно сглаженные дискретные распределения. Причем случаи разреженности, вот как показано на левом графике, нас как раз интересуют больше всего, потому что у нас есть гипотеза о том, что каждая тема состоит, в общем-то, из небольшого числа тем, слов, и каждая... каждый документ состоит из небольшого числа тем. То есть кажется, что вот как раз, когда параметры распределения Дирихле меньше 1, это самый интересный для нас случай, потому что распределения получаются разреженными. Ну и если параметры распределения Дирихле в точности 1, то это получается просто равномерное распределение, и в таком случае LDA просто переходит в PLSA. Интересен другой взгляд на латентное размещение Дирихле, даже более простой, но чтобы его рассказать, я должен ввести понятие дивергенции Кульбака — Лейблера — это вообще очень полезное понятие, и в машинном обучении, и в теории вероятности довольно часто встречается. Это способ померять расстояние между двумя распределениями. Поскольку нас интересует дискретное распределение, мы сейчас только вот такой случай рассматриваем, а вообще, в общем случае вместо суммы здесь должен стоять интеграл. Итак, дивергенция Кульбака — Лейблера определяется вот по такой несложной формуле. Основные ее свойства — это то, что это неотрицательное значение, и она равна 0 тогда и только тогда, когда распределение P и Q совпадают. Мера — несимметричная, она, строго говоря, поэтому не является функцией расстояния между распределениями P и Q в обычном смысле. Она меряет в некотором смысле степень вложенности распределения P в распределение Q. Другой способ посмотреть на это — это связать минимизацию дивергенции Кульбака — Лейблера с максимизацией правдоподобия. Оказывается, что это две эквивалентные вещи. Если P у нас — эмпирическое распределение, а Q — это какая-то параметрическая модель распределения с параметром α, то если мы используем принцип максимума правдоподобия, для того чтобы определить этот параметр так, чтобы наше эмпирическое распределение P как можно лучше соответствовало модели, то это все равно что минимизировать дивергенцию Кульбака — Лейблера между этими двумя распределениями. Вот такая вот интерпретация дает нам неплохую интуицию о смысле дивергенции и о том, как ее можно использовать, а мы ее будем использовать в роли регуляризаторов. Итак, другой взгляд на латентное размещение Дирихле, как я говорил, более простой, заключается в том, что если мы зададимся вот этими вот векторами — ну, рассмотрим матрицу Φ, тогда мы задаемся векторами β, это вектор над словарем слов. Так вот, те слова, которые соответствуют значениям β > 1 — для них условные вероятности слов в темах будут сглаживаться, они будут приближаться к значениям βw. А если βw было < 1, то, наоборот, будет происходить разреживание, и максимизация дивергенции Кульбака — Лейблера между двумя распределениями будет означать, что распределение Φt, то есть столбцы матрицы Φ, соответствующие теме t, они будут разреживаться, то есть в них будут появляться больше нулевых или почти нулевых элементов. Так вот, в отличие от байесовской точки зрения здесь нам не нужно ни вводить распределения Дирихле, ни говорить об априорных вероятностях — здесь достаточно сказать, что мы просто либо просто приближаем столбцы матриц Φ и θ каким-то заданным распределением, либо удаляем от них. И это вот приближение-удаление можно даже делать для разных координат — в столбце некоторые сглаживать, некоторые разреживать. Более того, появляется свобода задавать параметры β и α какими угодно, без ограничений, что они должны быть строго положительны — это ограничение шло из самого определения, что такое распределение Дирихле. Но вот в нашей такой интерпретации, более свободной, никаких ограничений на β и α не возникает, и можно, например, сильнее разреживать эти распределения, добиваясь того, чтобы модель была разреженной, то есть было как можно больше 0 в матрице Φ и θ, но при этом, конечно, эти матрицы Φ и θ чтобы хорошо описывали нашу коллекцию. Итак, от частных случаев перевернемся снова к общему случаю. Мы ведь получили некую систему уравнений и договорились, что мы будем решать ее методом простых итераций, то есть по вспомогательным переменным P, t, d, w — это условные вероятности тем для слов в документах — мы будем считать основные параметры модели Φ и θ, ну и, наоборот, по Φ и θ мы рассчитываем P, t, d, w. Так вот, можно, ничего не меняя в формулах вот этой системы уравнений, организовать вычислительный процесс метода простых итераций таким образом, чтобы он шел максимально быстро, причем именно на больших коллекциях текстовых документов. Дело в том, что матрицы Φ и θ находятся в неравноправном положении. Матрица Φ относится ко всей коллекции целиком, и каждый ее столбец относится ко всей коллекции целиком, а вот в матрице θ каждый столбец относится только к одному документу. Поэтому если мы будем итерировать таким образом, что только после просмотра всей коллекции мы будем обновлять матрицы Φ и θ, процесс будет работать крайне медленно. Потому что матрица Φ будет обновляться слишком редко. Мы можем делать по-другому: мы можем просматривать каждый документ, строить для него тематическую модель и обновлять матрицу Φ. Можем обрабатывать по несколько документов, образуя такие вот порции или пакеты, батчи — по-разному их называют. И после вычисления модели для нескольких документов обновлять матрицу Φ. Вот оказалось, что такой процесс самый быстрый, и появились онлайновые алгоритмы. Они появились не сразу, а примерно лет 5 назад, и они сразу сделали тематическое моделирование очень эффективным инструментом анализа больших коллекций текстов. Оказалось, что матрица Φ может сходиться даже до того, как мы просмотрели всю коллекцию. Ну, например, коллекция — огромна, избыточна, содержит миллионы документов, а оказывается, что после просмотра нескольких первых десятков тысяч, мы уже получаем более или менее хорошую, устоявшуюся матрицу Φ — она уже сошлась. И нам остается только тематизировать остальные документы. Вот в таком режиме работы мы можем прогнать EM-алгоритм через всю коллекцию, сделав фактически только одну итерацию. Приходится, правда, делать несколько итераций для каждого документа. Вот такой вот алгоритм — он называется онлайновым алгоритмом — используется практически во всех основных пакетах по тематическому моделированию, которые сегодня существуют даже в открытом коде и рекомендуются для обработки больших коллекций. Итак, краткое резюме. EM-алгоритм — это основной инструмент в тематическом моделировании. Онлайновый алгоритм, как мы сейчас видели, позволяет вообще делать обработку большой коллекции за один проход. И вот тот подход, который основан на использовании регуляризаторов, хорош тем, что можно единообразно, с помощью одного и того же EM-алгоритма, вот этого онлайнового, строить огромное разнообразие тематических моделей, не только LDA и PLSA, а менять регуляризаторы, добавлять новые регуляризаторы, учитывать все новые и новые требования к модели. Вот такой вот подход оказался очень продуктивным, простым, универсальным, гибким. И в следующем видео я расскажу еще немножко больше о том, какие конкретно регуляризаторы, кроме стандартных, как в LD, можно использовать.

[БЕЗ_ЗВУКА] В предыдущих видео мы поговорили о формальной постановке задачи тематического моделирования и увидели, как эта задача решается с помощью итерационного процесса, который называется EM-алгоритмом. В этой постановке задачи у нас была большая свобода выбора дополнительных критериев, которые называются регуляризаторами. На самом деле, запас неединственности решения основной задачи матричного разложения настолько большой, что мы можем использовать одновременно много регуляризаторов. Каждый из них должен выражать то или иное наше дополнительное пожелание к тематической модели или учитывать какие-то дополнительные данные. И таких ограничений на модель можно наложить одновременно несколько. Это позволяет нам сделать подход, который называется аддитивной регуляризацией, то есть регуляризаторы складываются, каждый из них умножается на свой коэффициент регуляризации. Это, с одной стороны, дает огромную свободу действий — можно использовать сразу много регуляризаторов, но, с другой стороны, возникает техническая проблема: а как определять вот эти самые коэффициенты регуляризации? Ну, в общем-то, пока экспериментальным путем, добавляя регуляризаторы по одному и у каждого регуляризатора оптимизируя этот коэффициент, выбирая его в ходе нескольких пробных экспериментов, запусков модели. Давайте рассмотрим пример, когда используется несколько регуляризаторов для того, чтобы наделить тематическую модель нужными нам свойствами. Нам бы хотелось бы, чтобы получаемые темы были хорошо интерпретируемы, понятны людям. Оказывается, что хорошей интерпретируемости тем часто мешает наличие слов общей лексики, общеупотребительных слов, которые оказываются намешанными с довольно большими вероятностями во всех темах. Вот хочется их выделить в какие-то отдельные темы, и пусть они описывают вот такое вот фоновое распределение слов языка. Будем называть такие темы фоновыми, а, соответственно, все остальные темы, которые нас в основном и интересуют, будем называть предметными, потому что они описывают отдельные предметные области нашей текстовой коллекции. Так вот, нам хотелось бы, чтобы предметные темы были достаточно сильно разреженными, то есть чтобы у каждой такой темы было свое лексическое ядро, которое бы существенно отличало эту тему от остальных. То есть они должны быть не только разреженными, но и еще как можно более различными, или декоррелированными. Вот эти требования можно выразить с помощью регуляризаторов. Итак, мы хотели бы, чтобы предметные темы были разреженными, причем как столбцы матрицы Φ, так и соответствующие строки матрицы θ. То есть это предположение о том, что каждая тема состоит из небольшого числа слов и в каждом документе небольшое число предметных тем. А вот фоновые темы — нам бы хотелось, чтобы они были более сглаженными, чтобы у всех слов была существенная вероятность находиться в фоновом фоне, в фоновой теме, а документы тоже, конечно же, на существенный процент — и даже, скорее всего, больше половины — должны состоять из фоновых слов. Вот с этими мыслями мы можем записать те самые регуляризаторы, которые мы рассматривали в прошлом видео, когда говорили о методе латентного размещения Дирихле. Но тогда мы применяли этот регуляризатор ко всем темам, а идея здесь состоит в том, чтобы регуляризатор сглаживания применить только к фоновым темам и сказать, что вот фоновые распределения слов, они близки к заданному распределению, например к распределению всех слов в языке, которое можно заранее вычислить и здесь использовать. Но и распределение θ тем в документах тоже в тех местах, где у нас темы фоновые, вот, значит, это распределение должно быть сглаженным, то есть надо увеличивать значение θtd для этих тем. Интересно, что если мы просто в этой формуле поменяем множество фоновых тем на множество предметных и поменяем плюсы на минус, то мы тут же получим разреживающий регуляризатор для предметных тем. То есть это вот та самая свобода действий, которую нам дал отказ от априорных распределений Дирихле и взгляд на вот эти регуляризаторы просто как на максимизаторы или минимизаторы KL-дивергенции между столбцами матриц Φ и θ и вот заданными распределениями β и α. Но возникает, конечно, вопрос: откуда брать эти β и α? Их должен задавать эксперт — специалист, который занимается построением тематической модели. Как я уже сказал, β — это может быть распределение всех слов в документах, а α очень часто выбирают просто равномерным распределением, потому что мы не можем априори сказать, какие темы чаще или реже встречаются по документам коллекции. Но вот интересное обобщение этих двух регуляризаторов — сглаживающего и разреживающего — возникает в том случае, если мы скажем, что вот эти вот векторы β и α могут быть вообще свои для каждого столбца. Ну, казалось бы, это какое-то слишком большое... слишком большая свобода выбора параметров, управляющих построением модели, однако все становится ясно, когда мы разберемся в интерпретации и в цели введения такого регуляризатора. Вот оказывается, что если мы просто будем задавать эти распределения равномерными, но на подмножествах. Для каждой темы можно задать свои подмножества слов, которые типичны для этой темы, являются ключевыми словами для этой темы, и документов, которые являются важными в этой теме. Вот такую информацию могут задавать эксперты, которые занимаются построением тематической модели. Например, они сделали эксперимент, и оказалось, что не все темы чистые — в каких-то темах встречаются лишние слова, а нужные слова не встречаются. Вот такую информацию, поправку к модели, можно задать с помощью вот этих самых белых списков слов и документов в теме и черных списков — наоборот, тех слов, которых не должно быть в данной теме, или тех документов, которые ошибочно были отнесены моделью к данной теме. Вот это хороший способ внесения такой вот частичной обучающей информации — semi-supervised learning. То есть экспертам совершенно необязательно просмотреть все слова, все темы, все документы, но если они встречают в ходе работы с тематической моделью какие-то огрехи, то их легко ввести в модель с помощью вот такого регуляризатора, который учитывает эти самые белые и черные списки. И, наконец, мы добрались еще до одного требования — мы хотели бы, чтобы в каждой теме выделялось свое лексическое ядро и чтобы темы, как столбцы матрицы Φ, как можно меньше коррелировали друг с другом. Этого тоже можно добиться, если ввести вот такой вот ковариационный регуляризатор — взять попарно все столбцы матрицы Φ и потребовать, чтобы они были попарно далеки друг от друга. Вот такой регуляризатор, который тоже, как легко видеть, можно продифференцировать по Φ, подставить в формулу M-шага, и там получаются очень несложные вычисления. То есть вообще все рассмотренные сейчас регуляризаторы приводят к модификациям формул M-шага очень простым — там просто добавляются какие-то константы или добавляются какие-то очень легко вычислимые величины. И еще один регуляризатор, который может оказаться полезен на практике и который тоже основан на представлениях о дивергенции Кульбака-Лейблера — это регуляризатор, который разреживает распределение P(t). Что такое P(t)? Это вероятность тем во всей коллекции, то есть какая доля слов всей коллекции отнесена к теме t. Так вот, если мы скажем, чтобы максимизировалась KL-дивергенция между этим распределением и равномерным, это будет фактически означать, что мы хотим, чтобы как можно больше вероятностей P(t) приняли нулевые значения, то есть мы хотим, чтобы это распределение было как можно дальше от равномерного. Это происходит в том случае, если у нас там образуются нулевые значения, то есть каких-то тем вообще в коллекции нет, а это означает, что просто тема выводится, удаляется из коллекции, и это открывает возможности определения числа тем. Надо сначала взять число тем избыточным. Например, мы предполагаем, что тем где-то от 100 до 200 — возьмем 300, и начинаем темы потихонечку с помощью этого регуляризатора одна за другой уводить. Чем сильнее коэффициент регуляризации τ, тем быстрее этот регуляризатор будет удалять вот такие вот мелкие, ненужные темы. Но интересным побочным эффектом этого регуляризатора оказалось то, что он удаляет также линейнозависимые темы, расщепленные темы, то есть если представить, что тема образовалась путем разделения слов по двум темам и что это избыточно. Вот если это так, то одна из этих тем будет уведена из модели, а вторая вберет в себя слова обеих тем. Вот это вот очень интересный эффект, который оказался нетривиальным и до сих пор теоретического обоснования не имеет, но хорошо работает на практике. Итак, резюмируем. Подход, который основан на аддитивной регуляризации, то есть на добавлении еще и еще дополнительных критериев для построения тематической модели (они образуют такую взвешенную комбинацию критериев, которые добавляются к логарифму правдоподобия), вот этот подход, он очень прост, очень продуктивен и позволяет использовать все тот же самый EM-алгоритм (можно использовать онлайновый EM-алгоритм), в котором просто подставляются еще члены в регуляризатор. И мы увидели, что вот такое вот комбинирование, разреживание, сглаживание и декоррелирование способно повышать интерпретируемость тем, добиваться того, чтобы стоп-слова и общеупотребительные слова выводились в отдельные темы, а оставшиеся предметные темы были как можно сильнее непохожи друг на друга.

[БЕЗ_ЗВУКА] В предыдущих видео мы ввели формальную постановку задачи вероятностного тематического моделирования; увидели, что в исходной постановке задача некорректно поставлена, у нее бесконечное множество решений; и ввели общий механизм, который позволяет получать регуляризованное решение — введение регуляризаторов. Можно дальше заниматься тем, что в разных прикладных задачах строить тематические модели с заданными свойствами, вводя те или иные специальные регуляризаторы, но есть еще одно важное обобщение, которое тоже очень часто применяется на практике при построении тематических моделей, а именно аппарат модальностей. Что такое модальности? В своем исходном виде тематическая модель берет на входе коллекцию документов и строит распределение тем в каждом документе и распределение слов в каждой теме. Ну, кстати, по формуле Байеса по этому распределению можно легко посчитать и тематику каждого слова, то есть определить условные вероятности тем в каждом слове. Но оказывается, что на практике встречаются коллекции документов, которые содержат не только слова. Например, каждому документу может быть приписана метка автора, метка времени или еще какая-то метаинформация, которая связывает данный документ не только со словами, а с элементами еще каких-то других конечных множеств. Вот такие конечные множества мы и будем называть модальностями. Какие еще модальности бывают? Ну, например, если в документе имеются изображения, то можно выделить на изображениях отдельные элементы и тоже считать их модальностями. То есть изображение — это такой мини-документ, который состоит вот из таких псевдослов — элементов изображений. Дальше, между документами могут быть ссылки. Это могут быть гиперссылки, если речь идет о документах в Интернете, или это могут быть цитаты, если речь идет, скажем, о научных статьях. В этом случае множество документов как раз становится модальностью, потому что если документы ссылаются друг на друга, то получается, что вот такими псевдословами в документе являются упоминания других документов, значит, множество всех документов — это еще одна модальность. Дальше, если мы говорим об интернет-рекламе, то на странице могут быть рекламные баннеры. Так вот, оказывается, что множество рекламных баннеров с точки зрения рекламной сети и системы рекомендации рекламы — это тоже конечное множество, значит, некоторая модальность. И мы можем считать, что все баннеры, которые появились на данной странице в ходе ее жизни, — это тоже такого вот сорта слова. А если пользователи еще и кликнули на эти баннеры, то это еще один очень важный тип информации, который тоже можно учесть при построении тематической модели. Дальше, пользователи, которые эти документы читают, комментируют, скачивают, лайкают, рейтингуют и вообще еще что там может быть с ними делают, они тоже становятся своего рода словами в этом документе, потому что если пользователь сделал какую-то операцию с документом, информационный след об этом действии остается в системе, и мы можем считать, что пользователь оставил свою метку, свой идентификатор в этом документе, то есть документ теперь состоит не только из слов, а еще и содержит в себе всякие разные другие сущности, в том числе и метки пользователей. И хотелось бы строить тематические модели, которые увязывают все вот эти типы информации, все модальности воедино и описывают появление элементов разных модальностей в документах тем, что документы имеют определенную тематику. То есть благодаря тому, что документ относится к тем или иным темам, в этом документе появляются определенные слова из этой темы, этот документ читают пользователи, которые любят эту тему, в этом документе появляются картинки, которые тоже про это тему, ну и так далее. То есть тематическая модель, которая описывает появление сразу всех модальностей, она имеет для документа по-прежнему один и тот же тематический профиль или тематику — столбец матрицы Θ, и вот этот столбец, он единый для всех остальных модальностей. Как формализовать вот эту вот постановку задачи? Как ее превратить в такую оптимизационную задачу, которую мы тоже могли бы решать EM-алгоритмом? Оказывается, сделать это очень просто. Надо для каждой модальности ввести свой отдельный словарь и описать распределение слов (точнее, токенов) вот этой модальности в каждой теме своим нормированным распределением. Можно говорить о том, что словарь всех модальностей — это просто объединение непересекающихся множеств словарей отдельных модальностей, и строить тематическую модель, в общем-то, по-прежнему так, как мы ее строили раньше, но просто теперь вероятностная модель как принцип максимума правдоподобия расписывается отдельно для каждой модальности. Замечу, что здесь у каждого документа, независимо от того, какие модальности в нем содержатся, тематика, условное распределение темы в этом документе или столбец матрицы Θ — он единый, общий для всех модальностей. А вот столбец матрицы Φ для каждой модальности свой, и для каждой модальности можно выписать свой принцип максимума правдоподобия. Получается та же ситуация, которая у нас была с регуляризаторами. Есть много разных критериев, которые нам хочется максимизировать одновременно. Что делать? Сложить их с некоторыми весами. И поэтому появляются веса модальности — τm мы их будем обозначать. И можем добавить еще какие-то регуляризаторы, которые нам позволят наделить модель нужными нам свойствами. Вот такая постановка задачи оказывается ненамного сложнее той, которую мы уже ранее рассматривали, и для нее легко выписывается модифицированные EM-алгоритм. Единственная модификация (точнее, две модификации) здесь заключаются в том, что, во-первых, частота слова в документе ndw домножается на вес соответствующей модальности этого слова. Это первая модификация. Вторая модификация: при вычислении матрицы Φ она нормируется для каждой модальности по отдельности. Других модификаций нет. То есть очень простое расширение модели, тематической модели, но зато какая свобода появляется в решении разнообразных прикладных задач, где бывает много разных модальностей, и документы, конечно, на практике состоят отнюдь не только из слов. Итак, резюмируя, во-первых, модальности — это очень мощный инструмент, но он легко добавляется в общую схему регуляризации. Каждая модальность задается своим словарем — конечным множеством элементов, или токенов, а документ рассматривается как некий универсальный контейнер, к котором содержатся токены разных модальностей. Вот модальности имеют много разных нетривиальных применений. В частности, мы уже поговорили о пользователях. Но вот если у нас имеется параллельная коллекция, то есть коллекция, где есть тексты и переводы этих же текстов на другие языки, то тогда модальностями становятся языки. И если мы их учитываем в одной общей тематической модели, то это открывает возможности делать, например, кросс-язычный поиск, мультиязычный поиск, то есть запрос задавать на одном языке, а ответы получать на другом. Например, по тексту русскоязычному научной статьи ищем в коллекции, большой коллекции англоязычных текстов то, что еще есть по этой тематике. Ну и n-граммы или словосочетания, если мы умеем выделять термины в текстах или просто все подряд биграммы, то появляется возможность вот эти самые биграммы задать в качестве второго словаря и рассматривать как вторую модальность, и тем самым стоить вот те самые хорошо интерпретируемые модели, примеры которых были показаны в первом видео, где было видно, насколько лучше интерпретируется тема, если она построена на словаре биграмм. Также можно учитывать время, авторов и другие модальности. А можно учитывать все их вместе, то есть строить модели, которые мало того что на биграммах и многоязычные, да еще и распределены во времени, и документам приписаны авторы, и все это сразу, потому что все это описывается единой системой уравнения и решается одним EM-алгоритмом.