[ЗАСТАВКА] Мы начинаем урок, посвященный визуализации данных. И давайте сначала обсудим, зачем вообще данные нужно визуализировать. Когда мы на прошлой неделе говорили про понижение размерности, мы обсуждали вот такой простой пример: представьте, что данные выглядят вот так, видно, что закономерности явно нелинейные, и вы хотите понизить размерность до одного признака. Если вы будете пытаться спроецировать эти данные, эту выборку, на прямую, то есть будете пытаться применить линейный подход к понижению размерности, то у вас ничего не получится. Какую прямую вы бы ни выбирали, вы будете терять довольно много информации. Поэтому, чтобы информацию сохранить, нужно проецировать выборку на красную кривую, которая явно нелинейная, и ее нужно подбирать, используя нелинейные методы понижения размерности. Вообще в машинном обучении мы имеем дело с высокоразмерными выборками, то есть с выборками, в которых очень много признаков. Но при этом мы хотим как-то смотреть на эти данные, понимать, как они устроены, какие там взаимосвязи, какие признаки важны, а какие — нет, как соотносятся классы между собой. Чтобы это делать, можно нарисовать, например, вот такую картинку, то есть для всех пар признаков изобразить проекцию выборки на эту пару признаков, то есть пытаться понять, как соотносятся именно эти два признака между собой, а также для каждого одного признака нарисовать гистограмму распределения. Конечно, по этим картинкам можно что-то понять. Можно заметить, что какие-то пары признаков лучше разделяют классы, или что какой-то один признак очень неплохо их разделяет, но при этом мы не видим картины в целом, мы не видим, как взаимодействуют признаки между собой в совокупности. Хотелось бы отобразить всю выборку в двумерное или трехмерное пространство так, чтобы по вот такой, например, картинке сразу были видны все закономерности в данных, вся структура этих данных. Например, что какие-то классы сильно перемешаны между собой, какие-то классы выделяются и их можно хорошо отделить от всего остального, и так далее. Собственно, именно так мы приходим к задаче визуализации данных: это частный случай нелинейного понижения размерности, когда размерность пространства, в которое мы пытаемся спроецировать нашу выборку, это 2 или 3. Двумерное пространство — плоскость, или трехмерное пространство, то есть просто пространство. 2 или 3, потому что мы умеем отображать данные только в пространствах размерности 2 или 3, в пространствах большей размерности мы мыслить не умеем. И при этом мы хотим спроецировать данные так, чтобы сохранить всю их структуру, чтобы после проецирования сохранились все закономерности, и мы могли их наблюдать по этой картинке. Один из классических примеров, на которых рассматривают задачи визуализации данных — это набор данных MNIST. Этот набор данных состоит из некоторого количества изображений цифр, написанных от руки. Понятно, что все люди пишут цифры по-разному, кто-то их наклоняет, кто-то не наклоняет, кто-то делает засечки, кто-то не делает, и так далее. Эти цифры очень разные. И при этом нужно их разделить на 10 классов, то есть определить по изображению, что за цифра на нем нарисована. В этом датасете каждая картинка имеет размер 28 на 28 пикселей, то есть всего каждый объект имеет 784 признака. Но при этом понятно, что внутренняя размерность данных гораздо ниже. Мы могли бы использовать гораздо меньше признаков, чтобы характеризовать каждую рукописную цифру. Это можно понять, например, на простом примере. Если мы будем генерировать случайные картинки размером 28 на 28 пикселей, то будем получать что-то вот такое. На этих картинках нет ничего общего с изображениями цифр, это просто какие-то случайные изображения такого же размера. При этом вероятность того, что мы при такой случайной генерации получим именно изображение цифры или что-то похожее на нее, крайне мала. Хотя бы это дает понять, что внутренняя размерность сильно ниже. Если мы воспользуемся одним из методов визуализации, о которых будем говорить в этом уроке, то получим вот такое изображение. Всего лишь двух признаков хватит, чтобы изобразить наши цифры так, что все классы будут идеально разделимы. И при этом будут видны некоторые интересные закономерности. Какие-то классы совсем не похожи на остальные, а какие-то находятся близко, а значит, что начертание у них довольно похожее друг на друга. Итак, можно поговорить про ещё... про пример ещё одной задачи. Эта задача используется в одном из конкурсов на сайте Kaggle. В ней требовалось взять описание товара, его характеристики, и по этому описанию понять, к какой из 9-ти категорий этот товар относится. То есть задача была классовой классификации. При этом признаков было 93, то есть описание довольно высокоразмерное. Если изобразить эту выборку, опять же, с помощью одного из методов визуализации данных, то можно получить вот такое изображение, на котором видно, что некоторые классы хорошо выделяются в отдельные кластеры, а какие-то классы очень сильно перемешиваются между собой. И при этом эта визуализация очень качественная в том смысле, что даже если применять сложные методы вроде градиентного бустинга или случайных лесов для разделения классов в исходном пространстве, в пространстве размерности 93, то там эти свойства будут сохраняться. Те классы, которые здесь хорошо отделимы, будут отделимы и там, а те классы, которые здесь сильно перемешаны между собой, даже в пространстве высокой размерности будет очень сложно разделить. Ошибка для них будет довольно большой. Мы обсудили задачу визуализации данных. Это задача, в которой требуется отобразить выборку в пространство размерности 2 или 3. И это задача, которая нужна для того, чтобы как-то посмотреть на данные, понять, какую структуру они имеют внутри себя. В следующем видео мы поговорим о том, как, какими методами можно данные визуализировать, что можно использовать для решения этой задачи.

[ЗАСТАВКА] В этом видео мы поговорим про многомерное шкалирование — один из довольно простых методов нелинейного понижения размерности. Мы уже обсуждали линейные методы понижения размерности, например, метод случайных проекций, в котором каждый новый признак линейно выражается через исходные признаки, и при этом веса этой линейной комбинации выбираются случайно. Например, из нормального распределения с нулевым средним и какой-то дисперсией. Также мы обсуждали метод главных компонент, который более грамотно выбирает эти веса. Он выражает их через сингулярные векторы матрицы «объекты-признаки», но при этом он все еще является линейным. Оба этих метода не могут поймать нелинейные зависимости в данных. Поймать эти зависимости может, например, метод многомерного шкалирования, или сокращенно MDS. Гипотеза этого метода состоит в том, что попарные расстояния между объектами в новом признаковом пространстве должны быть как можно ближе к попарным расстояниям в исходном пространстве признаков. То есть, например, если есть два объекта, и расстояние в исходном пространстве между ними было равно 10, то в новом пространстве будем пытаться разместить их так, что расстояние между ними будет как можно ближе к этой десятке. Давайте введем несколько формальных обозначений. Объекты в исходном высокомерном пространстве будем обозначать как x1, ..., xl. Всего их l-штук. Объекты после проецирования, объекты в новом маломерном пространстве, будем обозначать как x1 с волной, ..., xl с волной. Расстояния между объектами в исходном пространстве — это dij. Они измеряются по какой-то метрике ρ, это необязательно евклидова метрика, может быть какая угодно метрика. Расстояния между объектами в новом маломерном пространстве — это dij с волной, и их мы уже будем измерять с помощью евклидовой метрики. Обратите внимание, что здесь нам необязательно знать признаковые описания объектов в исходном пространстве. Достаточно знать лишь попарные расстояния между ними. То есть этот метод можно использовать в задачах, где признаки мы не знаем, а вот расстояния между объектами измерять умеем. В задаче многомерного шкалирования будет следующая постановка: мы для каждой пары из i-того и j-того объекта будем пытаться сделать как можно ближе расстояние между объектами в новом маломерном пространстве (то есть расстояния между xi с волной и xj с волной) и расстояние между объектами в исходном пространстве, то есть dij. Отклонения между этими расстояниями будем измерять с помощью квадратичного отклонения, то есть считаем их разность и возводим в квадрат. Мы уже хорошо знакомы с этим подходом. И вот этот функционал, который для всех пар объектов измеряет такое отклонение, мы будем минимизировать. Обратите внимание, что эта задача довольно сложная. Раньше мы обычно вели суммирование просто по всем объектам. Здесь же суммирование ведется по всем парам объектов. Также здесь используются не какие-то веса или параметры модели, а в оптимизации участвуют непосредственно сами признаковые описания в новом пространстве. Поэтому задача сложная, и метод оптимизации будет тоже довольно сложным. Мы не будем его обсуждать, скажу лишь, что это метод SMACOF, который обычно используется. Он состоит в том, что на каждой итерации данный функционал оценивается сверху другим функционалом, который оптимизировать уже гораздо проще. Давайте посмотрим, как работает метод MDS — метод многомерного шкалирования — на задаче MNIST. Видно, что он довольно неплохо разделяет цифры. То есть действительно, каждый класс оказывается хорошо отделимым от остальных классов. Но при этом как объекты внутри классов, так и сами классы оказываются довольно близки друг к другу. Это следствие того, что мы пытаемся отобразить довольно многомерное (почти тысячемерное) пространство в двумерное. А в пространствах высокой размерности есть такая штука как проклятие размерности, которое состоит в том, что чем выше размерность пространства, тем более похожи друг на друга расстояния между различными парами объектов. Итак, мы обсудили метод многомерного шкалирования, который пытается найти такое отображение объектов в маломерное пространство, что при этом как можно лучше сохраняются попарные расстояния между объектами. В следующем видео мы поговорим про метод TSNE, который является развитием метода многомерного шкалирования и устраняет довольно много его проблем.

[ЗАСТАВКА] В этом видео мы поговорим про метод t-SNE, один их наиболее современных подходов к визуализации данных. В прошлый раз мы говорили про задачу многомерного шкалирования, где производится попытка найти новое признаковое описание объектов (маломерное признаковое описание объектов), что расстояние между парами объектов в новом пространстве как можно ближе к расстоянию между объектами в исходном многомерном признаковом пространстве. При этом задача, которая получается, она довольно сложная, потому что понятно, что очень непросто сохранить расстояние при радикальном уменьшении размерности пространства. Скорее всего, решение задачи будет получаться не очень качественным. Метод SNE пытается решить эту проблему следующим образом: он больше не требует близости попарных расстояний, он требует лишь, чтобы сохранялись пропорции расстояний между объектами. А именно: допустим у нас есть тройка объектов — xi, xj и xk, и мы знаем, что расстояние между i-тым и j-тым объектом в α раз больше, чем расстояние между i-тым и k-тым объектом. Тогда в методе SNE будет требоваться, чтобы и в новом пространстве, маломерном пространстве, это свойство сохранялось, то есть чтобы расстояние между проекциями объектов xi и xj было в α раз больше, чем расстояние между проекциями объектов xi и xk. Чтобы это формализовать, воспользуемся следующим подходом. Введем следующие распределения на объектах при условии i-го объекта. Вероятность j-го объекта при условии i-го объекта будет вычисляться как нормированное расстояние между i-тым и j-тым объектом, где расстояние считается как экспонента в степени евклидова расстояния между этими объектами. Тут еще присутствует параметр σ, который является гиперпараметром метода. В каждом пакете есть рекомендации по выбору этого параметра, которые работают довольно хорошо. Итак, вероятность вычисляется как расстояние между этими объектами, деленное на сумму расстояний от i-го объекта до всех остальных объектов. Получаем такое распределение, оно как раз нужно для того, чтобы учитывать только пропорции, то есть этому распределению неважны абсолютные значения, например, евклидовой метрики в исходном пространстве. Ему важны лишь соотношения: расстояние до какого объекта во сколько раз было больше расстояния до другого объекта. Также мы вводим такие же распределения в новом маломерном пространстве, которое обозначается как q от xj при условии xi с волной, и вычисляются таким же образом: это экспонента в степени расстояния между парой объектов делить на сумму экспонент для всех пар между i-тым и другими объектами. Чтобы как-то сравнить эти два распределения, нам нужно выписать функционал, который находит различия между двумя вероятностными распределениями. Известно, что для этого очень хорошо подходит дивергенция Кульбака-Лейблера. Она вычисляется вот таким образом: там стоит суммирование, и под знаками суммы находится вероятность xj при условии xi, умноженная на логарифм отношения этого расстояния в исходном признаковом пространстве и расстояния в новом признаковом пространстве, q от xj при условии xi. Мы минимизируем этот функционал и пытаемся найти описание объектов в новом маломерном пространстве. Минимизировать его можно, например, с помощью стохастического градиентного спуска. У этого подхода есть одна проблема, которая решается в методе t-SNE. Дело в том, что в пространствах высокой размерности, как мы уже упоминали, имеет место проклятье размерности, которое состоит в том, что, во-первых, чем больше размерность пространства, тем более близки друг к другу расстояния между парами объектов, а во-вторых, вообще, чем больше размерность пространства, тем проще нам разместить объекты ближе друг к другу, тогда как ту же самую близость в двухмерном пространстве сохранить очень сложно. Собственно, тот способ, с помощью которого мы штрафуем за несохранение пропорций, а именно: экспонента в степени евклидова расстояния, и потом это еще подставляется в дивергенцию Кульбака-Лейблера, на самом деле очень сильно штрафует, если мы увеличиваем расстояние в новом пространстве. Нужно с этим как-то бороться. Оказывается, проблему можно устранить, если использовать немного другой способ вычисления распределений в новом пространстве. В исходном пространстве признаков мы используем точно такой же подход с экспонентой и евклидовым расстоянием, а в маломерном пространстве вероятности вычисляются по вот такой формуле. Здесь уже не экспонента, а единица делить на один плюс расстояние между парой объектов. Действительно, можно проверить, что при таком подходе в методе t-SNE объекты в новом пространстве — двумерном или трехмерном — оказываются гораздо более сильно разнесенными между собой. Разделение гораздо более четкое. Собственно, если посмотреть на результат применения метода t-SNE к набору данных MNIST, мы увидим вот такую картину. Видно, что классы разделяются очень хорошо. Те классы цифр, которые похожи по начертанию, оказываются близко друг к другу; те классы цифр, которые очень непохожи по начертанию на остальные, сильно отделены от общего облака. При этом некоторые классы разбиваются еще на подкластеры, как, например, единичка. У нее выделяется три подкластера, и они зависят от того, как именно единица рисуется: есть ли у нее верхняя засечка или нет; рисуем ли мы внизу палочку или нет. Собственно, это все характеризует эти кластеры. Визуализация получается очень хорошая, и она реально отображает структуру объектов в исходном пространстве. Если же вспомним, как этот набор данных был визуализирован методом MDS (методом многомерного шкалирования), видно, что ситуация не такая хорошая. Да, объекты разделимы, но при этом классы довольно близки друг к другу, и нет никакой кластеризации внутри отдельных классов, то есть визуализация получается довольно хаотичной. Итак, мы поговорили про метод t-SNE, один из наиболее качественных методов визуализации данных, который, во-первых, требует лишь сохранения пропорций расстояния между объектами и не сильно штрафует за то, что расстояния увеличиваются в новом маломерном пространстве.

[БЕЗ_ЗВУКА] Привет! В этом видео мы продолжим говорить про задачи визуализации данных и также на примерах рассмотрим все эти методы, которые вы уже успели изучить. Работать мы будем с dataset digits — это стандартный dataset, который представляет из себя написанные от руки цифры. Вот давайте его загрузим. Делаем это с помощью модуля dataset из библиотеки sklearn. И теперь посмотрим, что же он из себя представляет. Посмотрим на описание. Мы видим, что набор представляет из себя изображение цифр, всего их 5620. Каждая цифра представляется в виде вектора из 64 цифр, каждая из которых характеризует яркость пикселя в данной точке. Соответственно, изображение представляет из себя прямоугольник 8 на 8. Мы видим, что пропущенных значений у нас, конечно же, нет. И также у нас есть ссылка на UCI репозиторий, в котором вы можете ознакомиться с полным описанием dataset и скачать его. Итак, теперь давайте посмотрим, как же выглядят данные и как представлены целевые метки. Вот мы видим, что… Смотрим на первый объект, у него целевая метка 0 — означает, что это рукописная цифра 0, и, собственно, мы видим набор его признаков. Вектор длиной 64, а изменяющийся в диапазоне от 0 до 16. Теперь давайте попробуем его отрисовать, посмотреть, как же эта картинка выглядит. Для того чтобы отрисовывать изображение, Python представляет нам функцию imshow — это функция библиотеки matplotlib. Вот давайте попробуем сделать следующее: передадим этой функции в качестве аргумента описание нашего объекта и попробуем его нарисовать. Вот потренируемся на самом первом объекте. Отметим, что почему-то ничего не получилось. Давайте смотреть почему. Написано, что размерность нашего объекта, размерность данных, не походит для того, чтобы строить рисунок. Давайте посмотрим, какая же у нас размерность. Видим, что размерность 64 и 0. То есть фактически мы передаем в данную функцию не матрицу, а вектор. Поэтому не понятно, как же этот вектор отобразить. Что мы с вами можем сделать? Мы можем вспомнить, что ndarray представляет нам функцию reshape, которая позволяет изменить размерность нашего вектора. Вот давайте вместо того чтобы работать со строчкой длиной 64, будем работать с матрицей длиной 8 на 8. Для этого вызовем метод reshape с параметрами 8 и 8. Вот давайте посмотрим. Видим, что данные не изменились, но теперь это уже не вектор, а матрица, поэтому кажется, что в этом виде можно попробовать нарисовать. На всякий случай вызываем метод shape. Видим, что размерность правильная, и теперь давайте рисовать. Вот видим, что теперь все получилось. Перед нами изображение рукописной цифры, и мы знаем, что это 0. На самом деле, создатели dataset уже позаботились о том, чтобы нам было удобно отображать цифры и не нужно было каждый раз делать reshape. Давайте посмотрим на все эти атрибуты, которые предоставляет наш dataset. Видим, что помимо данных, целевой переменной, имен классов и описаний, у нас есть еще ключ images. Давайте посмотрим, что это такое. Видим, что это фактически то же самое описание объекта, но только уже не в виде вектора, который очень удобен для решения задач классификации или кластеризации, а в виде матрицы, которая прекрасно подходит для того, чтобы объект рисовать. Вот давайте попробуем его отрисовать, используя images вместо data. И видим, что получаем ту же самую картинку, еще и удобнее — не нужно делать reshape. Теперь давайте посмотрим, как мы можем влиять на то, какое изображение получается. Возможно, в таком виде не очень видно, что это 0. Изображение довольно яркое, разноцветное, ну на это можно легко повлиять. Во-первых, мы можем влиять на то, каким и в какой цветовой она карте, в каком colormap отображается наше изображение. Мы можем выбрать любое. Например, выбрать colormap hot или выбрать colormap gray — это черно-белые тона. Также мы можем влиять на так называемую interpolation, или параметр, который отвечает за то, как будут выглядеть границы наших пикселей на изображении. Давайте рассмотрим 4 разных варианта. И видим, что, в общем-то, каждый может выбрать себе то, что вам больше нравится. Вот мне больше всего нравится черно-белое изображение, поэтому давайте в этих же цветах отрисуем и другие цифры. Посмотрим, как они выглядит, и легко ли их узнать глазами. Так, воспользуемся subplot и отрисуем первые 10 объектов из нашего dataset, будем надеяться, что это разные цифры. Так, видим, что мы получили изображение. Вот действительно, перед нами изображение цифр от 0 до 9, и в принципе невооруженным взглядом их узнать можно. Соответственно, это дает нам следующее предположение: если мы с вами цифры можем довольно неплохо узнать, мы видим, что, в общем-то, их начертание отличается, то, наверное, можно решать задачи классификации более-менее успешно. Наверное, она решается неплохо. Из предыдущих лекций вам известно, что такое представление цифр, а именно представление в виде 64 признаков, является несколько избыточным. Для того чтобы успешно отличать эти цифры друг от друга, нам достаточно гораздо меньшей размерности данных. Вот давайте попробуем сделать следующее: сначала уменьшим размерность данных теми методами, которые вы изучали, и посмотрим, как же это сказывается на качестве классификации. Для начала сделаем следующее: давайте отрежем небольшую подвыборку из нашей выборки данных и будем работать с ними. Это нужно только для того, чтобы все наши команды, всё то, что мы запускаем, работало несколько быстрее. Вы, конечно же, можете тренироваться на всем наборе данных. Вот я возьму первую тысячу. И теперь давайте посмотрим, что у меня получилась сбалансированная выборка, то есть, что все объекты встречаются приблизительно в одинаковой пропорции, объекты всех классов. Сделаем это с помощью Counter. Видим, что, ну действительно, каждый объект встречается приблизительно 100 раз, — то, что нужно. И чтобы это было более наглядно, можем нарисовать гистограмму. Это не сложно. И видим, что да, в принципе все объекты встречаются одинаковое количество раз. Теперь давайте построим некоторый классификатор. Ну так как мы хотим смотреть на то, как наши объекты отображаются на плоскости, нам подойдет метрический классификатор. Он работает на основе близости объектов, поэтому это то, что нужно. Выбираем классификатор K ближайших соседей. И давайте сделаем следующее: обучим его на всей выборке, на всех тех 1000 цифрах, которые мы выбрали, и посмотрим, какое получается качество. Фактически это нам скажет о том, насколько наши объекты легко классифицировать по их изображениям. Итак, мы видели довольно подробную информацию о качестве классификации. Сделали это с помощью метода classification_report. Он предоставляет нам подробную сводку о том, какую точность, полноту f1 мы получили для каждого класса (вот можем посмотреть, что классов здесь ровно 10), а также дает нам усредненные оценки. Видим, что в среднем мы правильно классифицируем практически всю выборку. В данном случае мы видим, что классификация производится на основании 5 ближайших соседей, поэтому что мы можем сказать? Мы можем сказать, что большинство объектов находятся в окружении объектов из своего класса. В общем-то, это хорошо. Это говорит о том, что наши изображения действительно похожи друг на друга, что начертания похожи, и на основе изображений мы можем решать эту задачу. Теперь давайте сделаем следующее: если мы перейдем в пространство меньшей размерности, то есть перейдем в пространство 2, нам снова будет интересно, остается ли это условие в силе. То есть в новом пространстве наши цифры так же находятся близко друг к другу или уже нет, или уже в ближайшем окружении начнутся встречаться объекты чужих классов, и тогда наше качество классификации уменьшится. Вот давайте на это посмотрим. Начнем с линейных методов понижения размерности. Первый метод, который мы рассмотрим, — это случайные проекции. Для начала нам нужно построить наши преобразования, делаем это с помощью модуля random_projection. Строим объект SparseRandomProjection и передаем ему следующие аргументы. Первый аргумент — это количество новых компонент, которые нас интересуют. Мы с вами хотим строить изображение наших объектов, поэтому нам удобно работать с двумя компонентами. И зададим random_state. Итак, после того, как мы наш объект построили, нужно это отображение обучить на исходных данных и применить, то есть получить данные в новой размерности. Вот давайте это сделаем с помощью метода fit_predict. Итак, готово наше новое представление о данных, теперь давайте его изобразим на плоскости. В данном случае мы видим набор точек, где разными цветами отображены объекты разных классов, то есть разными цветами у нас отображены разные цифры. Видим, что, в общем-то, они довольно сильно перемешены и так вот на глаз не очевидно, в какой области нужно ожидать встретить какую цифру. Ну на глаз оценивать такие вещи плохо, давайте попробуем обучить классификатор. Снова обучим ту же самую модель knn и сравним, насколько изменилось качество. Посмотрим для скольких объектов в ближайшем окружении вновь находятся объекты того же самого класса. Для этого оцениваем качество на обучающей выборке, и что мы видим? Наше качество в среднем упало до 50 %, то есть мы видим, что для половины объектов ближайшие 5 соседей уже могут быть числами из разных классов. Давайте посмотрим на следующий метод — это метод PCA, или метод главных компонент. Он уже находится в модуле decomposition RandomizedPCA. И давайте снова построим преобразование, в качестве результата получим представление объектов в двумерном пространстве. Снова применяем метод fit_predict, вернее fit_transform, и смотрим на наше изображение Видим, что, ну на глаз изображение кажется более качественным. У нас явно появились области разных цветов, то есть мы видим, что для некоторых классов получается отделить их от других. Получается сделать так, чтобы объекты этих классов были рядом друг с другом. Теперь давайте оценим это с помощью классификатора. Итак, считаем качество. И да, видим, что в среднем качество уже лучше, чем для предыдущего случая. Наша аккуратность 0,7, то есть, в общем-то, 70 % объектов мы все еще можем классифицировать. Представьте себе, как здорово — мы уменьшили размерность в 32 раза, вместо 64 признаков у нас теперь всего 2, и тем не менее, мы получаем довольно высокое качество классификации. Давайте двигаться дальше. Перейдем к нелинейным методам. Следующий метод, который мы рассмотрим, — это multidimensional scaling, или многомерное шкалирование. Он находится в модуле manifold. И давайте построим следующий объект: будем уменьшать размерность до 2 компонент и задачу оптимизации будем решать не более чем за 100 итераций. Строим обновленные данные, делаем это с помощью метода fit_transform, и теперь давайте отобразим изображение. Кажется, что нелинейные методы должны работать даже немножечко лучше. Вот давайте посмотрим. Ну да, видим, что вновь мы можем отличить группы объектов. И давайте получим оценку на основе классификации. Обучаем модель и видим, что качество примерно такое же, но немножечко больше. Тоже метод довольно неплохо работает. Последний метод, который мы рассмотрим, — это метод t-SNE. Его реализация также находится в модуле manifold, поэтому давайте создадим наш объект-преобразователь, основу оставим для компонента, инициализировать преобразование будем с помощью PCA. Другой вариант — делать это с помощью случайной инициализации, или random. И давайте получим преобразование данных с помощью метода fit_transform. Итак, это занимает какое-то время, потому что опять же мы решаем задачу оптимизации. А теперь давайте строить изображение. Видим, что это, наверное, самая лучшая картинка из всех, которую мы получили. Здесь очень легко убедиться в том, что объекты разных цветов или объекты разных классов находятся довольно далеко друг от друга. Кажется, что если мы будем решать задачу классификации в таких признаках, то мы должны получить довольно высокое качество. Может быть, даже сравнимое с изначальным, с данными в изначальном пространстве. Вот давайте это проверим. Снова обучим классификатор ближайших соседей и видим, что да, действительно, наше качество в среднем равняется 0,99 по метрике accuracy, то есть практически нигде мы с вами не ошибаемся. Тоже довольно неплохой метод уменьшения размерности. А на этом мы заканчиваем. На этом уроке мы научились на практике применять такие методы уменьшения размерности, как t-SNE, multidimensional scaling, PCA, а также метод случайных проекций. На этом мы заканчиваем модуль по визуализации данных, а на следующей неделе вы познакомитесь с такими интересными задачами, как topic modeling, или тематическое моделирование.