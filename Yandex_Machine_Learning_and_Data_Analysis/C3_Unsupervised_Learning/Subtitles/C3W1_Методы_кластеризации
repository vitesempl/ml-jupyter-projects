[ЗАСТАВКА] В этом видео мы подробнее поговорим о методе К средних. Сначала мы вспомним, как работает K Means, затем обсудим некоторые вариации K Means, затем обсудим, что же делать, когда объектов выборки довольно много. После этого — что делать, когда признаков довольно много. Затем обсудим проблему выбора начальных приближений, потому что, в зависимости от того, как вы их выберете, алгоритм может хорошо сходиться, не очень хорошо сходиться, делать это быстро или медленно. После этого разберём простой и наглядный пример использования K Means, затем обсудим особенности работы K Means с разными формами кластеров. Наконец, обсудим ещё один пример — мешок визуальных слов, который раньше часто встречался в анализе изображений. И в конце рассмотрим некоторый функционал, который K Means оптимизирует. Итак, начнём. Как работает K Means? Ну всё очень просто. Давайте представим, что у нас просто два кластера, и изначально мы выбираем какие‐то случайные центры кластеров, и теперь относим точки к тому кластеру, к которому каждая точка ближе. Ну теперь мы можем взять и посчитать центр кластера прямо по точкам. Давайте вычислим среднее арифметическое точек из одного кластера. И вот мы пересчитали центр. Но теперь, когда центры сдвинулись, распределение точек по кластерам тоже можно поменять. Снова относим точки к тому кластеру, центр которого ближе, и получаем новые кластеры. Теперь мы можем опять уточнить центр кластеров. Снова уточняем. И вот, повторяя такой итеративный процесс, постепенно сходимся к какому‐то ответу, к каким‐то кластерам. Тот K Means, который мы сейчас обсудили, называется K Means'ом Болла Холла. А есть ещё версия Мак Кина. В ней каждый раз, когда объект переходит из одного кластера в другой, центры кластеров пересчитываются. Если же у нас очень много данных, то становится понятно, что у нас нет никакого особого желания считать эти огромные суммы и вычислять среднее для объектов, которые попадают в кластер, когда объектов много. И возникает идея: давайте просто на каждой итерации брать случайную подвыборку и считать по ней. В среднем всё должно сходиться к тому же результату. Но теперь мы можем работать с достаточно большими выборками. Такой K Means называется Mini‐Batch K Means. Если же у нас очень много признаков, то можно справедливо заметить, что вычисление расстояния будет не очень приятной операцией. И действительно, оно требует порядка d элементарных операций, если размерность пространства d. И это означает, что, если признаков очень много, работать всё будет очень долго. Можно решить эту проблему, уменьшив число признаков. Какие у нас здесь есть варианты? Ну, можно отобрать признаки, можно использовать какой‐нибудь метод преобразования признаков, например метод главных компонент, об этом мы ещё поговорим позднее. Или сингулярное разложение (SVD), об этом тоже мы ещё поговорим. Что касается выбора начальных приближений, то достаточно понятно, что хочется, чтоб центры кластеров были довольно удалены друг от друга. При этом понятно как быть в случае двух кластеров. Мы могли бы просто взять точки, максимально удалённые друг от друга изначально, а дальше посмотреть, к чему это всё сойдётся. Но что делать, когда кластеров k? Что значит k достаточно удалённых друг от друга точек? Здесь возникает такое решение, называемое K Means ++. Давайте первый центр выберем случайно из точек, которые есть в выборке. А каждый следующий центр будем тоже выбирать случайно, но теперь уже не из равномерного распределения на точках, а из распределения, в котором вероятность взять каждую точку пропорциональна квадрату расстояния от этой точки до центра ближайшего кластера. Итак, простой пример. Представьте, что нам нужно понизить количество цветов в изображении. Изначально цветов очень много. Мы можем это сделать как‐нибудь просто, ну, например, взять какие‐то случайные точки в RGB пространстве и каждый пиксель заменить на пиксель того цвета, который соответствует ближайшей из случайных точек. С другой стороны, мы могли бы просто рассмотреть точки изображения как объекты в пространстве признаков и кластеризовать их, например, с помощью K Means. Смотрите, какой получается результат — заметно более приятный. Ну и в принципе, когда возникает такая ситуация, что хочется свести возможные варианты векторов признаков к какому‐то небольшому набору, бывает уместно сделать кластеризацию. То есть, это такой хороший, естественный способ огрубить признаки и использовать какую‐то более простую их версию. K Means может совершенно по‐разному работать в зависимости от формы кластеров, от того, угадали вы правильно количество кластеров или нет. Ну например, на картинке, которую вы сейчас видите на слайде, показана ситуация, когда количеств кластеров было указано неверно – два кластера, когда на самом деле следовало бы выделять три. Ну и конечно, результат получается не совсем тот, который хотелось бы ожидать. С другой стороны, кластеры могут быть достаточно своеобразные: они могут быть вытянутые, могут быть совершенно неодинакового размера. Вот в случае вытянутых кластеров запросто алгоритм может решить, что два вытянутых кластера — это одно скопление. В случае неодинакового разброса у кластеров, алгоритм может не очень оптимально поделить два рядом находящихся друг с другом кластера. В случае вообще неравного размера кластеров, кластеризация тоже может быть не очень удачной, как вы видите сейчас на картинке. Другой пример использования K Means — это мешок визуальных слов. До того как в анализе изображений начали активно использовать свёрточные нейросети, применяли следующий подход для классификации. Давайте просто нарежем много кусочков изображения и по аналогии с тем, как классифицируются тексты с помощью мешка слов, ну то есть по частотам разных слов в тексте, будем строить признаки изображений по частотам разных фрагментов изображения, которые встретились. Ну то есть видим много фрагментов, похожих на колёса, значит – какое‐то транспортное средство. Видим фрагменты, похожие на глаза, на нос, значит, наверно, лицо. И так далее. Но вот какая проблема. Если мы просто пробежимся по большой выборке изображений, нарежем вот таких вот кусочков изображения и попробуем это использовать в качестве слов, получится не очень здорово. Этих кусочков будет очень много, и совершенно непонятно – что значит фрагмент входит в изображение или не входит? Конечно же, точного вхождения в другое изображение почти никогда не будет. Как же выкрутиться из этой ситуации? А всё просто. Можно просто представить в каком‐то признаковом пространстве все эти кусочки изображения, а их уже кластеризовать с помощью K Means. Ну например, на 100 или на 500 кластеров, и каждый кластер уже интерпретировать как визуальное слово, ну то есть набор каких‐то похожих фрагментов. И теперь, когда нам нужно понять, какие визуальные слова представлены в изображениях, мы просто проходимся по изображению и смотрим: каждый фрагмент попадает в какой‐то определённый кластер, и к этому кластеру мы и относим этот фрагмент. И именно в этот признак и добавляем единичку. Так мы получаем признаковые описания для изображений, и их уже можно дальше классифицировать. K Means, на самом деле, не просто какой‐то набор эвристик, в нём есть некоторая математическая логика. Мы уже с вами обсуждали, что можно измерять среднее внутрикластерное расстояние, а вот в случае, когда у кластеров есть центры, можно измерять его чуточку проще. Можно просто смотреть на среднее расстояние от центра кластера. Ну вот, K Means, оказывается, в варианте Мак Кина находит локальный минимум функционала Ф0. Это, в принципе, интуитивно понятно: ведь K Means итеративно минимизирует средние внутрикластерные расстояния. В самом деле, объект у нас присваивается к тому кластеру, центр которого ближе. Ну понятно, что лучше защитать тому кластеру, который ближе: функционал будет меньше. С другой стороны, центр кластера перемещается в среднее арифметическое векторов признаков объектов, которые принадлежат этому кластеру. А это тоже означает, что мы выбираем такую точку, которая минимизирует среднеквадратичное отклонение от других точек кластера. В этом легко убедиться, если просто выписать среднеквадратичное отклонение, продифференцировать, приравнять к нулю и получить, что самая подходящая точка — это среднее арифметическое точек кластера. Обратите внимание: здесь мы смотрели именно на среднеквадратичное отклонение, и это отсылает нас к евклидовой метрике в первую очередь. И, в принципе, K Means особенно естественно работает в ситуации с евклидовой метрикой. Итак, подведём итог. Мы с вами вспомнили, как работает K Means. Выяснили, что есть вариация Болла Холла и вариация Мак Кина. Выяснили, что делать, когда много данных, когда много признаков. Обсудили вопрос выбора начальных приближений и познакомились с алгоритмом K Means ++. Разобрали пример с квантизацией изображений. Изучили работу K Means с разными формами кластеров. Разобрали пример с мешком визуальных слов и даже обсудили математическую подоплёку K Means.

В этом видео мы продолжим рассматривать подробные методы кластеризации и разовьём тему Cummins, перейдя к другому алгоритму, но очень похожему на Cummins — EM‐алгоритму. Итак, сначала мы обсудим, как же выглядит кластеризация с помощью EM‐алгоритма. После этого разберёмся с постановкой задачи и выясним, почему нельзя было эту задачу решать в лоб, просто максимизируя правдоподобие. Затем мы опишем EM‐алгоритм, рассмотрим его частный случай для ситуации, когда мы считаем, что плотности кластеров гауссовские. Затем будет простое объяснение метода. А после него — классическое объяснение. В конце мы обсудим, для чего ещё используется этот алгоритм. Итак, как всё выглядит? Метод, так же как и Cummins, итеративный. Сначала мы задаём какое‐то начальное приближение для наших кластеров и затем уточняем кластеры, то есть перемещаем центр кластера, если он есть как параметр распределения, и уточняем форму кластера на каждом шаге. Как мы вообще смотрим на задачу кластеризации в случае решения её с помощью EM‐алгоритма? У нас есть априорные вероятности кластеров w1, ..., wk-тая. У нас есть плотности кластеров p1, ..., pk-тая. Ну то есть это плотность распределения признаков внутри кластеров. Если мы посмотрим на плотность распределения вектора признаков вообще по всем кластерам, то получится просто взвешенная сумма плотностей по каждому кластеру. Ну, действительно, надо просто априорную вероятность умножать на плотность кластера и просуммировать это по всем кластерам. Так что же нам хочется делать? Нам хочется на основе выборки оценить параметры модели. То есть веса компонент, ну то есть кластеров, и плотности. Зачем мы это всё делаем? Ну, для того чтобы как‐то решать задачу кластеризации. То есть чтобы оценив эти параметры, относить объект к тому или другому кластеру. Как это можно делать, мы тоже обсудим в процессе. Такая задача называется задачей разделения смеси распределений, потому что мы имеем некоторое распределение p(x), которое представляется в виде взвешенной суммы других распределений. И нам нужно оценить веса в этой сумме и оценить параметры других распределений. В этой задаче обычно говорят о случае, когда все распределения отдельных компонент из одного семейства. Ну, например, все компоненты распределены нормально со своими параметрами у каждой компоненты. Ну как выглядит смесь распределений? Всё очень просто. Вы можете представить себе ну, например, гауссовскую плотность. А можете представить себе плотность, которая выглядит как совмещение нескольких гауссиан. У неё есть несколько пиков, и они плавно переходят друг в друга. Ну вот такая плотность сейчас показана на слайде. Примерно так смесь распределений и выглядит. И это просто соответствует ситуации, когда у вас не одно сгущение точек, а несколько сгущений точек. В двумерном случае пример будет выглядеть вот так. Казалось бы, почему бы не оценить все параметры, просто максимизируя правдоподобие. Ну, давайте выпишем правдоподобие, напишем, что мы хотим попасть в ту точку, где правдоподобие достигает максимума, и выразим оттуда параметры. Ну почему бы не попробовать? Для этого нам просто нужно записать сумму логарифмов p(x) i-тых по всем i из выборки. Что это означает? Это означает, что нам нужно будет максимизировать сумму логарифмов сумм. А это уже ни капельки не удобно. Действительно, логарифм суммы ни во что хорошее не расписывается. Ну вот, оказывается, что можно достичь успеха, если решать эту задачу итеративной процедурой, в которой повторяются два шага — E‐шаг и M‐шаг. В E‐шаге мы просто оцениваем некоторые параметры, которые на M‐шаге считаем зафиксированными, то есть константами. При этом, зная эти параметры, нам уже намного проще оптимизировать правдоподобие. И если явно выписать формулы, которые получаются из максимизации правдоподобия, получается уже гораздо более приятное выражение. Вот как выглядит EM‐алгоритм для случая, когда классы имеют гауссовскую плотность. В этой ситуации мы можем даже ещё более подробно расписать M‐шаг. Можем явно выписать формулы для обновления вектора средних кластера и для обновления матрицы ковариаций кластера. Это всё имеет достаточно простое объяснение. Просто в EM‐алгоритме мы выбираем скрытые переменные таким образом, чтобы с ними было проще максимизировать правдоподобие. То есть на E‐шаге мы оцениваем скрытые переменные, а на M‐шаге оцениваем веса w1, ...,wk-тая и параметры плотности p1, ..., pk-тая, считая скрытые переменные известными. Если посмотреть, что это значит для E‐шага, то становится понятно, что для задачи разделения смеси довольно неплохо подходят в качестве скрытых переменных вероятности Pj-тая при условии xi-того. То есть вероятность объекта xi-тая принадлежать к j‐тому кластеру. Ну, при условии xi-того. Если расписать это всё по формуле Байеса, то мы получим простое выражение для вычисления этих скрытых переменных. На M‐шаге нам нужно просто максимизировать правдоподобие по весам и по параметрам плотностей компонент. Если выписать производные по параметрам и приравнять их к нулю, становится понятно, что веса можно считать просто как среднее скрытых переменных, а параметры компонент можно считать как максимум в другой оптимизационной задаче, которая расписывается явно в случае каких‐то конкретных семейств распределений. С другой стороны, EM‐алгоритм был придуман не из таких простых соображений, а из несколько других, что, собственно, и объясняет его название: — expectation-maximization. Сейчас мы не увидели никакого expectation, никакого матожидания. А вот оказывается, можно было бы посмотреть на задачу следующим образом. У нас есть какая‐то выборка, она получена из какого‐то распределения. У нас есть какие‐то параметры модели. Ну, то есть веса, параметр компонент. И мы можем ввести какие‐нибудь скрытые переменные. И дальше мы можем на E‐шаге уточнять некоторый оптимизированный функционал, вычисляя его как матожидание по Z при условии известной выборки и известных оценок с предыдущей итерации логарифма правдоподобия θ, X и Z вместе. А на M‐шаге вычислять новую оценку для параметров, максимизируя выражение, полученное с E‐шагом. Почему это будет приводить к тому же, что мы описали только что, и как можно было до этого додуматься, конечно, не очевидно. Мы поговорим ещё об этом впоследствии. Какие ещё задачи решаются с помощью EM‐алгоритма? Ну, во‐первых, можно оценивать параметры в других вероятностных моделях. Ну, например, необязательно мы решаем задачу разделения смеси распределений. Вот уже на следующей неделе этого курса мы поговорим о матричных разложениях, которые будут очень близки к вероятностному математическому моделированию, которое будет ещё через неделю. И вот там мы снова встретим EM‐алгоритм. Кроме того, можно использовать EM‐алгоритм для восстановления плотности распределения. Нам необязательно считать, что какое‐то распределение описывается каким‐то совершенно стандартным, ну то есть, там, нормальным или пуассоновским, или ещё каким‐то. Можем считать, что распределение — это смесь каких‐то компонент. И использовать EM‐алгоритм, для того чтобы просто оценить распределение какой‐то величины. Ну и кроме того, EM‐алгоритм можно использовать для решения задачи классификации. Ну, как минимум для того, чтобы восстановить плотности классов, если это нужно нашему классификатору. Ну а кроме того, можно просто использовать EM‐алгоритм для кластеризации, немножко его видоизменить. Ну, например, жёстко приписывать объекты к тем классам, которые для них известны. Подведём итог. Мы с вами посмотрели, как выглядит кластеризация с помощью EM‐алгоритма. Обсудили постановку задачи и выяснили, почему нельзя просто взять и максимизировать правдоподобие. Выяснили, как выглядит EM‐алгоритм. Как он выглядит в случае гауссовских распределений, довольно частая ситуация, когда используются гауссовские распределения в EM‐алгоритме. Как этот метод объяснить просто. Как его объясняют обычно. И для чего ещё используется этот алгоритм.

[БЕЗ_ЗВУКА] В этом видео мы поговорим про агломеративную иерархическую кластеризацию. Итак, сначала вспомним, что такое вообще иерархическая кластеризация, затем о том, как устроена агломеративная иерархическая кластеризация, о том, как можно вводить расстояние между кластерами, эта тема плавно приведет нас к формуле Ланса-Уильямса. Затем мы обсудим очень удобную визуализацию для иерархической кластеризации, а именно дендрограммы, и после этого рассмотрим некоторые примеры работы метода кластеризации на реальных данных. Итак, иерархическая кластеризация — это кластеризация, в которой кластеры вложены друг в друга. Но в то же время этого можно добиваться двумя разными способами. С одной стороны, можно сначала сказать, что пусть все объекты образуют по кластеру, и будем сливать в дальнейшем эти кластеры и в итоге получим какой-то итоговый результат. А можно подойти к этому по-другому: давайте сначала всё объединим в один большой кластер, этот кластер на каждом шаге будем делить и в итоге получим сколько-то кластеров. Эти подходы называются: агломеративный, когда мы объединяем кластеры, и дивизимный, или дивизионный — по-русски говорят и так, и так, по-английски это будет divisive — тот случай, когда мы делим один большой кластер на кластеры поменьше. Мы будем говорить про агломеративную кластеризацию, и вообще, когда говорят «иерархическая кластеризация», часто имеют в виду именно ее. Поэтому приходится понимать из контекста, что имеется в виду: кластеризация с иерархией кластеров или именно агломеративная. Как устроена агломеративная кластеризация? Ну давайте всё поместим в отдельный кластер, ну то есть каждая точка будет представителем отдельного кластера, причем единственным представителем. Ну почему бы и нет? Между прочим, среднее внутрикластерное расстояние для такой кластеризации будет очень ничего. Затем объединим те точки, которые ближе друг к другу. Теперь мы попадаем в не очень удобную ситуацию: нам нужно объединить какие-то кластеры, но уже не все кластеры представлены одной точкой. Таким образом, нам нужно как-то уметь считать расстояние от кластера, который может состоять из более чем одной точки, до какой-то точки. Ну давайте считать, что мы как-то это расстояние введем и продолжим дальше объединять кластеры. Продолжаем, вот уже какие-то кластеры расширяются, ну и постепенно кластеров становится все меньше, точки попадают в уже существующий кластер, при этом какие-то точки могут долгое время оставаться вне всего этого процесса объединения кластеров. Но потом и они к этому присоединяются, и в какой-то момент мы получаем какое-то небольшое количество кластеров, ну вот, например, в данном случае получилось 3 кластера, и, казалось бы, можно было бы уже здесь закончить. Но нет, нам этого недостаточно, мы продолжим объединять, таким образом, получив два кластера и, наконец, один огромный кластер, в который все попало. Зачем же нужно было продолжать? Главная особенность метода агломеративной иерархической кластеризации в том, что если мы захотим получить другое количество кластеров, нам не нужно будет заново запускать алгоритм. У нас есть все это дерево объединений кластеров, и мы можем остановиться в любой момент. Ну то есть сначала мы вычисляем все дерево, а потом говорим, что хотим его в каком-то месте обрезать. Ну то есть если нам нужно 2 кластера или если нам нужно 3 кластера, мы просто не делаем последние итерации объединения. Теперь о расстоянии между кластерами. Как его можно ввести? Ну, во-первых, можно просто посмотреть на среднее расстояние между объектами из двух кластеров. Можно посмотреть на максимальное расстояние между объектами, можно — на минимальное, можно придумать еще что-нибудь. Оказывается, есть такая формула, называемая формулой Ланса-Уильямса, которая обобщает множество разных способов ввести расстояния между кластерами. Она выражает расстояние между кластером, которым получается в результате слияния двух U и V, и каким-то третьим кластером, кластером S. Как вы видите, формула получается рекурсивной, то есть она определяет расстояние между кластерами через расстояние между кластерами, но при этом через расстояние между более простыми кластерами. То есть, например, если U, V и S — кластеры из одной точки, то с помощью формулы Ланса-Уильямса мы сразу же получаем определение для расстояния между кластером из двух точек и кластером из одной точки. И так далее, можно продолжать рассуждение, увеличивая количество точек в кластерах. Ну и вот оказывается, что достаточно просто рассмотреть сумму нескольких слагаемых, описывающих расстояние от кластера U до кластера S, от кластера V до кластера S, между сливаемыми кластерами U и V и модуль разности расстояний от сливаемого кластера до S. Ну то есть для U и для V. И оказывается, что при разных коэффициентах в этой формуле можно получить разные уже знакомые нам способы вычислять расстояния между кластерами. Ну например, можно получить расстояние ближайшего соседа, то есть минимальное из расстояний между объектами из двух кластеров. Можно получить максимальное из расстояний между точками кластеров. Можно получить среднее расстояние. Можно получить расстояние между центрами кластеров и расстояние Уорда, которое особенно хорошо работает в случае евклидовой метрики. Также есть очень удобный способ визуализировать иерархическую кластеризацию. Давайте снова посмотрим на процесс кластеризации. Изначально у нас кластеров столько же, сколько точек в выборке. Затем мы сливаем какие-то два кластера. Давайте посмотрим на расстояние между этими кластерами. Такое же расстояние мы откладываем на графике, который мы сейчас будем строить, называемом дендрограммой. Затем происходит следующее слияние. Здесь мы снова откладываем такое же расстояние, то есть на изображении, приведенном на слайде, расстояние между кластерами — это просто высота дуги, которой мы соединяем метки кластеров. Ну и давайте продолжать эту процедуру, таким образом выстраивая вот такую вот интересную древовидную структуру, в которой мы также сохранили информацию о том, какое расстояние было между кластерами в момент слияния. [ЗВУК_НАЖАТИЯ_КНОПКИ] Ну вот, теперь можно посмотреть на эту структуру и понять, в какой момент можно всё обрезать, исходя из того, что расстояния между кластерами при слиянии стали уж слишком большие. Но в то же время это еще не очень наглядно, может быть, будет интересно построить график зависимости расстояния между сливаемыми кластерами от номера итерации. Ну и конечно, мы можем отсечь всё в каком-то определенном моменте и получить нужное нам количество кластеров. Рассмотрим графики, полученные на реальных данных в задаче кластеризации писем по теме. На слайде показана дендрограмма, полученная на таком датасете. Если посмотреть на небольшие подвыборки и строить дендрограммы по ним, построить по ним уже упомянутый график зависимости расстояния между сливаемыми кластерами от номера итерации, то мы увидим интересное явление. Смотрите: ближе к последним итерациям расстояние между кластерами быстро взмывает вверх, то есть есть ощущение, что существует некоторое разумное количество кластеров, то есть какое-то количество групп точек, которые друг от друга уже более-менее серьезно удалены. Если мы возьмем выборку побольше, то мы увидим, что график, с одной стороны, стал более гладким, что ожидаемо — итераций у нас стало больше, с другой стороны, и дендрограмма получилась поразнообразнее, и эффект сохранился. Ну и на еще более большой выборке из 10000 писем мы видим, что дендрограмма уже совсем разрослась, а график по-прежнему имеет точку, в которой начинается особенно быстрый рост. Ну, точнее говоря, не точку, а какой-то регион. Ну вот если мы сравним эти три графика, то мы увидим, что, с одной стороны, действительно, с увеличением выборки график получался глаже, но это, в принципе, ожидаемо, а с другой стороны, что любопытно, вот этот вот изгиб на графике возникал при разном расстоянии между сливаемыми кластерами. Чем же это может быть вызвано? С одной стороны, можно подумать: «Ну конечно, у нас больше кластеров, поэтому следующие слияния уже, определенно, при существенно большем расстоянии, нежели первые». Это все, конечно, правда. Но, в каком-то смысле, здесь тогда настораживает, что получается, прошлые изгибы потерялись на графике. Но есть еще другое объяснение. Другое объяснение связано с тем, что мы использовали разные подвыборки, и в этих подвыборках была разная лексика, а в качестве признаков здесь мы использовали частоты слов. Поэтому фактически признаковые пространства здесь разные, количества признаков тоже разные, и поэтому эти расстояния между собой сравнивать некорректно. Было бы это делать корректно, если бы мы сначала получили весь словарь, а потом бы уже делали кластеризацию. Другая интересная особенность иерархической кластеризации в том, что часто возникает какой-то один большой кластер и несколько кластеров маленьких. Это не всегда хорошо, иногда хочется выделить какие-то более-менее похожие по размеру кластеры. Здесь можно начать придумывать какие-нибудь ухищрения. Ну например, можно подумать, что слишком много шума в наших признаках и попробовать понизить размерность. Ну например, сделать SVD, которое мы уже использовали в одном из примеров в прошлом уроке и о котором еще поговорим подробнее в следующих уроках. Ну вот мы видим, что, если понизить размерность с помощью SVD, дендрограмма выглядит вроде бы разнообразнее, хотя это все еще зависит от того, в какой момент отсекать. Если еще больше уменьшить количество компонентов, то кажется, гигантская компонента становится меньше. Но в то же время не стоит забывать анализировать результаты по-разному. Ну, в частности, можно снова построить график, который мы строили ранее, и увидеть, что здесь уже перегиб совершенно пропал, и, возможно, мы уж слишком сильно понизили размерность, и теперь уже явно кластеры не выделяются. Итак, подведем итог. Мы с вами поговорили про иерархическую кластеризацию; о том, как устроена агломеративная кластеризация; как можно вводить расстояния между кластерами, в том числе с помощью формулы Ланса-Уильямса; как можно строить дендрограммы; и обсудили некоторые примеры использования кластеризации на реальных датасетах.

[ЗАСТАВКА] Итак, узнав про задачу кластеризации, мы бросились обсуждать разные интересные методы кластеризации. В то же время, возможно, можно придумать что-нибудь очень простое. Сейчас мы поговорим именно об этом. Итак, мы вспомним, что такое связные компоненты в графе и придумаем, как можно решать задачу кластеризации с помощью выделения связных компонент. После этого мы поговорим про минимальное остовное дерево, вспомним алгоритм Крускала и разберемся с кластеризацией с помощью минимального остовного дерева. Итак, начнем. Связные компоненты — это подграфы в графе, которые обладают свойством связности, и в то же время никакие вершины из графа нельзя добавить в этот подграф, сохранив свойства связности. Что такое связность? Связность — это свойство, заключающееся в том, что из любой вершины графа можно попасть в любую другую вершину графа по ребрам. Таким образом, часто встречается ситуация, когда граф состоит из нескольких связных компонент, и может быть полезно выделить эти связные компоненты. Это поможет нам и для кластеризации. Давайте просто соединим ребром те объекты, расстояние между которыми меньше какой-то зафиксированной константы R, и дальше в полученном графе выделим компоненты связности. Если граф сразу получился связным, то есть компонента связности одна, то тогда можно взять R поменьше. Однако непонятно, как же нам выбрать R, если нам нужно какое-то конкретное число кластеров, ну, например, K кластеров. Здесь больше подойдет другой простой графовый подход. Минимальным остовным деревом в графе называется такой связный граф без циклов, ну то есть такое дерево, что все вершины этого графа являются вершинами исходного графа, все ребра тоже являются ребрами исходного графа, но при этом все вершины исходного графа задействованы. Вот если мы хотим построить минимальное остовное дерево, нам будет очень полезен алгоритм Крускала, или Краскала, в зависимости от того, как вам больше нравится. Изначально множество уже найденных ребер будет пустым. Затем на первом шаге мы добавим ребро с минимальным весом. Мы рассматриваем случай взвешенного графа, потому что наша задача — получить именно минимальное остовное дерево, то есть дерево, для которого суммарный вес ребер будет минимальным, и в то же время это будет остовное дерево. На каждом шаге мы добавляем ребро, одна из вершин которого уже принадлежит множеству выбранных вершин, а другая еще не принадлежит. И при этом среди всех таких ребер вес у добавляемого ребра должен быть самым маленьким. В тот момент, когда у нас задействованы все вершины графа, мы получаем остовное дерево, и оказывается, можно доказать, что это будет минимальное остовное дерево. Легко придумать, как решить задачу кластеризации с помощью остовного дерева. Давайте построим взвешенный граф, в котором веса ребер — это расстояния между объектами. Построим минимальное остовное дерево для этого графа и просто удалим K − 1 ребро с максимальными весами, ну то есть с максимальным расстоянием. В итоге мы получим K компонент связности, и именно их и будем интерпретировать как кластеры. Итак, мы с вами вспомнили, что такое связные компоненты, и придумали, как решать задачу кластеризации с их помощью. Также мы поговорили про минимальное остовное дерево, алгоритм Крускала и кластеризацию с помощью минимального остовного дерева.

[ЗАСТАВКА] В этом видео мы поговорим про кластеризацию на основе плотности точек. То есть density-based кластеризацию. Сначала мы поговорим об идее этих методов, затем рассмотрим пример основных граничных и шумовых точек, вспомним, как работает алгоритм DBSCAN, посмотрим на примеры его работы, поговорим про определение числа кластеров в DBSCANе, насколько оно может быть верным или неверным. Ну и, наконец, обсудим, как же подобрать параметры в DBSCANе. Итак, идея density-based методов, мы уже с вами обсуждали, заключается в том, чтобы рассматривать плотность точек в окрестности каждого объекта выборки. Если в некоторой окрестности некоторого заданного радиуса точек много, ну то есть больше какого-то, опять же, заданного количества, радиус и количество точек — это параметры алгоритма, то мы будем говорить, что эта точка основная. Если точек меньше, чем нужно, но в то же время в окрестность попадает основная точка, мы будем говорить, что эта точка граничная. Ну и если в окрестности очень мало точек, и никакой основной точки туда не попадает, мы будем говорить, что эта точка шумовая. Ну вот сейчас на слайде изображён пример, в котором точки классифицированы на шумовые (это точка N), граничные точки (B и C) и основные точки. Что происходит в алгоритме DBSCAN? Сначала мы помечаем все точки как основные, пограничные или шумовые. Затем мы отбрасываем шумовые точки. Ну то есть не рассматриваем их в дальнейшей кластеризации. После этого мы можем соединить точки, находящиеся на расстоянии меньше заданного радиуса окрестности одна от другой. Таким образом, мы получим некоторый граф. Дальше мы можем объединить каждую группу соединённых основных точек в отдельный кластер, ну то есть выделить связанные компоненты в получившемся графе. Дальше каждую пограничную точку можно назначить одному из кластеров, ассоциированных с одной из основных точек, находящихся в окрестности пограничной. Ну и вот результаты работы этого алгоритма могут быть довольно впечатляющими. То есть на примере, который вы сейчас видите на слайде можно хорошо увидеть, что DBSCAN хорошо справляется с нетривиальными формами кластеров: кластерами-лентами, вложенными друг в друга, какими-то отдельными частями кластерами, и в то же время успешно отделяет шумовые точки, которые могли бы сильно испортить работу других алгоритмов кластеризации. В то же время не стоит думать, что DBSCAN прямо-таки идеально определяет количество кластеров. Всё это, конечно, зависит от того, какой радиус рассматривать в окрестности, но в каких-то случаях всё работает хорошо, а в каких-то случаях мы можем совершенно неправильно определить количество кластеров. Ну, например, на картинке, показанной сейчас на слайде, вы видите ситуацию, когда кластеров явно 4, но они достаточно хорошо перетекали друг в друга, и поэтому три кластера слились воедино. В то же время, если бы один из этих кластеров был чуть подальше, то этого бы не произошло. Ну и наконец возникает вопрос: как же понять, какое количество точек достаточно, чтобы считать, что плотность точек в окрестности велика? И какого радиуса брать окрестность? Для этого можно построить следующий любопытный график. Давайте по оси x расположим количество точек, отсортированных по расстоянию от этих точек до их k-того соседа. Ну, например, четвертого соседа, третьего, пятого. Зафиксировав число k и отсортировав точки по расстоянию до k-того соседа, мы можем построить график зависимости этого расстояния от номера точки в отсортированном массиве и увидеть интересную вещь. Оказывается, что у хвоста массива, то есть у тех точек, для которых расстояние до k-того соседа очень большое, будет резкий рост этого расстояния. Что это означает? Это означает, что мы можем выбрать то расстояние, которое соответствует началу этого резкого роста, в качестве радиуса окрестности. А число k ну, в случае этого графика 4, выбрать в качестве количества точек, которые должны быть в окрестности, чтобы сказать, что точка, в окрестность которой мы смотрим, основная. Поперебирав разные k, мы можем получить разные графики. А дальше посмотреть на то, как много точек мы готовы сделать шумовыми, потому что хвост вот этого отсортированного по расстоянию массива, будет в основном представлен шумовыми точками. Итак, мы вспомнили основную идею методов, которые используют понятие плотности точек. Мы рассмотрели примеры основных граничных и шумовых точек, вспомнили алгоритм DBSCAN и посмотрели, как он может работать на разных модельных данных. Обсудили вопрос определения числа кластеров DBSCAN и вопрос настройки параметров DBSCAN.

[БЕЗ_ЗВУКА] Это видео завершает тему кластеризации в нашем курсе. Давайте снова поговорим об оценке качества и обсудим некоторые рекомендации по решению задачи кластеризации. Сначала мы вспомним про внутрикластерные и межкластерные расстояния и про то, как можно использовать их вместе для оценки качества кластеризации. Затем познакомимся с некоей новой метрикой — с коэффициентом силуэта, обсудим подбор количества кластеров по этой новой метрике, обсудим проверку наличия кластеров данных и вопрос выбора хороших признаков. После этого мы перейдем к обсуждению метрик, использующих разметку — в частности, это будут полнота и однородность. Ну и также мы поговорим про то, как использовать асессоры для оценки качества кластеризации. Итак, среднее внутрикластерное расстояние устроено очень просто — это просто среднее расстояние между объектами, которые принадлежат одному кластеру. Понятно, что в случае когда есть центры кластеров, можно несколько проще записать аналогичную метрику и просто смотреть на среднее расстояние до центра кластера. Абсолютно аналогично вводится среднее межкластерное расстояние, но объекты теперь берутся из разных кластеров. И если в прошлой ситуации мы хотели минимизировать метрику — ну потому что мы хотели, чтобы кластеры были как можно более кучными, — то здесь, очевидно, метрику нужно максимизировать, чтобы кластеры как можно дальше отстояли друг от друга. И тоже есть простой вариант этой метрики для случая, когда известны центры кластеров. Например, можно мерить расстояние от центров кластера до какого-то общего центра, ну, например, среднего арифметического центра в кластерах. Можно скомбинировать эти функционалы, например, взяв частные, и в таком случае мы будем учитывать значения и одного функционала, и другого. Но так или иначе эти функционалы имеют ряд недостатков. Во-первых, с помощью таких функционалов нельзя подобрать количество кластеров. Казалось бы, было бы очень просто перебрать разные варианты, попробовать кластеризовать на разное количество кластеров и выбрать тот вариант, для которого метрика лучше. Но эти метрики приведут к тому, что самым лучшим вариантом будет отнести каждую точку из выборки к одному кластеру, ну, то есть сделать кластеров по количеству точек, потому что в этом случае среднее внутрикластерное расстояние будет 0 — уж меньше никак не получится. Становится понятно, что нужны какие-то метрики, которые позволят выбирать количество кластеров. Один из вариантов — это коэффициент силуэта. Он определяется формулой, показанной сейчас на слайде, где a — это среднее расстояние от данного объекта — да, сейчас мы определяем коэффициент силуэта для какого-то фиксированного объекта. Так вот, a — это среднее расстояние от данного объекта до всех других объектов из того же кластера. А b — это среднее расстояние от объекта до всех объектов из ближайшего другого кластера. Таким образом, мы предполагаем, что, скорее всего, a будет заметно меньше, чем b, и поэтому коэффициент силуэта в хорошей ситуации будет положительным. Но на деле он может меняться от −1 до 1. Обратите внимание, что в некоторых ситуациях, например, когда объект находится близко к границе кластера и рядом есть другой небольшой кластер, коэффициент силуэта для каких-то точек может получаться особенно маленьким, потому что, действительно, свой кластер получился очень большим, среднее расстояние до объектов из него тоже получается большим, а среднее расстояние до объектов из другого кластера получается маленьким просто за счет того, что он маленький. Такая ситуация может быть, но она не должна вас сильно смущать. Если говорить о коэффициенте силуэта для кластеризации в целом, то можно использовать его по-разному — ну, например, считать среднее значение по всем кластерам, и вот на тех графиках, которые сейчас мы будем рассматривать, это среднее значение отмечено пунктирной линией. А можно считать количество точек с разным значением коэффициента силуэта в каждом кластере. Такие графики построены отдельно для каждого кластера тоже на слайде. Оказывается, с помощью подобных графиков можно выбрать некоторое адекватное количество кластеров. Ну, например, сейчас вы видите, что в обоих кластерах — сейчас мы пробуем выбрать два кластера — коэффициент силуэта бывает более высоким, чем среднее значение по всей выборке. Кроме того, мы видим, что по кластерам разброс этого значения не очень большой. Хотя, конечно, у нас тут только два кластера, и говорить о разбросе довольно сложно, но в то же время изменение не очень велико. При этом саму выборку вы видите на слайде справа. Если попробовать три кластера, то разброс уже становится довольно большим. Действительно, в одном кластере коэффициент силуэта скорее большой, в другом кластере — какие-то средние значения, а в третьем кластере — вообще небольшие значения. И при этом в третьем кластере все значения оказались меньше среднего всей выборки. В этом случае мы считаем, что кластеризация выглядит не очень убедительно и такое количество кластеров лучше не выбирать. В случае четырех кластеров опять же разброс значений по кластерам получается не очень большой — во всех кластерах коэффициент получается больше, чем средний по всей выборке, и выглядит это очень даже хорошо. В чем мы можем убедиться, посмотрев на визуализацию прямо на выборке. Конечно, в реальных задачах мы часто сталкиваемся с ситуацией, когда признаков очень много и вот так просто визуализировать на плоскости не получается. И в этом случае такой метод особенно полезен — он позволяет хоть как-то сориентироваться, какое же количество кластеров взять. Если мы попробуем пять кластеров, то видим, что разброс по кластерам становится еще больше и опять же есть кластеры, у которых коэффициент меньше среднего значения по всей выборке для всех точек. И в случае шести кластеров опять же получаем довольно большой разброс. Кроме того, нас может заинтересовать вопрос: а есть ли вообще кластеры в этой выборке? Один из способов это проверить мы уже использовали в прошлых видео, а именно строили график расстояний, для которых происходит слияние кластеров от номера итераций. Такой график можно построить для случая агломеративной иерархической кластеризации. Но что делать в произвольном случае? В произвольной ситуации можно сделать так: давайте сгенерируем p случайных точек из равномерного распределения и возьмем p случайных точек из нашей выборки. Дальше для каждой точки посчитаем расстояние до ближайшего соседа. Для случайных точек — до ближайшей случайной, для точек из выборки — до ближайшей точки из выборки. Затем мы можем посчитать следующую величину: сумма расстояний до ближайших соседей для точек из выборки делить на такую же сумму, плюс сумму расстояний до ближайших точек среди случайных точек. Понятно, что если расстояния до ближайшего соседа для точек из выборки будут такие же, как для случайно сгенерированных, эта ситуация не очень хороша. Значит, у нас выборка более-менее равномерно заполняет пространство признаков. Если же расстояние до ближайшего соседа получается небольшим для настоящих точек, это означает, что точки как-то группируются, и в этом случае статистика получится вблизи нуля. В противном же случае статистика будет близкой к 1 / 2. Кроме того, нам нужно уметь выбирать какие-то хорошие признаки для задачи кластеризации, но сейчас мы это делать не можем — все метрики, которые мы обсудили, используют расстояние, а расстояние зависит от того, какие признаки мы выбираем. В то же время хотелось бы уметь понимать, что на этих признаках задача решается с таким-то качеством, а на других признаках — с другим качеством, и получать числа, которые можно между собой сравнивать, для того чтобы мы могли выбрать какие-то адекватные признаки. Здесь мы приходим к идее использования некоторой разметки — ну, то есть если нам известно, к каким классам можно было бы отнести объекты, но этой разметки недостаточно для того, чтобы обучить классификаторы. Мы можем использовать эту разметку, для того чтобы оценивать качество кластеризации. Например, мы можем использовать все ту же метрику точности (accuracy), которая была в классификации, только попробовав по-разному сопоставить кластеры и классы, как мы это делали в примере про кластеризацию текстов. Другие подходы — это однородность, полнота и V-мера. Однородность будет максимальна, если кластер состоит только из объектов одного класса. Полнота будет максимальной, если все объекты из класса принадлежат только к одному определенному кластеру. Если вводить формально эти понятия, то однородность будет представлять собой величину, равную единице минус отношения энтропий класса при условии кластера и энтропии класса. А полнота — абсолютно симметричную величину, только теперь класс и кластер меняются местами. V-мера — это просто средняя гармоническая однородности и полноты. Давайте разберемся чуть подробней с этими энтропиями. Как мы знаем, энтропия — это мера неопределенности, мера нашего незнания о том, какая же конкретно реализация случайной величины будет. То есть в случае отношения энтропии класса при условии кластера и энтропии класса мы смотрим, добавляются ли какие-то знания о том, какие классы следует ожидать, в случае когда мы знаем, что объект относится к этому кластеру. Если кластеры идеально совпадают с классами, мы получим в этой дроби 0 и получим идеальное значение — единичку. Аналогичные рассуждения можно провести и для полноты. Теперь подробнее о формулах. Энтропия для распределения с вероятностями p1, ..., pn обычно представляется просто как минус сумма p умножить на логарифм p по всем вероятностям. Поэтому энтропия для класса вычисляется следующим образом: нужно просто взять и расписать такую сумму по всем классам, а в качестве вероятности у нас будет выступать вероятность класса, то есть количество точек, отнесенных к этому классу в разметке, делить на количество точек вообще. В случае энтропии класса при условии кластера нам нужно будет использовать просто условную вероятность класса при условии кластера. Ну и наконец, если у вас нет готовой разметки, у вас есть уж как минимум три выхода. С одной стороны, вы можете пытаться использовать метрики без разметки, но, как мы понимаем, не всегда это нас устраивает. С другой стороны, мы можем попытаться создать разметку с помощью людей, специально привлеченных для этого — таких людей обычно называют асессорами, — и использовать эту разметку. Но тогда нам нужно сразу подумать о том, какие кластеры должны быть в выборке. Можно задать более простой вопрос — а какие объекты похожи, а какие — нет? И предложить асессорам отвечать на вопросы вида: допустимо ли отнести эти объекты в один кластер? Или допустимо ли отнести эти объекты в разные кластеры? И, используя их ответы на уже готовой кластеризации, оценить ее качество. Подводя итог, мы с вами поговорили про средние внутрикластерные и межкластерные расстояния, познакомились с коэффициентом силуэта, научились подбирать количество кластеров, используя этот коэффициент, выяснили, как можно проверять наличие кластерной структуры и какие метрики можно использовать в случае, если нам нужно подобрать хорошие выборки. В частности, познакомились с полнотой, однородностью и V-мерой. И, наконец, поговорили о привлечении асессоров к оценке качества кластеризации.