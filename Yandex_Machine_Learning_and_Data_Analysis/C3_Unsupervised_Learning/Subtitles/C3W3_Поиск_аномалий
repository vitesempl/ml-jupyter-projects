Мы начинаем урок, посвященный задаче обнаружения аномалии. И в данном видео мы обсудим, что это вообще за задача, и чем она отличается от остальных задач обучения без учителя. Вы уже знаете несколько примеров задач поиска структуры данных или обучения без учителя. Например, задача кластеризации, где нужно найти такие группы объектов, что объекты внутри одной группы в некотором смысле похожи друг на друга. Например, можно пытаться кластеризовать клиентов мобильного оператора. Или, скажем, задача понижения размерности, где есть некоторая выборка и нужно спроецировать ее в пространство меньшей размерности так, чтобы сохранить как можно больше информации. То есть найти какие-то направления, которые наиболее информативны для данной выборки. Задача поиска или обнаружения аномалии немного отличается. В ней нужно найти в выборке объекты, которые не похожи на большинство объектов, объекты, которые выделяются, которые являются аномальными. При этом примеров аномалии либо нет вообще, либо их очень мало, и мы даже не знаем, где именно в выборке они находятся. Именно поэтому задача относится к обучению без учителя. У нас нет никакой разметки. Таким образом нам нужно как-то научиться понимать, похож ли новый объект, который к нам приходит, на остальные, на те, которые мы видили до этого. Измерять похожесть можно по-разному. Мы будем далее обсуждать то, как именно это можно делать. Но сначала давайте обсудим несколько примеров применения обнаружения аномалий. Первый пример про клиентов банка. Представьте, что каждый объект — это клиент банка в определенный момент времени. Мы можем его описывать, например, характеристиками его транзакций в данный момент времени, тем, как он ведет себя в интернет-банке и так далее. Собственно, вопрос, на который мы хотим отвечать: не является ли его поведение каким-то необычным, не является ли оно аномальным. Если его поведение выбивается из среднего по всем клиентам, то это повод заподозрить, что клиент делает что-то не так. Возможно, это мошенник, который украл карту и пытается вывести с нее деньги. И это будет повод, например, заблокировать карту и позвонить клиенту банка. Другой пример про мониторинг сложной компьютерной системы, которая состоит из большого количества взаимосвязанных машин. Мы можем отслеживать много показателей: загрузку процессоров, использование памяти на каждой машине, нагрузку на сеть и так далее. А вопрос, на которой хочется отвечать, — это отличается ли текущее состояние системы от характеристик тех состояний, которые мы видели до этого и про которые знаем, что они нормальные. Если отличается, если система ведет себя как-то иначе, не так, как раньше, то опять же это повод задуматься, не случилась ли какая-то поломка, не нужно ли продиагностировать систему и что-то починить в ней. Наконец третий пример состоит в следующем. Представьте, что мы сделали модель, которая по отзыву на банк определяет его тональность: позитивную или негативную, хорошо или плохо клиент отозвался о банке. То есть процедура следующая: клиент банка заходит на сайт этого банка, в специальную форму вводит некоторый отзыв, и дальше этот отзыв приходит нашей модели, которая определяет позитивный он или негативный. Если он негативный, то нужно что-то с этим делать. Нужно сообщить об этом сотрудникам банка, чтобы они могли как-то решить возникшую проблему. Но нам также хотелось бы понимать, когда приходит новый отзыв, можем ли мы вообще применять к нему модель машинного обучения, которую построили. Можем ли мы его классифицировать этой же моделью. Дело в том, что распределение признаков, описание этого объекта могло измениться. Например, банк мог поменять название продуктов, и поэтому теперь слова, которые употребляют в отзыве, совершенно другие. Модель к ним не готова. Или, скажем, банк мог поменять ограничение на длину отзыва. До этого, скажем, длина была не ограничена, и клиенты писали довольно длинные тексты. А теперь длину ограничили, скажем, 1000-ой символов. То есть отзыв должен быть очень коротким. В этом случае понятно, что объекты станут совершенно другими. Клиенты будут стараться максимально сжато объяснить свою проблему. И из-за этого тоже модель может стать непригодной. Если мы бы могли обнаруживать, что объекты, которые к нам приходят, стали другими, стали аномальными по сравнению с теми, на которых мы обучали модель, то это повод обучить модель заново, уже на новых размеченных данных. Мы обсудим 2 подхода к обнаружению аномалий. Первый основан на восстановлении плотности. То есть мы находим некоторое распределение, которое хорошо описывает имеющуюся выборку. И дальше, когда приходит новый объект, мы смотрим, насколько вероятно было его получить из этого распределения. Чем меньше эта вероятность, тем больше шансов, что это аномалия или выброс. Второй подход — это методы, основанные на методах классификации. Об этом будем говорить в последнем видео этого урока. Итак, мы обсудили, в чем заключается задача поиска аномалии. В ней нужно уметь обнаруживать объекты, которые существенно отличаются от тех, которые мы видели раньше, от тех, которые были в обучающей выборке. И рассмотрели пару примеров. Например, про поиск мошенников в банке или про детектирование непригодности модели машинного обучения. В следующем видео мы начнем говорить о методах и начнем с параметрических методов восстановления плотности.

В этом видео мы поговорим о параметрических методах восстановления плотности и о том, как с их помощью решать задачу обнаружения аномалий. Как мы уже обсудили, задача обнаружения аномалий — это про то, как найти объекты, которые не похожи на большинство. Скажем, на этой картинке белые объекты — это те, которых большинство и которые похожи друг на друга, а черные — это те, которые явно выбиваются из большинства, наверняка это некие аномалии. В вероятностном подходе к обнаружению аномалий мы считаем, что аномалия — это такой объект, который был получен из другого распределения, не из того распределения, которое сгенерировало нам основную обучающую выборку. При этом возникает вопрос о том, а как найти распределение, из которого выборка была сгенерирована. Если мы ее найдем, то сможем на основе этого распределния оценивать вероятность того, что новый объект пришел из него же или из какого-то другого распределения. Если вероятность получить новый объект из этого распределения очень мала, то, скорее всего, это аномалия. Есть три основных подхода к восстановлению вероятностных плотностей — это параметрический подход, непараметрический подход и восстановление смесей. Про параметрический подход и восстановление смесей мы поговорим в этом видео, а про непараметрический — уже в следующем. Итак, мы считаем, что существует некоторое вероятностное распределение p(x) на наших объектах — на всех объектах, которые мы можем получить. В параметрическом подходе считается, что это распределение является параметрическим, то есть на самом деле это некое распределение φ от x, которое зависит от некоторого параметра θ. Один из самых ярких примеров параметрического семейства распределений — это нормальное распределение. Как вы знаете, нормальное распределение состоит из двух параметров — μ и Σ, центр и ковариационная матрица. По форме нормальное распределение похоже на шляпу, при этом параметр μ определяет, где находится центр этой шляпы, а параметр Σ определяет разброс этой шляпы, то есть то, насколько она сосредоточена вокруг центра или размазана по всему пространству. Таким образом, если мы пользуемся нормальным распределением и пытаемся смоделировать с его помощью нашу выборку, то нам нужно как-то найти два параметра — μ и Σ. Как это делать? Было бы логично искать параметр распределения так, чтобы вероятность объектов обучающей выборки с точки зрения этого распределения была максимальна. В этом случае объекты, которые не похожи на эту выборку, скорее всего, будут получать низкие вероятности. Собственно, именно так и работает метод максимального правдоподобия, который старается подобрать такое распределение из параметрического семейства, что с его точки зрения объекты обучающей выборки будут как можно более вероятны. Собственно, формально эта задача записывается вот так. Работать с самим правдоподобием неудобно, поскольку это — произведение значений плотности во всех точках обучающей выборки. Вместо этого мы берем его логарифм и получаем такую задачу: логарифм... сумма логарифмов значений плотности во всех точках обучающей выборки. И эту сумму мы пытаемся максимизировать. Для некоторых распределений эту задачу можно решить аналитически, если посчитать частные производные и приравнять их к 0. Например, для нормального распределения такие решения существуют. Оказывается, что параметр μ оценивается просто как среднее по всем объектам выборки, а параметр Σ вычисляется как выборочная ковариационная матрица для этой же выборки, которая считается вот по такой формуле. Таким образом, если мы выбираем нормальное распределение, если мы считаем, что выборка действительно пришла из него, то нужно просто вычислить параметры этого распределения вот по этим формулам — совершенно ничего сложного. Вот пример: есть некоторая выборка, которая... в общем-то похоже, что она пришла из нормального распределения, поскольку ее форма похожа на эллипс. Если мы подгоним методом максимального правдоподобия нормальное распределение под эту выборку, то оно будет выглядеть так, как показывают линии уровня. Внутри синей линии уровня констатируется 90 % всей вероятности, внутри зеленой — 95 %, внутри красной линии уровня — 99 % распределения вероятности нормального распределения. Таким образом, вероятность получить объект вне красного эллипса очень мала. При этом в нашей выборке есть две точки, которые находятся вне него. Поскольку основная выборка очень хорошо описывается этим нормальным распределением, а те две синие точки очень плохо им описываются, то можно предположить, что они пришли из другого распределения, что это как раз таки аномалия. Собственно, мы уже описали алгоритм поиска аномалий, если у нас уже есть некоторое распределение на объектах. Итак, допустим, мы нашли распределение p(x), и к нам приходит новый объект x. Мы вычисляем его вероятность осуществления, этого распределения — p(x) — и сравниваем с некоторым порогом t. Если вероятность меньше этого порога, то мы объявляем этот объект аномалией. При этом встает вопрос о том, а как вообще выбирать порог t. Здесь нет каких-то однозначных рекомендаций — можно, например, выбирать его из априорных соображений, то есть вот на прошлом слайде мы рассуждали, что внутри линий уровня находится 99 % распределения, и мы верим в то, что все нормальные объекты будут там расположены. А все, что вне — объявляем аномалиями. Или, например, если у вас откуда-то есть размеченные примеры аномалий, то есть у вас есть несколько объектов выборки и вы точно знаете, что они аномальные, то можно подобрать порог t так, чтобы эти объекты были объявлены аномальными, а все остальные объекты считались нормальными, считались сгенерированными из нашего распределения p(x). В некоторых случаях параметрического подхода оказывается недостаточно. Скажем, на этой выборке, на этой картинке нарисована выборка, которая на самом деле сгенерирована из двух распределений. Эти оба распределения являются нормальными, у них одинаковые матрицы ковариаций, но разные центры. Таким образом, получается два облака точек: левое и правое. Синее облако сгенерировано из одного нормального распределения, красное — из другого. Объяснить эту выборку одним нормальным распределением будет невозможно, зато отлично подходит модель смеси распределений. Вы уже познакомились с этой моделью и ее настройкой, когда говорили про кластеризацию с помощью EM-алгоритма. Собственно, для восстановления плотности EM-алгоритм тоже отлично подходит. Напомню, что смесью называется такое распределение p(x), которое представляется в виде взвешенной суммы других распределений pj(x). pj(x) называются компонентами смеси, и, как правило, они уже являются параметрическими распределениями. Например, каждая из них — это нормальное распределение, но у каждого — свои параметры. Собственно, каждая компонента pj является членом параметрического семейства φ(x) со своим параметром θj. EM-алгоритм состоит из повторения E-шага и M-шага до тех пор, пока не будет достигнута сходимость. На E-шаге мы вычисляем апостериорные вероятности того, что i-тый объект принадлежит j-той компоненте смеси. И это делается вот по такой формуле. Она выражается через веса компонент, через значения вероятности в каждой компоненте и нормируется на общее значение вероятности p(x). На M-шаге мы используем эти апостериорные вероятности, чтобы обновить наши оценки на параметре θ. И эти оценки вычисляются путем решения задачи максимизации взвешенного правдоподобия, то есть здесь формула такая же, как в методе максимума правдоподобия, но только перед каждым слагаемым стоит вес gji, где gji — это те самые апостериорные вероятности. И эти шаги повторяются до тех пор, пока значения параметров θj не станут меняться слишком слабо. Благодаря этому алгоритму можно подогнать смесь из K- компонент, из K-распределений, под нашу выборку. И понятно, что такая модель распределения — гораздо более сложная, чем, например, попытки приблизить всю выборку одним нормальным распределением. Итак, мы обсудили вероятностный подход к обнаружению аномалий, в котором считается, что аномалия — это такой объект, который приходит из другого вероятностного распределения. И обсудили параметрические методы восстановления распределения, в частности восстановление с помощью метода максимального правдоподобия и восстановление с помощью смесей, с помощью моделирования выборки смесью распределений. После того как мы имеем распределения, описывающие нашу выборку, мы можем очень легко обнаруживать аномалии путем вычисления вероятности нового объекта. Чем она меньше, тем больше шансов, что этот объект аномальный и не похож на большинство других объектов. В следующем видео мы продолжим разговор о вероятностных методах и поговорим про непараметрическое восстановление плотности.

В этом видео мы продолжим разговор о вероятностном подходе к обнаружению аномалий и поговорим про непараметрические методы восстановления плотности. В прошлом видео мы говорили про параметрический подход, когда мы вводим некоторое параметрическое семейство распределений, например, нормальное, и пытаемся подобрать такие его параметры, при которых объекты обучающей выборки получают наибольшие возможные вероятности. Это называется методом максимального правдоподобия. Непараметрический подход имеет немножко другую идею. Мы не будем вводить никаких семейств распределений, а будем пытаться восстановить вид распределения, вид плотности, глядя непосредственно на данные. Давайте рассмотрим это на одном простом примере. У нас есть одномерная выборка, объекты которой обозначены черными засечками по оси X. Этих объектов шесть штук. Видно, что здесь, скорее всего, две компоненты. Одна компонента слева имеет три или четыре точки, вторая компонента справа имеет две точки. Поместим с центром в каждой точке обучающей выборки небольшую гауссиану, которая присваивает всем точкам на оси некоторые вероятности. Чем ближе точка на оси к данной точке обучающей выборки, тем больше будет вероятность. А далее, в каждой точке просуммируем эти гауссианы. Тогда мы получим вот такую оценку на плотность распределения. Видно, что она вполне адекватная. Она имеет пики с центрами в этих двух компонентах, и чем меньше объектов концентрируется в данной точке, тем ниже будет там значение плотности. Формально это можно сделать с помощью формулы Парзена-Розенблатта, которая записывается вот так. Такая непараметрическая оценка на плотность распределения — это по сути сумма вот чего. В каждой точке обучающей выборки xi мы вычисляем разность между новым объектом, в котором мы хотим оценить плотность x, и данной точкой обучающей выборки, x − xi, и делим на ширину окна, на некоторый параметр h, про который мы поговорим чуть позже. Далее от этой дроби мы вычисляем ядро K, которое по сути характеризует вероятность того, что эти точки похожи друг на друга. По идее, чем дальше точка x от точки xi, тем меньше должно быть значение этого ядра K. Дальше мы просто усредняем значения этих ядер по всей обучающей выборке и еще делим на ширину окна h. Итак, у нас K — это некий параметр метода, это ядро. Ядром называется четная функция, то есть функция, симметричная относительно оси ординат, и к ней еще одно требование: интеграл от этой функции должен быть равен 1. Если это требование не выполнено, то мы будет получать ненормированные плотности, плотности, интеграл под которыми не равен 1. Кстати, здесь, как вы уже поняли, мы говорим про одномерные выборки, То есть x и xi — это просто вещественные числа. Ядра можно выбирать самые разные. Здесь нарисованы некоторые из них и представлены формулы для этих ядер. Например, мы уже с вами употребляли гауссовское ядро, которое обозначено красной кривой и, собственно, записывается как гауссиана через экспоненту от минус квадрата аргумента. Еще один параметр оценки Парзена-Розенблатта — это ширина окна h. Давайте посмотрим, на что она влияет. Как я уже говорил, чем больше значение аргумента ядра, тем меньше будет значение самого ядра, потому что чем больше расстояние между этими точками, тем меньшую вероятность мы хотим приписать. Соответственно, если мы делим разницу между x и xi на какое-то очень маленькое число h, то значение дроби будет большим. Таким образом, если ширина окна маленькая, то даже точки x, близкие к данной точке xi, будут получать низкие значения вероятности, низкие значения ядра. Если же значение h большое, то даже если разность x − xi большая, то мы поделим ее на большое число, и в итоге вероятность, которую присвоим, будет не такой уж маленькой. Давайте посмотрим это на примере. Есть некоторая выборка, которая обозначена синими засечками, которая сгененерирована из нормального распределения. И красной, черной и зеленой кривыми показаны непараметрические оценки с гауссовским ядром для разных значений ширины окна. Красная кривая соответствует очень маленькой ширине окна. Из-за того, что ширина окна маленькая, эта плотность очень чувствительна ко всем точкам. Даже если в каком-то месте есть всего одна точка обучающей выборки, плотность будет иметь всплеск. Она получается не очень гладкой и скорее всего переобученной. Черная кривая соответствует чуть более высокому значению ширины окна. Видно, что оно скорее всего оптимально. Эта плотность очень неплохо восстанавливает нормальное распределение, из которого эти данные были сгенерированы. Зеленая кривая соответствует оценки плотности с очень большой шириной окна. Из-за того, что ширина окна большая, эта плотность получается с очень большой дисперсией. Даже точки на прямой, которые очень далеки от объектов обучающей выборки, получают довольно большое значение плотности из-за того, что окно очень широкое. Данный подход с непараметрическим оценивание плотности хорошо обобщается на многомерный случай. Нам нужно всего лишь заменить разность между точкой x и точкой xi на значение метрики для этих двух точек, ρ от x и xi. Ну, например, это может быть евклидова метрика или какая-то другая, которая лучше подходит под данную задачу. Также всю формулу нужно уже делить не на h, а на V от h. Это некоторая нормировочная константа, которая нужна для того, чтобы интеграл от этой плотности все еще был равен 1. На самом деле, у многомерного непараметрического подхода к восстановлению плотности есть одна большая проблема. Давайте рассмотрим двумерную выборку. Если бы эта выборка была спроецирована всего на одну ось, если бы мы оставили только первый признак, то объектов было бы очень много, и мы по ним легко могли бы восстановить плотность, потому что если бы мы разбили ось x на некоторые корзинки, то в каждую корзинку попадало бы довольно много объектов этой выборки. Если мы переходим в двумерное пространство и разбиваем все двумерное пространство тоже на такие корзинки-клеточки, то в каждую клеточку уже попадает в среднем гораздо меньше объектов выборки. Из-за этого оценка плотности, скорее всего, будет более шумной и менее репрезентативной. Понятно, что если мы с таким же количеством объектов будем переходить в еще более высокоразмерные пространства, мы будем получать еще менее надежные оценки плотности. На самом деле, число объектов, которое необходимо для того, чтобы качественно оценить плотность непараметрическим методом, растет экспоненциально по мере роста размерности. То есть если, например, мы хотим восстановить такую плотность в пространстве размерностью тысяча, нам понадобится очень и очень много объектов. Таким образом, подход с непараметрическим восстановлением, скорее всего, будет хорошо работать только в пространствах не очень высокой размерности. Но если мы все-таки смогли восстановить плотность этим методом, то дальше находить аномалии можно так же, как и с параметрическим подходом. Мы объявляем аномалиями те объекты, вероятность которых меньше некоторого порога t. При этом порог t выбирается либо из каких-то определенных соображений, либо на основе размеченных аномалий. Итак, мы обсудили непараметрический подход к восстановлению плотности, который можно использовать для восстановления очень сложных плотностей с большим количеством пиков. У формулы Парзена-Розенблатта, с помощью которой делается это восстановление, есть два параметра — ядро и ширина окна. Мы обсудили, на что влияют эти параметры и как их можно выбирать. И также мы отметили одну проблему такого подхода. Для того чтобы непараметрически восстановить плотность в многомерном пространстве, может понадобиться очень большая выборка, что не всегда доступно. В следующем видео мы поговорим о немножко другом подходе к поиску аномалий, который основан на классификации выборки с помощью метода опорных векторов.

[БЕЗ_ЗВУКА] В этом видео мы поговорим про одноклассовый SVM — еще один метод обнаружения аномалии в выборке. Мы уже знаем про вероятностный подход к поиску аномалий, у которого есть параметрический и непараметрический способы. В параметрическом мы подгоняем распределение из некоторого семейства под нашу выборку, например из нормального семейства. В непараметрическом мы пытаемся восстановить сам вид распределения, исходя из того, как у нас устроены данные, например с помощью формулы Парзена-Розенблатта. Давайте забудем про этот подход и вспомним, что мы умеем решать задачу классификации. В обычной задаче классификации, бинарной, у нас есть 2 класса, и мы их отделяем друг от друга некоторой прямой. Или, если это пространство высокой размерности, будем разделять их гиперплоскостью. В задаче же обнаружения аномалий у нас тоже есть выборка, но нам нужно построить некоторую кривую, которая отделяет эту выборку от всего остального. И все, что находится вне этой кривой, мы будем объявлять аномалиями, потому что это нечто, что не попадает в множество типичных объектов из выборки. Как можно подружить эти две задачи? Ведь в задаче обнаружения аномалий мы, конечно, должны построить тоже некоторую разделяющую поверхность, но при этом у нас нет примеров аномалий, у нас есть только примеры нормальных объектов, типичных объектов. Это можно решить следующим образом. Будем считать, что у нас есть 2 класса. Первый класс — это нормальные объекты, и к нему относится вся наша обучающая выборка. Второй класс — это аномалии, и будем считать, что аномальным является начало координат. Это довольно странное предположение, и чуть позже мы поговорим о том, как его обойти. А пока будем считать, что есть 2 класса, первый состоит из всех объектов, второй — это начало координат. И теперь мы можем решать эту задачу как задачу классификации: нужно отделить нормальные объекты от аномального, и будем пользоваться самым простым способом разделить их — линейным способом, при этом будем выбирать разделяющую гиперплоскость так, чтобы она давала максимальный размер зазора. То есть мы хотим построить ее так, чтобы вероятность переобучения была минимальный. Это лучше всего делать с помощью метода опорных векторов, или SVM. Задача конкретно для нашего случая, когда мы отделяем всю выборку от начала координат будет выглядеть вот так. Она, в общем-то, похожа на задачу метода опорных векторов. В функционале здесь тоже есть норма вектора весов, есть сумма штрафов за выход за разделяющую полосу, но есть дополнительное слагаемое ρ, и ограничение немного отличается от того, которое было в SVM-е. Мы не будем сильно вдаваться в подробности и обсуждать, почему эта задача устроена именно так — получить ее не очень легко, при этом обратим внимание лишь на параметр ν (ню) этой задачи. У него есть одна интересная интерпретация. Он определяет верхнюю оценку на долю аномальных объектов в выборке. То есть если ν = 0,1, это будет означать, что при обучении данного классификатора мы имеем право 10 объектов обучающей выборки объявить аномальными, оставить их вне нашей разделяющей гиперплоскости. Результат будет выглядеть примерно вот так: есть некоторая выборка, и, допустим, мы выбрали параметр ν так, что 3 объекта могут быть объявлены аномальными. В этом случае выборка будет отделена от тех 3 объектов вот такой гиперплоскостью, что, в общем-то, похоже на правду. Возможно, эти объекты действительно являются аномальными, а синие точки — это типичные объекты, не аномальные совершенно. На самом деле, предположение о том, что 0 — это аномальный объект, очень странное. С чего мы это взяли? Почему именно 0? Например, если выборка центрирована, то это явно будет ошибочный подход. На самом деле, одноклассовый SVM никогда не применяют для ситуации с линейным именно ядром. Можно показать, что ту задачу, которую мы записывали пару слайдов назад, к ней можно выписать двойственную, в двойственной будет зависимость не от самих признаковых описаний, а от скалярных произведений признаковых описаний пар объектов обучающей выборки. И в этом случае можно сделать ядровой переход, как и в обычном методе опорных векторов. В этом случае мы будем строить разделяющую гиперплоскость, но уже в пространстве гораздо более высокой размерности, и это за счет того, что мы заменили скалярное произведение на некоторое ядро K. Типичный пример ядра — это RBF-ядро, которое вычисляется по вот такой формуле, то есть это некоторая гауссиана. Пример применения ядрового одноклассового SVM с RBF-ядром в выборке можно посмотреть на этой картинке. Здесь выборка довольно сложная, состоит из двух компонент. При этом видно, что есть 2 зоны, где объекты действительно концентрируются, и несколько точек, которые сильно выбиваются из этих зон. Если мы применим одноклассовый SVM с RBF-ядром, то он с помощью красной кривой отделит нормальные объекты от аномальных. Видно, что красная поверхность состоит из двух частей, и она выделяет именно те две зоны, где концентрация объектов очень высокая. И все остальные объекты объявляются аномальными, что вполне логично. Итак, мы обсудили, как работает одноклассовый SVM — он пытается отделить нашу обучающую выборку от начала координат с помощью гиперплоскости. При этом такое предположение не очень осмысленное, поэтому гораздо удобнее и логичнее пользоваться ядровым одноклассовым методом опорных векторов, когда мы делаем ядровой переход и уже пытаемся отделить выборку от нуля в некотором спрямляющем пространстве, в некотором пространстве гораздо более высокой размерности, и в этом случае удается получить неплохие результаты.