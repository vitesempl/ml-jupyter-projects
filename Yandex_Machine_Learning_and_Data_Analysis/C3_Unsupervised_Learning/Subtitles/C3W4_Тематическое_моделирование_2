[БЕЗ_ЗВУКА] В этом видео мы рассмотрим вопрос об оценивании качества тематических моделей. Вообще, оценки качества делятся на 2 большие категории: это внутренние критерии и внешние критерии. Внутренние критерии — это те, которые позволяют оценить качество построенной модели по тем матрицам Φ и Θ, которые она дала на выходе, а внешние критерии измеряют качество модели, глядя на то, как она решает ту конечную прикладную задачу, ради которой она, собственно, и создавалась. В этом видео мы рассмотрим внутренние критерии. Перплексия — это очень известная в вычислительной лингвистике мера качества модели языка. В нашем случае моделью языка является условное распределение слов в документах. Эта мера качества тесно связана с правдоподобием. По сути дела, это значение логарифм правдоподобия, усредненное по всем словам всех документов, и к этому значению применяется экспонента. Каков смысл или интерпретация значения «перплексия»? Ну во-первых, если подставить вместо распределения слов в документах равномерное распределение, то мы увидим, что перплексия равна просто мощности словаря. То есть можно сказать, что это мера различности или неопределенности слов в тексте. Если распределение слов неравномерно, то перплексия уменьшается по сравнению с тем значением, которое дает равномерное распределение. Еще можно сказать, что перплексия — это коэффициент ветвления текста, то есть сколько мы ожидаем различных слов после каждого слова в документе в среднем. Вот такая оценка может быть вычислена по самой коллекции, по которой построена тематическая модель, однако тут есть опасность, что произойдет переобучение, и эта оценка будет оптимистично занижена. Вообще, перплексия чем ниже, тем лучше. Чтобы этого эффекта избежать и получить несмещенную оценку, считают перплексию тестовой (или отложенной) коллекции, hold-out perplexity. Она очень похожа на предыдущую, но есть тонкость, что сама тематическая модель строится по одной части коллекции, а вот проверяется эта оценка на другой части коллекции, и по первой части мы строим матрицу Φ, которая общая для всей коллекции, а по второй части мы только тематизируем отдельные документы. Но и тут возникает опасность переобучения, поскольку если мы с помощью модели определим тематический профиль документа, то есть распределение тем в документе, то мы опять-таки будем оценивать модель саму по себе. И здесь прием такой: каждый тестовый документ еще и делится на 2 половинки, и по первой половинке мы оцениваем распределение тем в данном документе, а вот вторая половника уже используется для вычисления перплексии, и таким образом мы получаем честную несмещенную оценку перплексии в тестовой коллекции. Ну а эксперименты на больших коллекциях показывают, что большого различия между перплексией на обучающей и на тестовой выборке, как правило, нету. Точнее, разница-то есть, но если мы с помощью перплексии сравниваем разные модели, то, как правило, качественные выводы о том, какая модель лучше, а какая хуже, сделанные по перплексии на обучающей коллекции и на тестовой коллекции, они примерно одинаковы, и поэтому есть рекомендация на очень больших коллекциях не считать hold-out perplexity и довольствоваться той перплексией, которая получается по основным данным. Перплексию ругают за то, что эта мера качества не очень хорошо интерпретируемая, она показывает лишь то, насколько хорошо построилось матричное разложение, но ничего не говорит о том, насколько тематическая модель будет полезна для конечных приложений или насколько хорошо будут интерпретироваться темы. Поэтому были придуманы меры качества, которые измеряют, насколько темы хороши, понятны в смысле их интерпретируемости для людей-экспертов, и, конечно же, такую оценку можно сделать только с помощью экспертов. Значит, мы просим экспертов рассмотреть темы как последовательности слов, упорядоченные по вероятности слов в каждой теме. То есть эксперты рассматривают тему за темой, смотрят на топовые слова, то есть имеющие наибольшие вероятности в этой теме, и принимают решение: эта тема интерпретируемая или нет; им нравится вот эта вот совокупность слов как набор слов, обозначающих ту или иную предметную область, тот или иной набор понятий; можно ли назвать это целостной темой. Ну критериев много, обычно экспертам дают такую инструкцию, что если вы понимаете, как можно именовать такую тему, дать ей одно название или вы понимаете, что вот по таким словам мог бы быть построен поисковый запрос и получена релевантная поисковая выдача, то вы тогда считаете такую тему интерпретируемой. Это один подход, и, естественно, это субъективные оценки, поэтому приходится привлекать многих экспертов и потом смотреть на то, насколько непротиворечивы их оценки и делать некое среднее. Есть другой метод, он тоже связан с привлечением экспертов, но он чуть более объективный и чуть более легкий для самих экспертов. Это так называемый метод интрузий, когда в список топовых слов каждой темы внедряется какое-то лишнее слово, которое заведомо этой теме не принадлежит, и экспертов просят определить, какое из списка слов данной темы лишнее. Ну это очень похоже на такую детскую задачу, которую иногда предлагают детям в качестве игры, и поэтому она простая, легкая, и люди таким способом могут оценить больше тем в единицу времени. Ну и измеряется доля ошибок, которые допустили эксперты при определении вот этого самого лишнего внедренного слова. Оказалось, что вот эти вот экспертные оценки очень хорошо коррелируют с такой мерой качества темы, как когерентность, которая уже может быть вычислена полностью автоматически без участия людей. Что такое когерентность, или согласованность, темы? Это мера, которая показывает, насколько часто слова, встречающиеся рядом в текстах, оказываются в топах одних и тех же тем. Как определяется эта величина? Мы берем заданное число топовых слов в темах, обычно небольшое, 10 или 20, и смотрим все пары слов, которые оказались в топе темы. Ну если мы взяли 10 слов, нетрудно посчитать, что пар всего будет 45. И мы смотрим по всем документам коллекции, насколько часто эти слова, пары слов вот из этих 45 пар, рядом стоят в документах, и оцениваем, насколько неслучайно это происходит. Для этого используется так называемая поточечная взаимная информация (pointwise mutual information). Что это такое по смыслу? Это величина, которая представляет собой логарифм отношения трех вероятностей. В числителе стоит вероятность того, что мы встретим два слова из заданной пары слов u и v, а в знаменателе стоят вероятности встретить их независимо друг от друга, произведение вероятностей. Поэтому если мы неслучайно часто встречаем данную пару слов именно рядом, то числитель будет намного больше знаменателя, и чем выше величина поточечной взаимной информации, тем выше неслучайность того, что два слова стоят рядом. Ну если эти слова стоят совсем рядом, такие пары слов принято называть коллокациями. Но здесь некое обобщение этого понятия, мы здесь учитываем пары слов, которые могут стоять в некотором окне рядом, обычно берут окно из 10 слов. Вот такая вот мера качества, которая может быть посчитана полностью автоматически. Делались эксперименты в попытке определить, какие же автоматически вычисляемые меры качества наилучшим образом коррелируют с экспертными оценками. Вот сравнивали много мер и оказалось, что именно вот эта когерентность, или согласованность, является такой мерой. Подытожим. Оценки качества тематических моделей делятся на две большие группы: это внутренние (мы их только что рассмотрели), внешние мы будем рассматривать в следующем видео. Основные внутренние меры качества тематических моделей — это перплексия и когерентность. Еще часто используются экспертные оценки, но они связаны с тем, что необходимо привлекать людей. Как правило, людям надо платить немного денег за то, чтобы они делали эту работу, поэтому это некое организационное мероприятие. Но в последнее время для этой работы очень часто используют краудсорсинг, что позволяет делать ее достаточно быстро, эффективно и с минимумом трудозатрат и затрат финансовых.

[БЕЗ_ЗВУКА] В этом видео мы рассмотрим внешние критерии качества тематических моделей. Если внутренние критерии оценивают матрицы Φ и Θ построенной тематической модели, то внешние критерии используют конечную прикладную задачу и пытаются измерить полезность тематической модели для пользователя или для того или иного приложения. Мы рассмотрим 2 основных приложения тематических моделей: это классификация и поиск. Классификация документов — это задача машинного обучения, обучения с учителем. И типичное применение: первое — определить жанр документа (художественный, научный, учебный, рекламный). Такая задача возникает при обработке потоков документов, полученных краулерами из Интернета. Например, мы можем упростить эту задачу до двух классов, если нас интересует только научный контент. Следующий пример задач — это определение тематики сообщения в новостном потоке, например политика, экономика, наука, здоровье, спорт и так далее. Приходящие новости раскладываем по папкам. И есть целый класс задач чуть более сложных — задача категоризации или рубрикаризации, когда есть некая иерархическая структура — папки, подпапки, в которые нужно складывать приходящие документы, например те же новостные потоки. Ну вот здесь приведены 2 примера, что папки высшего уровня — это наука, спорт — такие большие темы, крупные. Наука делится на области науки: физика — в физике могут быть новости про большой адронный коллайдер, в футболе могут быть новости про чемпионат мира, ну и другие какие-то подразделы. И поток новостей нужно разбить на вот такие рубрики и подрубрики, при том что обучающая информация, которая, собственно, используется для построения модели классификации, это результат раскладывания документов экспертами. И мы здесь считаем, что во всех этих типах задачи у нас есть обучающая выборка, в которой эксперты указали, к какой рубрике, жанру, тематике принадлежит каждый документ. И вот эта информация может быть использована для оценивания качества тематической модели. Итак, как же строить классификаторы с помощью тематической модели? Есть тонкий момент, который здесь должен быть объяснен в явном виде. Конечно, мы можем применить наш механизм мультимодальных моделей и использовать две модальности. Первая — это слова или термины, вторая — это классы. Тонкость заключается в том, что у нас чуточку разная модель работает на этапе обучения и на этапе классификации. На этапе обучения на входе мы имеем коллекцию документов. И здесь для каждого документа нам известен не только состав его слов, но еще и множество классов, к которому этот документ относится. И это далеко не всегда один-единственный класс. Например, в задачах рубрикации очень часта ситуация, когда один и тот же документ может быть отнесен в несколько рубрик. Например, он может быть и про футбол, и про экономику. И про адронный коллайдер, и опять-таки про экономику или про политику Евросоюза в отношении адронного коллайдера. И вот такие вот случаи множественной принадлежности документов к классам нам тоже необходимо рассматривать. На выходе мы должны построить тематическую модель, которая описывает распределение слов в каждой теме и распределение классов в каждой теме. Вот по этим двум матрицам мы сможем классифицировать новые документы, для которых исходно никаких классов не известно. На этапе классификации нам на входе подают документ, и подается модель классификации, которую мы построили на этапе обучения. Про этот документ известны только слова, а нам нужно определить, к каким темам принадлежит этот документ. Поэтому первым делом определяем тематику данного документа. Для этого нам достаточно распределения слов в этом документе, а затем мы используем модель, которая была построена на этапе обучения, для того чтобы определить условные вероятности каждого класса для данного документа. И дальше можем разными способами работать с этим распределением классов, чтобы указать, к каким из этих классов документ принадлежит. Теперь, если у нас есть алгоритм, который выдает по данному документу вероятности принадлежности к каждому из классов, то можно разными способами оценивать качество такого классификатора. Ну во-первых, можно задать порог на эту вероятность, считать, что если вероятность больше этого порога, то документ принадлежит к данному классу, и просто подсчитывать число ошибок классификации. Это не очень хорошая мера, поскольку классы могут иметь разную важность и разную мощность или численность. Поэтому чтобы работать с несбалансированными выборками, можно использовать критерии чувствительности и специфичности, о которых мы уже рассказывали в других курсах. Есть также критерий площади под кривой чувствительность-специфичность, который уже нечувствителен к тому, какой именно порог мы выберем для классификации документов. Наконец, есть критерий точности и полноты и, соответственно, площадь под кривой точность-полнота. Вот давайте точность и полноту рассмотрим чуть поподробнее в связи с тем, что у нас, как правило, в задачах категоризации классов много. Как действовать в такой ситуации? Ну напомню, что для того чтобы определить, что такое точность (precision) и полнота (recall), нам нужно определить число верных положительных классификаций (положительными считаются те классификации, которые отнесли данный документ к классу c), ложные положительные классификации (это когда документ отнесен к классу c, но это ошибка, на самом деле в обучающей выборке сказано, что документ не принадлежит к данному классу) и ложные отрицательные. Ложные отрицательные — это когда алгоритм говорит, что данный документ не принадлежит к классу c, а на самом деле он ему принадлежит. И точность определяется как доля релевантных документов среди всех найденных, если считать, что мы вот хотели найти документы класса c, а релевантные — это те, которые действительно класса c. Полнота, или recall, — это, наоборот, доля найденных документов среди релевантных. Это характеристика определена для данного конкретного класса c. Если классов много, как мы поступаем с этими величинами? Есть 2 способа, 2 стратегии. Первый называется «микроусреднение», когда мы суммируем по классам отдельно величины верных положительных, ложных положительных и ложных отрицательных классификаций. Другая стратегия заключается в том, что мы сначала высчитываем точность и полноту для каждого класса, а потом их усредняем. В чем принципиальная разница? В том, что макроусреднение более чувствительно к качеству классификации маломощных классов. Оно потребует от нас примерно одинаковой работы на... одинакового качества работы на всех классах, независимо от их мощности. А при микроусреднении, когда мы складываем число верных или ложных положительных или отрицательных классификаций, то малочисленные классы теряются на фоне многочисленных. Следующая задача, которая часто используется для оценивания качества тематических моделей, — это тематический поиск. Здесь на входе нам также подается коллекция документов в виде матрицы слов в документах, а на выходе строится тематическая модель коллекции. Но если в случае классификации у нас есть еще модальность классов, и нас интересует распределение классов в темах, распределение слов в темах, то в данном случае нас интересует распределение тем в документах. Когда мы делаем тематический поиск, мы ищем документы, которые были в коллекции, по документу, который является поисковым запросом. И те и другие документы должны быть представлены в виде компактного векторного описания, то есть в виде распределения по множеству тем. Итак, на этапе поиска мы имеем входной запрос. Это текст произвольной длины. И это важный момент, который отличает тематический поиск от того поиска, к которому мы все привыкли в поисковых системах типа Google или Yandex. Тематический поиск открывает перед нами новую возможность — искать длинные документы по длинным документам. Итак, первым делом мы должны тематизировать наш запрос, то есть построить для данного запроса q распределение тем в запросе. А далее мы должны посмотреть на все документы нашей коллекции и найти из них те, которые имеют тематические профили, близкие к тематическому профилю нашего запроса, то есть мы будем сравнивать условные распределения тем у запроса документа и у тех документов, которые лежат в нашем поисковом индексе. Как их сравнивать? Как сравнивать два вероятностных распределения? В информационном поиске принято использовать несколько мер — конечно же, их гораздо больше, но эксперименты на многих задачах показали, что наиболее эффективно работают следующие. Косинусная мера — это просто косинус угла между двумя векторами в пространстве размерности t. Первый вектор — это вектор документа, значит, распределения по всем темам, второй — это вектор запроса тоже распределения по всем темам. И просто по известной всем формуле — косинус угла между двумя векторами — вычисляется вот эта мера. Чем она больше, тем ближе друг к другу два вектора. Значение косинусной меры заключено от 0 до 1. Следующая мера, которая тоже принимает значения от 0 до 1, — но это уже функция расстояния, то есть она меряет не близость, а расстояние; чем меньше это значение, тем ближе друг к другу два вектора — называется расстояние Хеллингера. Оно очень похоже на евклидово расстояние, только вместо самих условных вероятностей — координат сравниваемых векторов, — стоят их квадратные корни. Ну и, наконец, KL-дивергенция — тоже очень полезная мера близости вероятностных распределений, но эта мера несимметричная в отличие от двух предыдущих. И здесь мы меряем, насколько запрос подходит под данный документ, насколько он хорошо вкладывается в данный документ. И эта мера, в общем-то, лучше подходит для тех случаев, когда запросы все-таки у нас в среднем короче, чем документы. Например, когда мы по небольшим аннотациям пытаемся найти научные статьи или по кратким резюме пытаемся найти какие-то длинные документы по этим темам. Конечно же, какую из этих мер выбрать — сильно зависит от прикладной задачи и от свойств коллекции, и от того, какие запросы мы собираемся обрабатывать. Еще один момент, который здесь можно упомянуть — это что на косинусную меру и на расстояние Хеллингера может сильно влиять такой прием, как обрезание хвостов распределений. Потому что темы — это все-таки распределения наиболее часто встречающихся слов в темах, и точно так же и документы — это распределения наиболее часто встречающихся тем в документах. Редкие темы нас мало интересуют. И когда строят тематический поиск, часто обнуляют маловероятные темы в документах или, например, берут сколько-то топовых тем, а остальные полагают равными 0. Это иногда повышает качество поиска, а иногда повышает его скорость, потому что получается разреженное распределение. Итак, каким же образом, научившись вычислять меры близости между запросом и документом, дальше оценивать качество тематической модели? Но поиск — это всегда ранжированная поисковая выдача, и оценивать надо список выдачи. Самая простая распространенная оценка — это попытаться оценить точность первых k позиций выдачи. Но для этого нужно привлекать эксперта, который возьмет по каждому запросу просмотрит эти k первых позиций выдачи и отметит, какие из них релевантные, какие нерелевантные. После этого можно будет применить тот самый критерий точности, о котором мы говорили выше. Можно упростить эту оценку и сделать ее полностью автоматической, если искать среднюю позицию документа, в случае когда мы ищем документ по его аннотации или по его коротким фрагментам. Обобщение этого поиска — это когда мы режем документ на фрагменты и пытаемся понять, насколько хорошо одни фрагменты могут быть найдены по другим фрагментам. Чем хорош такой способ организации измерительного эксперимента — тем, что здесь не надо привлекать асессора и здесь мы заранее знаем, какие результаты поиска релевантны, какие — нет. Если мы взяли из документа фрагмент и хотим найти по нему фрагменты того же документа, то мы знаем класс, который мы ищем целиком, и можем оценить как полноту, так и точность. Ну и есть всякие специфические приложения, например кросс-язычный поиск, когда мы по документу на одном языке ищем его переводы или переводы тематически схожих документов на других языках, то в данном случае для организации эксперимента по измерению точности тематической модели в качестве запроса может быть дан документ, а искать мы будем его переводы на другой язык. И средняя позиция в поисковой выдаче — это как раз и есть мера точности поисковой системы, которая построена по тематической модели, и, следовательно, это мера качества тематической модели. Итак, мы с вами рассмотрели как внутренние, так и внешние оценки качества тематических моделей. Внешние оценки связаны с теми или иными приложениями тематических моделей. Мы рассмотрели два, ну пожалуй, основных приложения: это классификация, или категоризация, и второе приложение — это тематический поиск. Но есть также и другие приложения тематических моделей: сегментация текстов, аннотирование текстов, суммаризация, то есть составление кратких выжимок по отдельному документу либо по подколлекциям документов. Мы эти критерии рассматривать не будем — по ним есть обширная литература в области тематического моделирования.

[БЕЗ_ЗВУКА] Тематические модели, как правило, создаются ради того, чтобы упростить пользователям понимание больших текстовых коллекций, навигацию по большим текстовым материалам коллекций. И конечно же, здесь не обойтись без средств визуализации. В последние годы таких средств, более-менее универсальных, сделано много. Многие из них находятся в открытом доступе, то есть можно скачать и свою построенную тематическую модель визуализировать с помощью уже готовых инструментов. Большинство из этих инструментов ориентированы именно на то, чтобы визуализировать текстовые коллекции через веб-интерфейсы. То есть поскольку это все-таки средство информационного поиска, то и вот отсюда такая ориентация на веб-интерфейсы. Ну вот один из канонических примеров, который был не так давно сделан и выложен в открытый доступ на GitHub. И вот на слайде показаны примеры того, как выглядит этот пользовательский интерфейс. Здесь демонстрационный пример, который вот авторами сделан на примере «Википедии». Вы попадаете когда на вот этот сайт, который сгенерирован этим средством Topic Model Visualization Engine, и сразу видите список тем. Можете кликнуть в каждую тему, вам покажут окно, в котором будут видны документы этой темы, термины этой темы. Дальше можно кликнуть в документ и увидеть, какие слова в этом документе, какие есть темы в этом документе, перейти на другие темы. И вот таким вот образом реализуется возможность навигации пользователя по коллекции, то есть пользователь может ходить по темам, по документам и по ссылкам между темами и документами. Такое вот простое базовое средство, для того чтобы посмотреть на тематическую модель, которая была построена. И сделать это можно очень быстро. Следующий инструмент для визуализации — он скорее не для пользователей, а для разработчиков — тематическая модель, для того чтобы получше посмотреть на темы, как на столбцы матрицы Φ, и попытаться дать им какие-то интерпретации, переименовать, посмотреть, насколько темы дублируют друг друга, насколько понятные сочетания слов в этих темах образуются. Поэтому вот в этом средстве — системе Termite — есть много функций, для того чтобы по-разному упорядочивать строки матрицы Φ, так чтобы стало видно глазами на фрагменте матрицы Φ, а что же за тема у нас получилась. Вот как видно на этом рисунке, здесь даже выделен вот этот в явном виде столбец матрицы Φ, подписаны все слова, и видно, какие слова сгруппировались вместе. Там даже есть такая функция, чтобы вместе сгруппировать те слова, которые в документах часто стоят рядом друг с другом. И тогда мы прямо вот способны читать эту тему, потому что слова как раз хорошо связываются друг с другом. Есть огромное количество средств визуализации для тематических моделей — потоков новостей или научных статей, или любых других коллекций, где каждому документу приписана метка времени. Тогда строить тематические модели очень удобно, визуализируя их в виде вот таких вот красивых графиков, где видно, как развивались темы во времени, какие темы в какие моменты набирали популярность, в другие моменты времени они совсем забывались. Ну вот, например, в этом примере показано, что видны темы, которые возникли в связи с финансовым кризисом 2008 года. И на таких графиках можно изучать предвестники, последствия и какие-то темы в окрестности кризиса, исследовать, связаны они с кризисом или не связаны. То есть это инструмент и навигации по коллекции, и вот таких вот аналитических исследований, как качественных, так и количественных. Вот еще более интересная визуализация — так называемая река тем, где показаны моменты зарождения тем, их исчезновение. Вот они хорошо видны, видны те моменты, где темы расщепляются. И также можно наложить вот такие вот волны — это траектории отдельных слов: вот в каких темах это слово активно использовалось. Причем частая такая волна означает, что слово использовалось часто, более редкая, что, значит, слово вот в этот промежуток времени использовалось более редко. То есть можно накладывать отдельные слова на темы и получать вот такие вот визуализации — тоже способ исследования динамики тем во времени. Вот другой пример, который показывает, что если тематическая модель учитывает не только слова, но, в частности, связи между документами, и, в частности, это связи цитирования между научными статьями, то можно ставить очень интересные задачи. Например, попытаться ответить на вопрос: а какие предшествующие работы действительно существенно повлияли на данную статью? Ведь в статье часто десятки ссылок. Многие из них чисто формальны или дань вежливости, или же какие-то незначительные моменты, которые для данной статьи где-то в одном месте только упомянуты. Но возникает вот какой вопрос: а на каких предшествующих работах действительно содержательно данная статья существенно опирается? Как выявить те статьи, которые действительно оказали существенное влияние на каждую работу? И оказывается, что это можно сделать с помощью тематической модели, то есть можно выявить тематику статьи, понять, к каким темам она относится. А дальше посмотреть, какие работы были в списке литературы, на которые она сослалась, и которые тоже соответствуют этой тематике. И получается, что тематическая модель позволяет решить вот эту вот важную функцию, то есть сказать, что из этих 30 или 40 статей, на которые здесь сослались, на самом деле определяющими для данной работы являются только 3. Вот на них она существенно опирается. Это важная вещь для вот такой вот... для библиометрии, для аналитики. Вот, а с другой стороны, использование ссылок и цитат позволяет уточнить саму тематическую модель, потому что есть такая здравая гипотеза, что если две статьи ссылаются друг на друга, то, значит, у них есть какая-то общая тематика. И вот это вот как раз можно учитывать с помощью регуляризатора. Еще один пример. Тематическая модель, которая строится тоже, как правило, вот такого сорта модели хорошо проходят на коллекциях научных текстов. Вот в данном случае был взят за многие годы известнейший американский журнал Science, в котором делаются публикации самые лучшие по самым разным отраслям науки. То есть это очень политематическая коллекция. Так вот, оказывается, что мы можем выявлять связи между темами. Здесь такие соображения, что если мы имеем статью, например, про археологию, то в ней скорее появятся термины из геологии, чем из генетики. И выявлением вот таких вот связей между темами, как отраслями знания, тоже представляет отдельный прикладной интерес. И можно построить тематическую модель: вот как на этом рисунке показаны темы кружочками, в темах вписаны главные слова, но самое интересное, что простроены связи между темами. Связь, или ребро, на этом графе тем означает, что вот эти две темы часто оказывались рядом в одних и тех же документах. Это называется коррелированная тематическая модель (correlated topic model). И там фактически тоже используется регуляризатор, который выстраивает и учитывает корреляционные связи между темами, как между строками матрицы Θ. То есть учитывается именно то, как темы входят в документ, а не то, как они... из каких слов они состоят. Итак, резюмируя, поскольку тематическое моделирование — это инструмент исследования больших текстовых коллекций, информационного поиска, разведочного поиска, навигации по коллекциям, конечно же, здесь не обходится без разных средств визуализации. И в этом коротком видео мне удалось показать только лишь малую долю из них. Их огромное количество. Для конечных пользователей мы можем им упрощать навигацию и поиск, но для разработчиков моделей (тематических моделей) визуализации тоже крайне важны, потому что Φ и Θ — это огромные объекты матрицы, которые трудно себе представить. И если построение модели пошло как-то не так, то как обнаружить, что модель несовершенна, что есть какие-то ошибки при построении модели? И вот средства визуализации используются и на стадии отладки, тестирования, разработки тематических моделей. Если в эти средства вставляются способы внесения каких-то модификаций или каких-то оценок от экспертов, то получается, что это еще и инструмент, для того чтобы собрать какие-то размеченные данные, а потом эту разметку учесть либо для оценивания качества тематической модели... Например, мы можем смотреть за тем, насколько хорошо в нужных темах группируются нужные слова, или за тем, что ненужные документы не оказываются в тех или иных темах. Но эту информацию также можно с визуализатора собрать, накопить и потом использовать для дообучения тематической модели, то есть это те самые дополнительные данные, которые можно учесть в регуляризаторе и дальше использовать для уточнения тематической модели.

[БЕЗ_ЗВУКА] Всем привет! Меня зовут Надя, и я расскажу вам про практические аспекты применения тематических моделей. В этом видео мы кратко и без математики резюмируем материал лекции, чтобы разобраться, в каких ситуациях в жизни вам могут пригодиться тематические модели. Затем мы рассмотрим инструменты, с помощью которых вы можете строить тематические модели на своих DataSet. Тематические модели — это еще один вид матричного разложения, но на этот раз оно адаптировано под текст. Коллекция текстовых документов здесь представляется как матрица частот слов. Причем эта матрица, как правило, сильно разрежена. В курсе вы уже встретились с несколькими видами матричного разложения, например метод главных компонент или неотрицательные матричные разложения. Разложение позволяет аппроксимировать исходную матрицу в виде произведения двух матриц низшего ранга. Чем больше промежуточная размерность, тем точнее аппроксимация. Разные виды матричных разложений отличаются, во-первых, ограничениями, которые накладываются на матрицы, и, с другой стороны, отличаются метрикой, по которой мы будем искать меру схожести произведения нашей матрицы с исходной матрицей. Тематические модели решают одну из популярных проблем машинного обучения, а именно интерпретируемость модели. Здесь вы можете не просто построить матричное разложение, то есть получить много чисел, но понять, что означают эти числа. Сначала стоит интерпретировать матрицу Φ, то есть матрицу распределений слов в темах. Если в каждом столбце, то есть в каждой теме, найти наиболее вероятные термины для этой темы, то вы сможете понять, о чем эта тема, и дать ей название. После этого вы можете интерпретировать вторую матрицу — матрицу Θ, то есть матрицу распределения тем в документах. Для каждого документа можно будет найти наиболее релевантные ему темы и таким образом понять, каков смысл этого документа. Тематическое матричное разложение приближает исходную матрицу частот по псевдометрике, называемой дивергенция Кульбака–Лейблера, про которую вам рассказали на лекциях. Что касается промежуточной размерности, то есть количества тем, то их нужно выбирать... то это количество можно выбирать несколькими способами. Чаще всего пробуют разные значения и каждый раз смотрять, хорошие или плохие получились темы. То есть нужно выбрать такое количество, при котором у вас не будет повторений тем, но в то же время вы увидите все темы, которые вы встретили в процессе обучения. Другой способ — это выбирать количество тем по какому-то критерию. Однако часто бывает сложно выбрать критерий, который будет не зависеть от количества тем и позволит выбрать оптимальное число. Кроме того, иногда количество тем следует из семантики задачи. Это наиболее удачный вариант. В интерпретируемости тематических моделей состоит их основная прелесть. То есть если у вас есть какое-то большое количество документов, например миллионы твитов, которые вы скачали, то чем просматривать все эти документы, вы можете построить на них тематическую модель, ну например, для твитов это будет несколько сотен тем, ну скажем, 200 или 300, и затем просмотреть эти темы. Это займет гораздо меньше времени, чем просмотр миллионов твитов, и вы сможете выбрать темы, которые вам могут быть потенциально интересны, и просмотреть только эти твиты. Таким образом вы сузите множество документов, которые вы хотите просматривать. А если воспользоваться также другими способами, средствами автоматического анализа текстов, например суммаризацией или автоматическим именованием тем, то можно автоматически строить целые навигаторы по коллекции текстовых документов. Пример такого навигатора приведен на слайде. Здесь документы выводятся по темам. В качестве документов выступают статьи Википедии. Для каждой темы также выводятся топы терминов и темы, которые похожи на данную тему. Интересная особенность тематических моделей состоит в том, что они проникли в самые разные сферы анализа данных. Тематические модели используют и в поисковых машинах, и в рекомендательных системах, при анализе биологических последовательностей, при медицинской диагностике и даже при распознавании изображений. Я покажу два примера. Эти изображения взяты из одной статьи на Хабрахабре, в которой разработчики рассказывали, как они применяли тематические модели для задачи рекомендации пользователям веб-страниц. Они поступили следующим образом: они в качестве матрицы частот слов задали матрицу популярности отдельных страниц для пользователя. То есть в качестве документов у них выступали пользователи, а в качестве слов — отдельные страницы. Они построили темы и выяснили, что страницы, которые входят в одну тему, действительно семантически похожи друг на друга. Чтобы визуализировать слова, авторы статьи нашли самые частые слова, которые встречались в документах какой-то темы в веб-страницах и изобразили их в виде облака тегов. И мы можем видеть, что одна тема связана с природой и пейзажами, фотографированием, а вторая — с сериалами и мультфильмами. Другой пример — это применение тематических моделей в поисковых машинах. В частности, они нужны в тот момент, когда нам нужно выбрать документы, которые потенциально могут быть релеванты запросу. Если это делать только по словам, то есть выбирать те документы, в которых встречаются те же слова, что и в запросе, то могут возникнуть неприятные ситуации — пример, который приведен на слайде. Здесь и в левом, и в правом документе встречаются слова keys и note, но в левом документе они имеют значение «ключи» и «записка», а в правом «клавиши» и «ноты». Таким образом, если наш запрос связан с пианино, как, например, на слайде, правый документ нам подходит и левый — нет. Предполагается, что тематические модели позволят отличить правый документ от левого за счет построения тематического профиля. Однако применение тематических моделей в таких нестандартных задачах обычно требует усложнения модели и, как следствие, необходимость отдельной реализации алгоритма обучения. Мы с вами рассмотрим классические тематические модели и несколько подходов к их построению. Один из первых подходов к построению тематических моделей был предложен в 1999 году и назывался «вероятностный латентный семантический анализ». Здесь буквально ставилась задача матричного разложения, то есть разложения матрицы частот слов, с дополнительными условиями, что столбцы двух матриц должны быть вероятностными распределениями. Решать такую задачу предполагалось самым стандартным методом статистики — методом максимального правдоподобия. В 2003 году эта задача была рассмотрена с вероятностной, даже с байесовской постановки, и здесь выполняется такое же матричное разложение, только вместо матриц уже используется распределение над этими матрицами. Этот подход получил название «латентное размещение Дирихле». Стоит отметить, что латентное размещение Дирихле на сегодня очень популярно, и это самая изученная, самая используемая модель тематического моделирования. Кроме того, как раз таки многие усложнения, совершенствования моделей делаются именно для модели LDA. Наконец, не так давно был разработан подход, получивший название «аддитивная регуляризация тематических моделей». В нем предполагается ввести дополнительные критерии (или регуляризаторы) в модель PLSA, за счет чего модель получается более гибкой, и ее как раз таки удобно адаптировать для разных задач, не только для простой задачи анализа текстов. Об ARTM вам очень много рассказали на лекции. Что же лучше из этих подходов? Основное преимущество LDA состоит именно в ее популярности. Если вам нужно применить тематическую модель к какой-то нестандартной задаче, ну или просто к любой задаче, например, вы решили тематизировать да те же самые твиты. Вы делаете гугл-запрос в стиле topic modelling for twit analysis и получаете ссылки на статьи, в которых, скорее всего, авторы рассказывают, как они применили LDA к анализу твитов, именно LDA. Но здесь кроется и недостаток этой модели. Дело в том, что для любого усложнения приходится заново реализовывать алгоритм, то есть искать новую реализацию. И нет реализации, которая бы умела все. С другой стороны, часто можно применять просто LDA, и тогда это очень удобно. В ARTM за счет мощного аппарата регуляризаторов можно строить какие-то усложнения модели в одной и той же реализации, и это удобно. Кроме того, в обоих методах нужно настраивать параметры, так что по этому показателю и ARTM, и LDA примерно равнозначны. Мы с вами рассмотрим две реализации тематических моделей. Для LDA мы рассмотрим реализацию в библиотеке gensim. Ее мы выбрали потому, что в gensim помимо тематических моделей реализованы также и другие алгоритмы для анализа текстов, и они понадобятся вам в следующих курсах. ARTM развивается параллельно со своей основной реализацией, которая именуется BigARTM. Это библиотека, которая написана в C++, но имеет очень удобный интерфейс для Python, которым мы и будем пользоваться. Gensim реализован в парадигме использования функций. То есть вы вызываете отдельные функции, получаете результат работы и присваиваете каким-то переменным. BigARTM реализован в объектно-ориентированном стиле. Вы создаете класс модели, дальше даете инструкции, что делать дальше, и если вам нужны какие-то параметры модели, то вы вызываете их отдельным методом. BigARTM — это чуть более гибкий инструмент, чем gensim, за счет аппарата регуляризаторов и модальностей, поэтому для него приходится писать чуть больше кода. В этом плане применение gensim несколько легче. Однако BigARTM быстрее обучает свои модели. Это связано с тем, что модель LDA алгоритмически более сложна. Обе библиотеки позволяют импортировать данные в нескольких форматах. Самый распространенный формат — это формат UCI Bag of Words. Мы его рассмотрим в одной из демонстраций. Однако этот формат не очень интуитивно понятен для человека, потому что, смотря на данные, невозможно увидеть сами тексты и понять, что вот это именно тексты. С другой стороны, этот формат очень компактный. BigARTM также позволяет работать с форматом данных, называемым vowpal wabbit. Его мы тоже рассмотрим в одной из демонстраций. Тематическое моделирование используется при решении самых разных задач: при решении социологических задач, прикладных задач анализа данных, для визуализации и упрощения навигации по коллекциям. Поэтому мы выделили этой теме отдельный модуль и два практических задания. В следующих видео мы рассмотрим примеры, как можно применять библиотеки gensim и BigARTM для построения тематических моделей.

[ЗАСТАВКА] В этом видео мы посмотрим пример использования библиотеки gensim для тематического моделирования. В gensim реализована модель lda, обучение которой основано на эволюционном байесовском выводе. lda, действительно, очень популярны, и часто их используют просто как «чёрный ящик», подают матрице частоты слов и получают на выходе построенные матрицы. Причина состоит в том, что хорошо понять, как обучается lda, можно только, прослушав курс байесовских методов машинного обучения. С другой стороны, если вам не нужно вносить какие-то усложнения в эту модель, то почему бы не воспользоваться ей как «чёрным ящиком». Сегодня мы с вами будем строить модель for fun. Мы возьмём dataset, состоящий из текстовых описаний к комиксам xkcd. Для примера я покажу вам вот такой комикс, в котором рассказывается, как полезна может быть теорема Байеса в реальной жизни. Ну а теорема Байеса, как несложно догадаться, это основа байесовских методов машинного обучения. Что ж, давайте приступим к построению модели. Для начала нам, разумеется, нужно импортировать модули. Мы импортируем компоненты corpora и models из модуля gensim. Первый поможет нам импортировать данные, а второй — построить непосредственно модель. Данные у нас представлены в формате UCI Bag of Words. Это популярный формат, который позволяет максимально сжато представить разреженную матрицу частот слов. Давайте посмотрим пример. Это исходные данные. Здесь указаны ссылки на комиксы и текстовые описания того, что на них изображено. Из этих текстов я удалила всю пунктуацию и выполнила лимитизацию, то есть приведение каждого слова к начальной форме. В итоге я получила матрицу частот слов, которая представлена вот так. В этом файле в первой строке записано количество документов, во второй — количество слов, в третьей — общее количество слов во всех документах коллекции. Дальше в каждой строке идёт три числа — это номер документа, номер слова и количество, сколько раз это слово встретилось в документе. Обращу внимание, что нумерация и документов, и слов здесь ведётся с единицы, а не с нуля, как в некоторых языках программирования, например, в Python. Спрашивается, где же здесь тексты? А тексты здесь во втором файле, который обязательно должен идти рядом с первым. Это файл-словарь. Здесь в каждой строке с номером I записано слово, которое отвечает этому индексу. То есть, например, под индексом 1 у нас идёт слово boy. Итак, давайте импортируем эти файлы в нашу модель. Для этого мы создадим объект класса UciCorpus и укажем путь к двум нашим файлам. Также gensim требует, чтобы мы в отдельную переменную обязательно сохранили словарь. Это можно сделать с помощью метода create_dictionary у объекта data, который мы получили в первой строке. Теперь мы готовы к обучению модели. Это можно сделать, вызвав функцию ldamodel из компоненты models. Давайте посмотрим на её параметры. Первый параметр — это corpus, у нас это, он хранится в переменной data, мы её и указываем. Второй параметр, который, конечно, необязателен, но крайне желательно его указать, это id2word. Это, собственно, отображение индексов слов непосредственно в слова. У нас это как раз тот dictionary, который мы создали во второй строке предыдущей ячейки. Далее идут некоторые параметры, которые влияют на обучение модели. Если у вас коллекция небольшая, как в нашем случае, то можно сделать много проходов по коллекции, и тогда модель получится наиболее адекватной. Тогда нужно указать параметр passes большим, например 20. Если же у вас коллекция действительно большая, например, как «википедия», то делать по ней двадцать проходов будет очень долго. Тогда можно указывать один или два прохода, но нужно обязательно подобрать аккуратно другие параметры, влияющие на сходимость алгоритмов. Особенно это параметры decay и offset, но об этом мы не будем подробно говорить в нашей демонстрации. Кроме того, полезно указывать параметр distributed, например, в true, тогда вы сможете обучать модель сразу в нескольких параллельных процессах. Ещё два важных параметра модели — это alpha и eta. Они создают априорные параметры распределения Дирихле, точнее, параметры априорного распределения Дирихле для моделей. Об этом распределении Дирихле вам рассказали на лекции. Распределение Дирихле — это распределение над векторами, и параметр для него — это тоже вектор. Если в этом векторе параметра все числа одинаковые, то это симметричные распределения Дирихле, если разные — то асимметричные. Gensim поддерживает разные варианты. Вы можете подавать одно число, тогда оно будет... gensim повторит его несколько раз, оставит вектор, и это будет симметричный prayer. Вы можете подавать вектор любой, вы можете подавать строку — symmetric или asymmetric, тогда gensim как-то сам установит эти параметры, исходя из своих каких-то природных соображений, но не факт, что эти значения будут оптимальными. Также можно указывать auto, тогда gensim подберёт оптимальное значение асимметричного природного распределения, но, правда, при этом модель будет обучаться несколько дольше. На практике достаточно попробовать просто числа с каким-то шагом в интервале от 0 до 2, скажем, и выбрать те, при которых модель получается наиболее интерпретируемой. Я это сделала до демонстрации и подобрала alpha и eta, равными 1.25. Так мы и укажем. Модели lda требуется несколько минут для обучения. Чтобы не ждать, мы загрузим готовую модель, которую я сохранила. Для сохранения и загрузки моделей в gensim есть соответствующие функции. Первая — это ldamodel.save, здесь мы должны указать имя файла, куда сохранить модель. Я это сделала заранее. И также это метод LdaModel.load. Здесь мы должны указать файл, в который предварительно сохранена модель. Давайте загрузим готовую модель. Отдельно стоит обратить внимание на то, что, даже если вы уже зафиксировали все параметры, построенная модель будет очень зависеть от начального приближения, потому что изначально все параметры в модели инициализируются какими-то случайными числами. Поэтому каждый раз, если вам позволяет размер коллекции и ваше время, стоит построить модель несколько раз и выбрать ту, в которой темы лучше всего. Понять, какие темы хорошие или плохие можно, посмотрев на «топы» слов. Их можно вывести, вызвав функцию print_topics, указав, сколько тем вы хотите посмотреть и сколько слов вывести в каждой теме. Лексика в комиксах xkcd достаточно специфичная, и особенно русскоговорящему человеку не всегда понятная. Поэтому давайте скопируем наши темы в яндекс-переводчик и посмотрим переводы этих слов. Мы видим, что в первых двух темах много имён, и, честно сказать, не очень понятно, о чём эти темы. Третья тема, она посвящена комиксам в таком стиле: там любят рисовать график, и этот график как-то связан со временем, и идёт какой-то сопровождающий текст. Потом говорят слова: «рисунок», «метка», «линия», «год», «время». Третья тема — это тема для комиксов из серии, когда на нём есть мужчина, женщина, то есть два человека, и они что-то обсуждают. А четвёртая тема весьма специфична. Она рассказывает о каких-то... о выживании на острове, я бы сказала. Это слова: «ждать», «остров», «карта», «обнаружена», и какие-то сопутствующие слова. Кроме того, давайте посчитаем величину перплексии на нашей модели. Перплексия — это очень распространённая мера качества тематических моделей, и особенное значение она имеет в контексте байесовских моделей. В первую очередь, потому что она описывает, насколько хорошо ваше распределение, описывает ваши исходные данные. И интерпретировать его можно так: чем меньше значение перплексии, тем лучше. С другой стороны, у перплексии нет максимального значения. То есть непонятно, если нам вручили число, непонятно, что оно означает. И ещё очень важно, что величина перплексии зависит, во-первых, от данных, а во-вторых, от количества тем. Поэтому, во-первых, нельзя сравнивать перплексию модели, построенных на разных данных, а во-вторых, нельзя по перплексии выбирать количество тем. Это очень логично. Чем больше у нас тем, тем лучше наша матрица аппроксимирует исходную тему, то есть описывает её лучше, тем, соответственно, меньше перплексия. Иногда перплексию измеряют на тестовом dataset, то есть отдельно отложенных документах или частях документов, но это важно для научных статей, а для практики не очень важно, поэтому мы можем измерять перплексию прямо по обучающей выборке. Давайте это и сделаем. Сейчас мы убедимся в том, что перплексия, как само число, мало о чём говорит, то есть её можно использовать только для сравнения моделей, но нельзя понять, хорошая модель или плохая, только для одной модели. Ну собственно, так и получается. Давайте также обсудим ещё две важные функции, которые предоставляет gensim. Во-первых, это функция update, которая позволяет дообучить модель на новых данных. То есть, если, например, вы бы считали ещё один другой corpus, например, из новых комиксов, в переменную data2 и указали бы её как первый аргумент функции update и также указали бы, сколько сделать проходов по коллекции, чтобы дообучить модель, то gensim обновит все параметры. Ещё одна важная функция — это get_document_topics. Он позволяет для каждого документа найти распределение над множеством тем. Давайте получим распределение для другого документа и увидим, что в нём наибольшую роль играет третья тема. Напомним, что это тема про мужчин и женщин. И также мы увидим, что все числа в правом столбце суммируются к единице, что неудивительно, потому что в столбце матрицы тета — это дискретные вероятностные распределения. Что ж, в этом уроке мы познакомились с библиотекой gensim, с тем, как туда можно импортировать данные и как можно строить модель lda. Напоследок я покажу вам ещё пару комиксов xkcd, главные герои которых не знали, что существуют тематические модели, потому что, на самом деле, они могли бы, в каком-то смысле, решить их проблемы.

[ЗАСТАВКА] В этом уроке мы научимся строить тематические модели с помощью библиотеки BigARTM. Как вы знаете, тематические модели позволяют находить темы в коллекциях текстовых документов. Сегодня мы будем работать с набором конспектов по нескольким школьным предметам. Эти конспекты мы предварительно скачали с одного интернет-ресурса, помогающего школьникам готовиться к ЕГЭ. Мы постараемся найти в этих конспектах темы, связанные с математикой, физикой, химией и другими школьными предметами. Давайте начнем. Для начала импортируем все необходимые модули. Нам понадобится matplotlib, чтобы строить графики. И, разумеется, нам понадобится artm. Первое и самое важное, что нужно сделать, это импортировать данные. Как вы знаете, тематические модели работают с коллекцией текстовых документов, представленной в так называемом формате мешка слов. Таким образом, нам важно, сколько раз слово встретилось в документе и не важен порядок слов. Кроме того, в документ, помимо слов, могут входить элементы других классов или модальностей. Например, в документ могут входить хештеги или ссылки. Эти объекты можно по-разному учитывать в модели, поэтому желательно отдельно указать, к какому классу принадлежит тот или иной объект. Такие данные можно представлять в разных форматах, но один из наиболее удобных и понятных человеку — это vowpal_wabbit формат. BigARTM умеет с ним работать. Давайте посмотрим, что это такое. Коллекция в vowpal_wabbit- формате — это текстовый документ, в котором отдельная строчка соответствует отдельному документу. Вначале может идти название документа, но в нашей коллекции названий документов нет. Затем идет вертикальная черта, после которой мы пишем название класса или модальности. Для простоты в нашей модели будет только одна модальность — текстовая. Мы ее так и назовем — текст. Затем через пробел идут отдельные слова. После слова может стоять двоеточие и количество раз, сколько оно встретилось в данный момент. Когда BigARTM будет считывать данные, он посчитает суммарное количество раз, сколько слово встретилось в документе, и запишет в матрицу частоту слов. Матрицу частоту слов BigARTM представляет в своем внутреннем формате, называемом батчами. Батчи — это несколько отдельных файлов. Нам их нужно сохранить в отдельную папку. Поэтому давайте на компьютере создадим новую папку (пустую) и назовем ее school_batches. Теперь мы готовы к импортированию данных. Мы создаем объект класса artm.batchvectorizer. В параметре data_path мы указываем путь к нашему текстовому файлу, который мы только что смотрели, — school.txt. Дальше мы указываем, что наш формат — это vowpal_wabbit. И указываем путь к target_folder, то есть к папке, в которую нужно складывать батчи. Мы назвали ее school_batches. Кроме того, есть необязательный аргумент batch_size, то есть количество документов в одном батче. Давайте укажем его равным 100. Если вы его не указываете, то BigARTM по умолчанию будет складывать тысячу документов в один батч. Запустим ячейку и посмотрим, что в нашей папке появились файлы с батчами. BigARTM гораздо быстрее загружает данные уже из батчей, а не из текстового файла. Поэтому если вы захотите строить модель снова на этих же данных, то вам уже можно будет загрузить их прямо из батчей. Для этого мы создаем объект класса batch_vectorizer и в качестве data_path указываем уже путь к нашей папке с батчами, то есть school_batches. И говорим, что наш формат данных (data_format) — это batches. Давайте строить модель. Для этого мы создаем объект класса ARTM. Обязательный аргумент, который здесь нужно указать, это количество тем. Сейчас мы укажем количество тем равным 10, потому что мы знаем, что в нашей коллекции 10 школьных предметов. Обычно нужно попробовать разные величины и посмотреть, при каком количестве тем тема получается наиболее интерпретируемой. Кроме того, удобно указать названия для тем. Давайте укажем их как subject плюс индекс тем от 0 до 9. И кроме того, нужно указать классы. Классы указываются как словарь, в котором ключами являются названия модальностей, те же самые, что мы указывали в vowpal_wabbit-документе, и веса этих модальностей. Вес модальности — это то число, на которое мы будем умножать счетчики вхождений слова в документ. Например, если в нашей коллекции две модальности (тексты и хештеги), и мы знаем, что в одном документе встречается один или два хештега, то логично поставить вес при модальности хештега, равный 10. Когда мы будем домножать вот эти счетчики 1 или 2 на 10, и они будут как-то сильнее влиять на модель. Однако если в модели только одна модальность, это совершенно неважно, и главное, чтобы вес был не нулевым. Давайте поставим единичку. Когда мы обучим эту модель, мы получим две матрицы — фи и тета. Матрица фи — это матрица распределений слов в темах, а матрица тета — матрица распределения тем в документах. Первая позволяет понять, о чем наши темы, какие слова в нее входят, а вторая — о чем документы, какие темы входят в наши документы. Давайте создадим также некоторые метрики модели, по которой сможем понимать, насколько хорошо она обучилась. Для создания метрики мы пользуемся методом model_artm.scores.add и в качестве параметра указываем ту или иную метрику. Для начала давайте создадим score под названием перплексия. Перплексия позволяет понять, насколько хорошо наша модель описывает данные. Чем меньше перплексия, тем лучше. Мы создаем объект класса PerplexityScore и обязательно указываем ему название, чтобы потом мы могли обращаться к этой метрике. И также указываем некоторые дополнительные значения по умолчанию. Кроме того, давайте создадим метрики разреженности матрицы фи и тета. Здесь такая логика, что если одно слово входит в небольшое количество тем, например, в одну или две, то матрица фи будет разреженной, в ней будет много нулей. То же самое с матрицей тета: если документ относится только к одной или двум темам, то матрица тета будет разреженной. Поэтому считается, что чем больше разреженность, тем лучше. Также давайте создадим метрику под названием top_words. Это топы слов, которые входят в наши темы. По ним мы сможем понять, о чем наша тема, то есть интерпретировать ее. Здесь нам нужно указать, сколько слов мы хотим выводить в параметре num_tokens. Давайте укажем 15. И также по какому классу ее считать. Класс нужно указывать в любой метрике, которая меряет что-то по матрице фи, потому что в матрице фи у нас немного будут разные модальности. Итак, давайте создадим модель. Добавим метрики. Теперь нам нужно инициализировать модель. Для этого artm пройдет по всем батчам, в которых хранятся наши данные, и загрузит их в свой словарь. Мы указываем название этого словаря dictionary и также указываем путь к нашим данным, это наш batch_vectorizer. Когда мы собрали словарь, то есть прошлись по всем батчам и нашли все слова, которые входят в наши документы, мы можем инициализировать модель. В процессе инициализации создадутся две матрицы (фи и тета) и заполнятся случайными значениями. Мы указываем словарь, чтобы вспомнить те слова, которые мы нашли в результате использования метода gather_dictionary. Чтобы зафиксировать случайные приближения для матрицы фи и тета, мы можем указать параметр seed. Давайте укажем его seed = –1. Тогда если мы в следующий раз будем снова строить модель, то матрицы фи и тета инициализируются теми же самыми значениями, и мы увидим ту же самую модель, то есть мы как бы сможем сопоставлять темы между собой. Это удобно. Инициализируем модель. Теперь мы готовы обучать модель. Для обучения модели в BigARTM есть два метода. Это fit_offline и fit_online. На маленьких коллекциях (а у нас маленькая коллекция) удобно использовать offline-обучение. В результате offline-обучения BigARTM проходит по всей коллекции много раз, а именно столько раз, сколько мы укажем в параметре num_collection_passes. Давайте укажем 40. И вот в этом большом цикле он проходит столько раз по каждому документу, сколько мы укажем в параметре num_document_passes. Давайте укажем один проход. Также мы, разумеется, указываем в качестве параметра batch_vectorizer, то есть путь к нашим данным, по которым нужно производить обучение. Кроме того, модель можно обучать с помощью метода fit_online. Он проходит по коллекции гораздо меньшее количество раз, например, один или два. Но приходится настраивать некоторые дополнительные параметры сглаживания. Это удобно для больших коллекций, на которых сделать много проходов по коллекции просто долго. Итак, мы дожидаемся, пока наша модель обучится сделать все итерации. Обычно рекомендуется делать 30–40 проходов по коллекции, чтобы модель хорошо сошлась. Однако давайте посмотрим, в какой же момент она по-настоящему сошлась. Это удобно отслеживать по метрике перплексии. Построим ее график. От номера прохода по коллекции. Чтобы обратиться к результатам подсчета этой метрики, мы обращаемся к переменной model_artm.score_tracker и указываем имя нашей метрики, которое мы задавали при ее создании. Это PerplexityScore. В этом классе есть разные переменные. Например, это value. Переменная value хранит историю измерения этой метрики на каждом проходе по всей коллекции. Также Там, например, есть переменная last_value, то есть она показывает последнюю измеренную перплексию во всей коллекции. Давайте построим график. Мы видим, что уже где-то хотя бы к 20-й итерации наша модель полностью сошлась. То есть для нашей коллекции достаточно делать только 20 проходов по batch. Давайте выведем топы слов, то есть посмотрим, о чем наши темы. Для этого мы опять обращаемся к score_tracker, указываем имя нашей метрики, которая находит топы слов (top words), и спрашиваем token для каждой отдельной темы. Делаем это в цикле по всем темам. Здесь мы можем посмотреть на слова. Ну что мы видим? Что, например, первая тема — это что-то связанное с обществознанием, вторая — что-то про литературу (герой, человек, ребенок), третья (предложение, слово, речь) — это что-то про русский язык, и так далее. Можно посмотреть, что это за темы. Давайте также посмотрим на разреженность матриц. Эти величины мы потом будем сравнивать с разреженной моделью. Для этого мы обращаемся к score_tracker, спрашиваем значение метрики SparsityPhiScore и SparsityThetaScore и спрашиваем последнее измеренное значение. Мы видим, что матрица phi у нас достаточно разреженная — в ней 75 % нулей, а матрица theta неразреженная — в ней только 10 % нулей. Мы видели, что в наших темах много общеупотребительных слов. Например, это слова «быть», «который», «они», «что», «свой», «этот». Они очень мешают интерпретации тем, пониманию, о чем, собственно, получилась тема. Для того чтобы исключить их из нашей модели, особенно из топов слов, удобно пользоваться регуляризацией моделей. Для этого и придумана библиотека ARTM. Давайте рассмотрим пример регуляризатора, а именно мы воспользуемся разреживающим регуляризатором для матрица phi. Посмотрим, как его можно создавать. Мы пользуемся методом model_artm.regularizers.add и в качестве аргумента передаем объект класса регуляризатора. В нашем случае это SmoothSparseRegularizer. Мы обязательно создаем ему имя, чтобы потом к нему можно было обращаться, как и со всеми объектами в BigARTM, задаем коэффициент и можем задать название словаря. Если мы задаем название словаря, а это именно тот словарь, который мы получали при вызове функции get a dictionary, то при регуляризации вот этот вот коэффициент, который мы здесь указали, будет домножаться на частоту слова во всей коллекции. То есть суммарно значения будут получаться не очень большие. Если мы не указываем словарь, то регуляризация для каждого слова будет одинаковой, и значение коэффициента можно указывать поменьше, например −2,5 или −5, цифры такого порядка. Если мы используем словарь, то и коэффициент, который отрицательный, то есть у нас этот отрицательный коэффициент −100, например, то слово будет... чем более вероятно слово во всей коллекции, чем большую частоту оно имеет, тем менее вероятно оно будет входить в отдельные темы. Это именно тот эффект, которого мы добиваемся, поэтому мы создаем такой регуляризатор. Иногда может случиться так, что вы хотите применять регуляризатор только какой-то отдельной модальности, тогда еще можно указывать параметр class_ids и создавать список тех модальностей, к которым вы хотите его применять. После добавления регуляризатора мы снова обучаем модель, то есть вызываем метод fit_offline. Сейчас давайте сделаем 15 проходов по коллекции, чтобы модель точно успела сойтись. После этого мы снова посмотрим на топы слов. Что мы видим? Мы видим, что наш регуляризатор пока не принес почти никакого эффекта в нашу модель. Мы снова видим вот эти слова: «быть», «который», которые мешают интерпретации тем. Давайте попробуем изменять коэффициент регуляризации. Если мы зададим его очень большим, например −1e в шестой (минус миллион), все темы просто обнулятся, то есть модель перестанет быть совершенно интерпретируемой, поскольку у нас уже не будет тем. Если вы так опробуете некоторые величины, то сможете нащупать, найти оптимальное значение коэффициента. Я нашла его заранее. Он равен 5 * 1e в четвертой со знаком минус. Давайте снова дообучим нашу модель и посмотрим на топы слов. [БЕЗ_ЗВУКА] Наконец мы видим, что наши темы совершенно интерпретируемы. Первая тема — это тема про историю, вторая (ребенок, Лермонтов, поэма, сказка, отец, молодой) — тема по литературе, третья (слово, предложение, например, простой, речь) — это тема про русский язык. Следующая тема очень похожа на географию (Земля, энергия, вода, ядро, масса, количество, природный, океан). А может быть, география вместе с физикой. Дальше это тема про математику (x, a, число, b, функция). Дальше еще несколько тем тоже про обществознание и историю. Таким образом, с помощью регуляризации мы смогли получить более интерпретируемую тему. На что стоит обратить внимание? Что как и в любой модели с регуляризацией нужно аккуратно подбирать коэффициент. Например, если в линейной регрессии вы будете применять регуляризацию методом лассо, вам также придется подбирать коэффициент. Если вы зададите его маленьким, он не будет иметь никакого влияния на модель. Создадите большим — он обнулит веса при всех признаках. Вот этот вот оптимальный коэффициент приходится искать всегда, когда мы используем регуляризацию в той или иной модели. Еще один немаловажный момент состоит в том, что использовать регуляризацию в тематических моделях рекомендуется только после того, как модель хорошо сошлась. Особенно, если речь идет о разреживании. В нашем примере мы именно так и делали. Мы сначала обучили модель без регуляризации и потом добавили регуляризацию. В противном случае ваши темы могут намешаться. То есть, например, одна тема будет сразу и про физику, и про математику, и про историю, что в принципе неправильно. Давайте посмотрим на разреженность наших матриц и увидим, что разреженность матрицы phi стала значительно большей, чем была в предыдущий раз. Давайте вспомним, в прошлый раз у нас была разреженность 0,75, а сейчас 0,9, то есть стало гораздо больше нулей. И более того, больше нулей стало даже в матрице theta. Теперь там 33 %, хотя было 10. То есть мы видим, что регуляризатор действительно делает больше нулей в наших матрицах. Давайте напоследок посмотрим несколько полезных приемов, которые понадобятся вам при работе с библиотекой BigARTM. Во-первых, вы можете загружать и сохранять модели. Для этого можно пользоваться методом save и указывать файл, в который вы хотите сохранить модель. Файлу можно указывать совершенно любое расширение. BigARTM будет совершенно неважно. Я предпочитаю не указывать никакого расширения. Вы можете писать .dump, например, или .bin После этого вы сможете загрузить модель с помощью метода load. Хочу обратить внимание, что чтобы загрузить модель, нужно сначала создать объект класса ARTM, как мы делали это в начале урока, задать уже количество тем и только после этого загружать модель из бинарного файла. Еще два полезных приема — это доступ к самим матрицам. Давайте посмотрим, как это работает. Чтобы получить матрицу phi, можно воспользоваться методом get_phi. Метод get_phi возвращает нам pandas DataFrame, в котором по столбцам стоят отдельные темы. Это вот, собственно, имена тем, которые мы давали в самом начале: subject плюс index от 0 до 9. А по строкам идут наши слова, которые встречаются в нашей коллекции. Ну а значения — это их вероятности. Вот видим, что в матрице очень много нулей и изредка встречаются какие-то значения. То есть каждое слово принадлежит действительно буквально одной теме или двум. Это то, чего мы добивались. Таким же способом можно запросить матрицу theta и также получить pandas DataFrame, в котором по столбцам будут идти документы, а по строкам — темы, и также будут даваться значения. Вот мы видим, что, например, первый документ принадлежит в основном первой теме. Она имеет в нем большой вес. Кроме того, вы можете тематизировать новые документы, которые модель еще не видела. Для этого можно использовать метод transform. Здесь нужно указать новый batch_vectorizer (это batch_vectorizer уже по новым документам, которые модель еще не видела, то есть вам нужно самим его отдельно создать) и количество проходов по коллекции. После этого вы сможете получить для них матрицу theta и, например, категоризировать. Это удобно, если у вас есть много новых документов, которые идут сплошным потоком, вам нужно быстро определять по ним темы, находить категории, вы можете пользоваться методом transform. Итак, в этом уроке мы познакомились с форматом данных Vowpal Wabbit, научились импортировать его в BigARTM, научились создавать, обучать модели и также смотреть, какие у нас модели получились, и интерпретировать темы. Эти навыки понадобятся вам в следующем preview, посвященном построению тематических моделей в библиотеке BigARTM. Вы будете находить темы в коллекции текстов, записанных по научным видеолекциям.

Сегодня мы будем устанавливать библиотеку BigARTM под операционной системой Linux. Для этого вам нужно зайти на сайт с официальной документацией библиотеки bigartm.org, открыть раздел installation и подраздел инсталлирование под Linux и Mac OS-X. Для того чтобы библиотека заработала, вам нужно установить некоторые зависимости, которые можно поставить вот этой вот командой в терминале, что мы и проделаем. Вставляем. Продолжить. [БЕЗ_ЗВУКА] И придется немножко подождать, пока будет скачан и собран boost. [БЕЗ_ЗВУКА] [БЕЗ_ЗВУКА] [БЕЗ_ЗВУКА] Ну вот процесс сборки завершился и мы можем перейти к настройке использования библиотеки из Python. У вас должен быть установлен Python версии 2.7 и в нем должны быть библиотеки numpy и pandas. Предполагается, что всё это уже установлено. Теперь выполним несколько команд, для того чтобы сконфигурировать Python Interface BigARTM. [БЕЗ_ЗВУКА] [БЕЗ_ЗВУКА] [БЕЗ_ЗВУКА] [БЕЗ_ЗВУКА] Сборка protobuf завершилась и теперь нам осталось только установить переменную artm shared library на местоположение динамической библиотеки BigARTM. [БЕЗ_ЗВУКА] Всё, процесс установки закончен, давайте попробуем запустить Python. И импортировать библиотеку ARTM. Да, у нас это получилось. Давайте попробуем создать модель. Получилось. Всё, библиотека установлена.