[ЗАСТАВКА] Привет! С вами Евгений. В этом уроке мы с вами поговорим о задаче регрессии, обсудим некоторые ее важные свойства, как она решается, почему она решается именно так и как интерпретировать то, что получается в результате. Если подумать, термин регрессия довольно странный. Кажется, что в нем есть что-то негативное. Впервые этот термин появился в конце XIX века в работе Френсиса Гальтона, которая называлась «Регрессия к середине в наследственности роста». В этой работе Френсис Гальтон исследовал зависимость между средним ростом детей и средним ростом их родителей и обнаружил, что отклонение роста детей от среднего составляет примерно две трети отклонения роста родителей от среднего. Этот результат контринтуитивен. Кажется, он означает, что с течением времени люди должны рождаться все ближе и ближе к среднему росту. На самом деле, естественно, этого не происходит. Чтобы лучше понять эффект регрессии к среднему, давайте посмотрим на другое творение Френсиса Гальтона, которое называется машина, или доска, Гальтона. Это механическая машина, в которой сверху в центральной части находятся шарики. Когда открывается заслонка, шарики начинают постепенно сыпаться вниз, ударяясь о штырьки, которые расположены на одинаковом расстоянии друг от друга. При каждом соударении шарика со штырьком вероятность того, что он упадет налево и направо от штырька, одинакова. Постепенно шарики начинают собираться в секциях внизу в до боли знакомую нам фигуру — в гауссиану, или плотность нормального распределения. Чтобы понять эффект регрессии к среднему, давайте мысленно подставим к машине Гальтона снизу еще одну такую же машину. Если теперь мы уберем перегородку, которая удерживает шарики в верхней половине, они начнут постепенно осыпаться вниз и сформируют внизу еще одну такую же гауссиану. Давайте теперь зафиксируем какой-то конкретный шарик, который в нижней половине находится в одной из ячеек близко к краю, и попытаемся понять, откуда сверху он мог в эту ячейку попасть. Оказывается, что с достаточно большой вероятностью этот шарик пришел не из ячейки, которая находится в верхней половине прямо над ячейкой, в которой он оказался внизу, а от ячейки ближе к середине. Это происходит просто потому, что в середине шариков больше. Эффект регрессии к среднему проявляется во многих практических задачах. Например, если вы дадите какой-то достаточно сложный тест группе студентов, то большую роль в том, насколько хорошо они его пройдут, будут играть не только их знания по предмету, но и то, насколько им повезло, то есть случайный фактор. Поэтому если вы изолируете, например, 10 % студентов, которые прошли тест лучше всех (набрали больше всего баллов) и дадите им еще одну версию этого теста и заставите их пройти его снова, средний балл в этой группе скорее всего упадет. Просто потому что люди, которым повезло в первый раз, скорее всего уже не будут так удачливы во второй. Это эффект регрессии к середине. Френсис Гальтон был достаточно плодовитым ученым. Он был основоположником дактилоскопии, исследовал явление синестезии, внес существенный вклад в метеорологию, впервые описав циклоны и антициклоны, а также, например, изобрел ультразвуковой свисток для собак. Но именно регрессия и по сей день остается одним из наиболее важнейших инструментов, к которому он приложил руку. Давайте начнем его изучение. Чаще всего под регрессией понимают минимизацию среднеквадратичной ошибки: квадратов отклонений откликов y от их предсказанных значений a(x). Поскольку минимизируется сумма квадратов отклонений, этот метод называется методом наименьших квадратов (сокращенно МНК). Для линейной регрессии, в которой мы приближаем отклик линейной комбинации наших факторов x с весами w, эта задача имеет аналитическое решение. Именно этим частично объясняется популярность среднеквадратичной ошибки. В XIX веке, когда эта задача впервые возникла, никакого способа ее решения, кроме аналитического, быть не могло. Сейчас мы можем минимизировать не только среднеквадратичную ошибку, но и, например, среднюю абсолютную, то есть сумму модулей отклонений нашей модели от отклика. Такая задача является частным случаем класса задач квантильной регрессии, о которых мы будем говорить подробно в следующих видео. Далее в этом уроке вас ждет знакомство с методом максимального правдоподобия, подробное изучение свойств регрессии, регуляризации, а также задача логистической регрессии. [ЗАСТАВКА]

[ЗАСТАВКА] Прежде чем дальше изучать задачи регрессии, в этом видео мы познакомимся с методом максимизации правдоподобия — одним из мощнейших методов математической статистики. Представьте, что у вас есть некая случайная величина x и ее функция распределения F(x) зависит от неизвестного вам параметра θ. Пусть у вас есть выборка из этой случайной величины, то есть совокупность независимых, одинаково распределенных ее реализаций. Как по выборке лучше всего оценить неизвестный параметр θ? Чтобы понять метод максимального правдоподобия, давайте рассмотрим еще один исторический пример. Эти данные собраны в конце XIX века. В Генеральный штаб прусской армии ежегодно в течение 20 лет от десяти кавалерийских корпусов поступали данные о количестве смертей кавалеристов в результате гибели под ними коня. Эти данные — перед вами в таблице. Как видно, в большей части отчетов никто не умер, однако в 65-и отчетах умер один человек, в 22-х отчетах умерло два человека и так далее. Поскольку эта случайная величина — количество умерших кавалеристов — явно счетчик, логично попробовать моделировать ее распределением Пуассона. Но как выбрать неизвестный параметр λ для этого распределения? Давайте запишем функцию вероятности для распределения Пуассона. Вероятность того, что случайная величина из распределения Пуассона с параметром λ примет значение k, определяется вот такой величиной. Теперь вероятность получения значения, равного i-тому элементу выборки, записывается той же формулой. Поскольку наша выборка состоит из независимых, одинаково распределенных случайных величин, мы можем записать суммарную вероятность выборки, вероятность получения именно такой выборки, и она будет являться произведением вероятности каждого элемента этой выборки. Эта функция является функцией неизвестного параметра λ, обозначается за L, и называется правдоподобием выборки, то есть вероятностью получения именно такой выборки. Если теперь мы, в качестве нашей оценки λ, возьмем значение, которое максимизирует функцию правдоподобия, мы получим оценку, которая называется оценкой максимального правдоподобия. Логично оценивать λ именно таким способом, поскольку, выбирая именно такое λ, мы максимизируем вероятность получения именно таких данных, которые у нас есть. В рассматриваемой задаче несложно показать, что оценка максимального правдоподобия для параметра λ совпадает с выборочным средним. Чтобы это показать, нужно всего лишь взять логарифм от функции правдоподобия — логарифмирование не влияет на положение максимума этой функции, но превращает произведение вероятностей в сумму, с которой легче оперировать. После чего от этого логарифма нужно взять производную, приравнять ее к нулю, и таким образом найти точку максимума. Вы можете без труда проделать это упражнение. В данном случае выборочное среднее равно 0,61, то есть данные, которые мы рассматриваем, лучше всего моделировать случайной величиной с распределением Пуассона и параметром 0,61. Вот так в общем виде выглядит метод максимума правдоподобия. Трюк с логарифмированием, который я вам только что описал, используется достаточно часто, потому что оперировать с логарифмом правдоподобия действительно проще, чем с самим правдоподобием. Если вы имеете дело со случайной величиной из непрерывного распределения, метод максимального правдоподобия работает точно так же, за исключением того, что функция вероятности нашей случайной величины заменяется на ее плотность. Метод максимального правдоподобия обладает рядом очень полезных свойств. Во-первых, получаемые с его помощью оценки являются состоятельными, то есть при увеличении объема выборки они начинают стремиться к истинным значениям параметра θ. Во-вторых, они являются асимптотически нормальными, то есть опять же, с ростом объема выборки, оценки максимального правдоподобия все лучше описываются нормальным распределением с средним, равным истинному значению θ и дисперсией, равной величине, обратной к информации Фишера. Что это такое, совершенно не важно. Важно только, что эта величина также с успехом может быть оценена по выборке. Итак, в этом видео мы познакомились с методом максимизации правдоподобия — крайне мощным и полезным методом оценки неизвестных параметров распределения. Из следующего видео вы узнаете, при чем тут регрессия.

[ЗАСТАВКА] Теперь, когда вы знаете, что такое правдоподобие и зачем его максимизировать, давайте вернемся к задаче регрессии. Попробуем разобраться, что именно получается в результате минимизации среднеквадратичной ошибки. Строя регрессию, мы пытаемся значение отклика y приблизить нашей модельной функции a от факторов x. Это можно представить следующим образом. Значение отклика y представляет собой сумму регрессионной функции a(x) и компоненты ε, которая описывает некоторый случайный шум. Если этот случайный шум имеет нормальное распределение с нулевым средним и какой-то дисперсией σ², оказывается, что задача минимизации среднеквадратичной ошибки дает оценку максимального правдоподобия для регрессионной функции a(x). Казалось бы, какое это имеет значение? Дело в том, что опираясь на этот факт, мы можем использовать свойства метода максимального правдоподобия, в частности асимптотическую нормальность. Используя эту асимптотическую нормальность, мы можем определять значимость признаков xʲ в нашей модели и делать отбор этих признаков, а также мы можем строить доверительные интервалы для значения отклика на новых объектах, которых в нашей обучающей выборке нет. Распределение шума не обязательно должно быть нормальным, может быть каким-то другим. Например, можно попытаться описать его распределением Лапласа с нулевым средним. Формула для функции плотности вероятности такого распределения перед вами. Вот так выглядит ее график. По сравнению с нормальным распределением, распределение Лапласа имеет более тяжелые хвосты, то есть для него более вероятны большие значения ε. Если мы моделируем шум распределением Лапласа, мы разрешаем наблюдениям сильнее отклоняться от нашей модели, и за счет этого мы получаем решение, которое более устойчиво к выбросам. Оказывается, что если шум действительно описывается распределением Лапласа, то к оценке максимального правдоподобия приводит минимизация средних абсолютных отклонений. Итак, из этого видео вы узнали, что регрессия методом наименьших квадратов дает оценку максимального правдоподобия в случае нормального шума, а регрессия со средней абсолютной ошибкой дает оценку максимального правдоподобия для нашей регрессионной функции, если шум лапласовский. В следующем видео вы узнаете, как регрессию можно интерпретировать как оценку среднего.

[ЗАСТАВКА] Продолжим разбираться с тем, что же такое получается в качестве ответа в задаче регрессии. Начнем снова с метода наименьших квадратов. Разберемся со среднеквадратичной ошибкой. Чтобы разбираться было проще, давайте сделаем некоторые упрощающие предположения. Пусть для начала у нас нет никаких признаков x, а a — это просто константа. Пусть кроме того у нас есть бесконечная выборка из y, то есть фактически и не выборка вовсе, а полностью известно распределение случайной величины y. Пусть оно задается плотностью f (t). В таком случае среднеквадратичная ошибка принимает следующий вид. Нетрудно показать, раскрыв квадрат под знаком интеграла и продифференцировав полученное выражение, что минимум такому функционалу доставляет математическое ожидание y. То есть наилучшая константа, которая аппроксимирует значение y в смысле среднеквадратичной ошибки — это математическое ожидание. Пусть теперь a — это не константа, а некоторая произвольная функция от наших признаков x. Можно показать, что в этом случае минимумом среднеквадратичной ошибки является условное математическое ожидание y по x. То есть среднее значение y при таких x. Теперь, если мы имеем дело с конечной выборкой, получается, оценка, которую мы получаем, минимизируя среднеквадратичную ошибку — это наша лучшая аппроксимация условного математического ожидания. Если регрессия линейная, то есть отклик y моделируется линейной комбинацией наших признаков x с весами w, то w*, минимизирующее среднеквадратичную ошибку, задает наилучшую линейную аппроксимацию условного математического ожидания. В каком-то смысле этот результат интуитивно понятен. Пусть, например, y = 2. Поскольку среднеквадратичная ошибка будет симметрична относительно 2, мы будем одинаково штрафовать наши модельные предсказания a (x) за большие отклонения от 2 как в большую, так и в меньшую сторону. Неудивительно, что минимизируя симметричную функцию потерь, мы получаем в ответе какое-то среднее. Однако оказывается, что условное математическое ожидание доставляет минимум не только среднеквадратичной ошибке, но и более широкому классу функций потери, которые называются дивергенциями Брегмана. Дивергенции Брегмана порождаются любой непрерывной дифференцируемой выпуклой функцией φ. Среднеквадратичная ошибка является ее частным случаем. Таким образом, минимизируя любую дивергенцию Брегмана, мы получаем какую-то оценку для условного математического ожидания. И вот это уже довольно странно, потому что в семействе дивергенций Брегмана можно найти функции, которые относительно y несимметричны. Они могут выглядеть вот так или так или так, то есть они сильнее штрафуют за отклонение нашей модели от y в какую-то из сторон. Тем не менее, наилучшей оценкой является все еще условное математическое ожидание. Этот результат достаточно контринтуитивен, и получен он был не так давно. А вот средняя абсолютная ошибка в семейство дивергенций Брегмана не входит. Минимизируя вот эту среднюю абсолютную ошибку, график которой представляет собой такой треугольник, мы получаем тоже оценку какого-то среднего, но другого. Это уже оценка не условного математического ожидания, а условной медианы y|x. Треугольник, описывающий среднюю абсолютную ошибку, можно попробовать наклонить в какую-то из сторон на угол τ. Минимизируя такой функционал, мы получаем оценку для условного квантиля y|x порядка τ. τ, естественно, меняется от 0 до 1, поскольку это квантиль. Итак, в этом видео мы узнали, что решение задачи регрессии наименьших квадратов представляет собой наилучшую возможную по выборке оценку условного матожидания y при условии x. Решение задачи квантильной регрессии дает оценку условного квантиля y|x. А при использовании средней абсолютной ошибки мы получаем оценку условной медианы med (y|x). Далее в программе: регуляризация.

[БЕЗ_ЗВУКА] В этом видео мы поговорим о регуляризации линейных регрессионных моделей. Как вы видели ранее в этом курсе, регрессионные модели имеют свойство переобучаться. Если вы взяли слишком сложную модель и у вас недостаточно данных для того, чтобы точно определить ее параметры, вы легко можете получить какую-то модель, которая будет очень хорошо описывать вашу обучающую выборку, но при этом очень плохо обобщаться на тестовую. Бороться с этим можно разными способами. Можно попробовать взять больше данных. Имея много данных, вы сможете точнее оценить вашу модель и уменьшить переобучение. Очень часто это решения недоступно, поскольку дополнительные данные стоят дополнительных денег. Даже в задачах, когда, казалось бы, у вас есть терабайты данных, например в задачах веб-поиска, эффективный объем выборки зачастую часто оказывается очень маленьким, если, например, мы хотим показывать для каждого пользователя его персонализированные результаты. Мы вынуждены использовать только его историю. Еще один способ борьбы с переобучением — это упрощение модели. В частности, можно, например, взять просто меньше признаков. Какие-то из признаков просто выбросить. Для этого нужно перебрать большое количество подмножеств наших признаков xj-тое, и общее количество подмножеств, которые нужно перебрать, очень быстро растет с ростом размерности задачи. Полный перебор часто оказывается недоступен. Кроме того, если признаков действительно много и они сильно зашумлены, может оказаться, что в выборке находится какая-то пара признаков, которые на обучении очень похожи. В этом случае совершенно непонятно, какой из этих признаков следует взять в модель. Наконец, еще один способ борьбы с переобучением линейной модели — это ограничение весов у признаков. Вы видели ранее в курсе, что когда линейная модель переобучается, веса у признаков становятся большими по модулю и разными по знаку. Ограничивая значение этих весов по модулю, можно с переобучением до какой-то степени побороться. Мы рассматривали два способа регуляризации: L2-регуляризатор добавляет к функционалу потерь слагаемое, равное сумме квадратов весов нашей линейной модели с множителем λ; L1-регуляризатор использует вместо суммы квадратов сумму модулей весов. Регрессия с L2-регуляризатором называется ридж-регрессией или гребневой регрессией, а с L1-регуляризатором — лассо. Очень важно, что константное слагаемое в регуляризатор входить не должно. Штрафуя за большое значение константы, переобучение мы не уменьшим, а вот качество моделей и на обучающей, и на тестовой выборке упадет очень сильно. Чтобы понять, чем отличаются L1 и L2-регуляризаторы давайте рассмотрим простой модельный пример. Пусть матрица «объекты-признаки» X — квадратная, диагональная и единичная, то есть на ее диагонали стоят единицы, а вся остальная часть заполнена нулями. В этом случае решение метода наименьших квадратов дает вектор весов w со звездочкой, j-тая компонента которого равна yj-тому. Если мы делаем гребневую регрессию, то j-тая компонента w уменьшается в (1 + λ) раз. Если мы делаем лассо, формула для j-той компоненты вектора весов более сложная. Давайте посмотрим на графики. На графиках показана зависимость j-той компоненты оптимального вектора весов w со звездочкой от yj-того. Если мы минимизируем среднеквадратичную ошибку, не используя регуляризаторы, то эта зависимость единичная, то есть wj-тое и yj-тое всегда одинаковые. На графиках — это пунктирная диагональная линия. Если мы использует регуляризацию L2, зависимость wj-тое со звездочкой все еще линейная, но веса прижаты к нулю. Лассо делает кое-что более интересное. Оптимальные веса лассо также прижимаются к нулю, однако в середине на этом графике появляется интервал размером λ, в котором веса обращаются в ноль в точности. То есть если у нас значению отклика маленькое, то вес получается нулевым. Именно поэтому лассо отбирает признаки. Если у признаков низкая предсказательная способность, в лассо они получают нулевой вес и таким образом из модели исключаются. С другой стороны, посмотреть на регуляризацию можно, если рассмотреть, как устроена ошибка регрессии. Давайте посмотрим на матожидание квадрата этой ошибки. Оно представляет собой сумму трех компонент. Первая компонента — это квадрат смещения, то есть квадрат разности между математическим ожиданием регрессионной модели, оцениваемой по выборке, и истинной неизвестной нам регрессионной модели. Вторая компонента — это дисперсия нашей выборочной оценки. А третья — это дисперсия шума, на который повлиять мы никак не можем. Метод наименьших квадратов дает оценки, которые имеют нулевое смещение. Однако, используя регуляризацию, мы можем получить оценки, у которых матожидание квадрата ошибки меньше за счет того, что дисперсия у них может быть меньше, несмотря а то, что эти оценки смещенные. Чтобы лучше понять баланс между смещением и дисперсией, представьте, что вы стреляете по мишеням. Среднее количество очков, которое вы при этом набираете, определяется двумя величинами: во-первых, средним облака точек, которое образуют результаты ваших выстрелов; во-вторых, разбросом выстрелов относительно этого среднего, то есть дисперсией. Естественно, больше всего очков вы получите, если вы будете стрелять точно и в цель. В этом случае у вас может быть какое-то небольшое смещение и маленькая дисперсия. Переобучение в линейных моделях приводит к тому, что вы стреляете с цель, но не точно. Смещения у вас нет, но дисперсия очень большая. Часто оказывается, что можно набрать больше очков, если вы будете стрелять не совсем в цель, но более точно. Именно это позволяет сделать использование регуляризации в линейных моделях. В байесовской статистике гребневая регрессия соответствует заданию нормального априорного распределения на коэффициенты линейной модели, а метод лассо — заданию Лапласовского априорного распределения на коэффициенты. Подробнее о байесовской статистике вы узнаете из гостевого видео, которое вас ждет в конце этого урока. Задача гребневой регрессии имеет аналитическое решение. К матрице X транспонированное X, которая обращается в методе наименьших квадратов, вы добавляете диагональную матрицу, у которой на диагонали стоят значения λ — веса при регуляризаторе. Для решения задачи лассо аналитического решения не существует. Однако есть очень эффективный численный способ получения решения, поэтому методом лассо тоже можно прекрасно пользоваться. Итак, в этом видео мы поговорили про регуляризацию как один из способов борьбы с переобучением линейных регрессионных моделей. Регуляризация приводит к тому, что вы получаете смещенные оценки коэффициентов модели, но суммарная ошибка таких моделей может быть меньше за счет того, что оценки коэффициентов имеют меньшую дисперсию. Это справедливо и для L1, и L2-регуляризации, однако про L1-регуляризацию мы еще выяснили, что она отбирает признаки, обнуляя веса у некоторых коэффициентов, и разобрались в том, почему так происходит. В следующем видео мы поговорим про логистическую регрессию.

[ЗАСТАВКА] Из этого видео вы узнаете, что такое логистическая регрессия, для чего она нужна и как работает. Логистическая регрессия – это метод обучения с учителем. Имея обучающую выборку по признаковому описанию объектов, вы пытаетесь предсказать значение отклика. Единственное отличие от задач линейной регрессии, которые мы рассматривали до этого, в том, что значение отклика у нас бинарное, то есть y принимает значение 0 и 1. На первый взгляд кажется, что это задача бинарной классификации, ее можно решать своими методами, которые мы рассматривали до этого в этом курсе. Однако оказывается, что линейная регрессия в этой задаче тоже может быть полезна. Если мы будем просто минимизировать среднеквадратичную ошибку между откликом y и линейной комбинацией факторов x, мы получим вектор весов w*, и можем мы его использовать следующим образом: если значение линейной комбинации факторов на объекте больше, чем 1/2, мы будем предсказывать, что наш объект будет относиться к классу 1; если меньше 1/2, то он будет относиться к классу 0. Этот метод называется методом линейного дискриминанта Фишера, и это один из самых старых методов классификации. На самом деле, мы можем хотеть предсказывать не просто метки классов на наших объектах, а вероятности того, что объекты относятся к одному из классов. Обозначим условную вероятность того, что y = 1 при условии x, за π(x). Вот эту функцию π(x) мы и хотим как-то оценить. Можно попробовать делать это с помощью обычной линейной регрессии. На первый взгляд кажется, что эта идея не самая плохая. Дело в том, что π(x), поскольку y – величина бинарная, совпадает с условным матожиданием, y при условии x. Это внушает нам надежду, что обычная минимизация методом наименьших квадратов может дать какую-то хорошую оценку. Проблема здесь заключается в том, что получаемая линейная комбинация факторов не обязательно лежит на отрезке от 0 до 1. Представьте, что вы предсказываете вероятность невозврата платежа по кредитной карте в зависимости от размера задолженности. Если на такие данные вы настроите линейную регрессию, вы получите, что при задолженности в 2000 долларов вероятность того, что клиент просрочит платеж по кредиту, составляет примерно 0,2. С другой стороны, вероятность того, что клиент просрочит платеж по кредиту при задолженности в 500 долларов, равна нулю. Это немного странно. Еще более странно, если задолженность составляет меньше 500 долларов, вероятность просрочки отрицательная. Если клиент должен больше 10000 долларов, вероятность больше 1. Это очень странный результат, совершенно непонятно, как его интерпретировать. Решить эту проблему можно следующим образом. Давайте возьмем какую-то функцию g, которая переводит интервал от 0 до 1 на множество всех действительных чисел, и построим оценку не для условного матожидания, как мы привыкли в линейной регрессии, а для функции g от этого условного матожидания, или, что то же самое, условное матожидание мы будем приближать обратной функцией к g, к g в минус первой, от нашей линейной комбинации факторов. Такое семейство моделей в статистике называется обобщенными линейными моделями. В задаче бинарной классификации в качестве функции g в минус первой берется функция, которая выглядит следующим образом: это сигмоида. Для одномерного случая значение w0, константа, определяет положение центра сигмоиды на числовой оси, а w1, вес при единственном факторе, определяет форму этой сигмоиды. Если вес w1 положительный, то сигмоида возрастающая, если отрицательный, то убывающая. Чем больше по модулю значение w1, тем круче наклон сигмоиды в области ее середины. Если мы возьмем сигмоиду и построим логистическую регрессию в задаче с вероятностью невозврата кредита, мы получим что-то более вменяемое. Наша вероятность будет принимать значение от 0 до 1, как мы и хотели. Кроме того, полезное свойство заключается в том, что изменение на краях диапазона значений признака x приводит к меньшим изменениям вероятности, которую мы моделируем. Это логично, изменение в плюс-минус 100 долларов при размере задолженности около 2000 приводит к большим изменениям вероятности просрочки платежа по кредитной карте, а изменение в плюс-минус 100 долларов при размере задолженности около 500 – к небольшим изменениям. Чтобы получить саму функцию g обобщенной линейной модели, мы произведем несложные арифметические преобразования выражения для π(x), которое мы записали до этого, и получим следующую функцию. Дробь, которая стоит здесь под логарифмом, представляет собой отношение вероятности того, что y = 1, к вероятности того, что y = 0, вероятности эти условные по x. Это отношение называется риском. Вместе с логарифмом это выражение называется логит. Именно поэтому метод называется логистической регрессии, потому что мы приближаем логит линейной комбинации наших факторов. Как эту модель можно настраивать? Давайте будем делать это методом максимизации правдоподобия. Запишем выражение для правдоподобия обучающей выборки и сразу для удобства возьмем от правдоподобия логарифм. Если мы поставим перед логарифмом знак минус, то получившуюся функцию мы будем не максимизировать, а минимизировать. Это немного более привычно, поскольку мы привыкли минимизировать функции потери в задачах регрессии. Такой функционал называется еще log-loss, или кросс-энтропия, у него много названий. Если мы переобозначим нулевой класс за минус первый, то путем несложных преобразований можно получить логистическую функцию потерь, которую вы уже встречали до этого. Задача максимизации правдоподобия в логистической регрессии очень хорошо решается числами, поскольку эта функция выпуклая, она имеет единственный глобальный максимум. Кроме того, мы можем очень хорошо оценивать ее градиент и гессиан. Проблемы возникают, только если наши объекты разных классов линейно разделимы в пространстве признаков. Представьте, например, что в вашей обучающей выборке все клиенты, задолженность которых составляет меньше 1300 долларов, платеж вовремя вернули, а все клиенты, задолженность которых больше 1400, платеж не вернули. В этом случае максимизация правдоподобия приводит к тому, что значение веса w1 при нашем признаке уходит в бесконечность. Вместо сигмоиды мы получаем вот такую ступеньку. Это плохо, поскольку мы переобучаемся на нашу обучающую выборку. Чтобы решить эту проблему, можно использовать методы регуляризации, о которых мы говорили до этого: можно использовать как l1, так и l2 регуляризацию. Вероятности, которые дает логистическая регрессия, можно использовать для классификации, для предсказания итоговых меток классов. Для этого нужно на вероятность, которую мы оцениваем, в каком-то месте поставить порог p0, и при значении вероятности выше порога предсказывать метку y = 1, а ниже порога – y = 0. Интуитивно кажется, что лучше всего выбирать порог = 1/2. На самом деле этот порог p0 можно подбирать для каждой задачи отдельно так, чтобы обеспечить оптимальный баланс между точностью и полнотой классификатора. Итак, в этом видео мы поговорили о логистической регрессии, мы узнали, что это регрессионный метод, позволяющий предсказывать вероятность того, что y = 1, по каким-то факторам x. В логистической регрессии используется линейная модель для логита, для логарифма отношения вероятностей y = 1 при условии x и y = 0 при условии x. Оценка параметров логистической регрессии делается методом максимального правдоподобия. Вот и все. Далее в программе вас ждет знакомство с некоторыми важными техническими трюками, которые часто используются при настройке линейных моделей.