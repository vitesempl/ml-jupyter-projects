[ЗАСТАВКА] В прошлом уроке мы с вами узнали, что такое решающие деревья, и выяснили, что они могут восстанавливать очень сложные закономерности, из-за чего они склонны к переобучению. Решающее дерево слишком легко подгоняется под обучающую выборку и получается непригодным для построения прогнозов. Бороться с переобучением довольно сложно. Надо либо использовать критерии остановок, которые слишком простые и не всегда помогают, либо делать стрижку деревьев, которая, наоборот, слишком сложная. Зачем же мы тогда тратили на них время? Оказывается решающие деревья очень хорошо подходят для объединения в композиции, для построения одного непереобученного алгоритма на основе большого количества решающих деревьев. В этом и следующем уроке мы будем говорить, как именно объединять их в такие композиции, а пока давайте еще раз вспомним проблемы решающих деревьев. Представьте, что у нас есть вот такая выборка, и мы обучили на ней решающее дерево до конца, то есть до тех пор, пока в каждом листе не оказалось по одному объекту. Разделяющая поверхность будет очень плохой. Она очень разрезанная. Даже если есть объект, который попадает в гущу другого класса, например синяя точка внизу, разделяющая поверхность пытается уловить этот объект, выдать на нем правильный синий ответ. Из-за этого поверхность получается очень переобученная. Если мы немного изменим обучающую выборку, например выкинем пару объектов и обучим решающее дерево на том, что осталось, разделяющая поверхность будет все еще очень изрезанной и переобученной, но совершенно другой. Она очень неустойчива к изменениям в выборке. Итак, у решающих деревьев есть два недостатка. Первый — они очень сильно переобучаются, а второй — они очень неустойчивы, они очень сильно меняются даже при небольших изменениях в выборке. И на самом деле второй пункт можно обратить в их достоинство с помощью композиции, но об этом чуть позже. А пока давайте поговорим в целом о том, что такое композиция алгоритмов. Итак, композиция — это объединение n алгоритмов в один. Представьте, что мы каким-то образом нашли N большое алгоритмов b1, ..., bn. Пока не важно, откуда мы их взяли. Просто представьте, что мы их как-то обучили. Чтобы объединить их в композицию, мы усредняем их ответы, то есть суммируем ответы всех этих алгоритмов b1, ..., bn на объекте x, и делим на N большое, то есть на количество этих алгоритмов. Если мы решаем задачу классификации, то далее мы берем знак от этого среднего; если регрессии, то просто возвращаем это среднее как ответ. Алгоритм a(x), который возвращает знак среднего или просто среднее и называется композицией n алгоритмов. А алгоритмы b1, ..., bn, которые мы объединяем в композицию, называются базовыми алгоритмами. Рассмотрим простой пример. Представьте, что в нашей композиции 6 базовых алгоритмов, и есть некоторый объект x, на котором наши базовые алгоритмы выдали вот такие ответы: −1, −1, 1, −1, 1 и −1. Это задача классификации с двумя классами. Какие-то алгоритмы отнесли наш объект к классу +1, какие-то — к классу −1. Усредним ответы. Получим при этом −2/6 или −1/3. Знак этой дроби — это −1, значит ответ композиции — это −1. Мы отнесли объект к классу −1, поскольку именно за этот вариант проголосовало большинство базовых алгоритмов. Итак, для того чтобы строить композицию, нужно обучить n базовых алгоритмов. При этом понятно, что нельзя их обучать на всей обучающей выборке. Они получатся одинаковыми, и в их усреднении не будет никакого смысла. Нужно делать их немного различными. Как этого достичь? Например, с помощью рандомизации, то есть обучать их по разным подвыборкам обучающей выборки. Поскольку решающие деревья очень сильно меняются даже при небольших изменениях обучающей выборки, такая рандомизация с помощью подвыборок будет очень хорошо влиять на их различность. Один из популярных подходов по построению подвыборок — это бутстрап. В чем он заключается? Представьте, что у нас есть полная обучающая выборка, состоящая из l объектов. Мы генерируем из нее l объектов с возвращением, то есть мы берем из нее некоторый случайный объект, записываем его в новую выборку и возвращаем обратно. То есть в какой-то момент мы можем снова его вытянуть и снова поместить в обучающую выборку. При этом новая выборка будет тоже иметь размер l, но при этом какие-то объекты будут в ней повторяться, а какие-то объекты исходной обучающей выборки не встретятся ни разу. Можно показать, что количество различных объектов в бутстрапированной выборке будет равняться 0,632 * l, то есть примерно 63 % объектов исходной выборки будет содержаться в бутстрапированной. Есть и другой подход к рандомизации. Это просто генерация случайного подмножества обучающей выборки. Например, мы берем случайные 50 % объектов и на них обучаем базовый алгоритм. Этот подход чуть хуже, потому что в нем появляется гиперпараметр — размер подвыборки. В нашем примере это было 50 %. В случае же с бутстрапом никаких параметром нет. Он без какой-либо настройки выдает нам подвыборку, что гораздо удобнее. Если мы построим с помощью бутстрапа 100 базовых решающих деревьев и объединим их в композицию, то мы получим вот такую разделяющую поверхность в нашем примере, который мы разбирали в начале. Видно, что поверхность все еще довольно сложная, но при этом гораздо менее переобученная. Она уже не пытается подгоняться под большинство объектов, которые залезают в гущу другого класса. Она более или менее угадывает разделяющую поверхность между двумя классами. Да, все еще есть некоторые погрешности, но и их можно было бы устранить, если построить еще больше базовых алгоритмов. Итак, мы с вами вспомнили, что решающие деревья являются очень сильно переобученными алгоритмами и так же неустойчивы к любым изменениям в обучающей выборке. При этом усреднение ответов нескольких базовых алгоритмов, базовых решающих деревьев повышает качество композиции, дает более высокое качество и менее переобученную разделяющую поверхность, чем отдельные базовые алгоритмы. При этом строить отдельные базовые решающие деревья или базовые алгоритмы можно разными способами, например с помощью бутстрапа или генерации случайных подвыборок. В следующем видео мы продолжим разговор о композициях и разберемся, почему же усреднение улучшает качество базовых алгоритмов.

[БЕЗ_ЗВУКА] В этом видео мы поговорим о разложении ошибки на смещение и разброс — технике, которая позволяет понять, почему усреднение алгоритмов позволяет повысить их качество. Итак, ы прошлом видео мы выяснили, что усреднение решающих деревьев в задаче классификации делает разделяющую поверхность менее переобученной, позволяет повысить качество. Почему это происходит? В целом подход с усреднением довольно известный, например, в физике довольно распространенный подход — сделать несколько измерений одной величины и усреднить. И это уменьшает ошибку измерений. Но давайте попробуем понять, почему это работает хорошо именно в машинном обучении. Для этого нам пригодится разложение ошибки на смещение и разброс, о котором вы уже узнали в прошлом модуле. Итак, идея состоит в том, что ошибка алгоритма на новых тестовых данных складывается из трех компонент: шума, смещения и разброса. При этом все они характеризуют разные аспекты данных и модели, с помощью которой вы решаете задачу на этих данных. Начнем с шума. Шум показывает, какова ошибка лучшей из всех возможных в мире моделей на данной задаче. Поскольку данные могут быть зашумленные, и на них в принципе невозможно получить нулевую ошибку, идеальная модель тоже может ошибаться, и шум показывает, насколько сильно будет ошибаться идеальная модель на этой задаче. Шум никак нельзя улучшить, это характеристика ваших данных, с этим приходится жить. Вторая компонента разложения — это смещение. Чтобы понять ее, давайте проведем некоторый умственный эксперимент. Педставьте, что обучающая выборка — это некоторая случайная величина, ее можно генерировать из некоторого распределения. Генерируя ее снова и снова, будем получать немного разные обучающие выборки из одного и того же распределения. Итак, допустим, мы сгенерировали много обучающих выборок из этого распределения. На каждой мы обучили нашу модель, например решающее дерево. Смещение показывает, насколько сильно отклоняется средний прогноз по всем обученным моделям от прогноза идеальной модели. По сути, смещение говорит, насколько мы можем аппроксимировать идеальную модель, насколько наше семейство алгоритмов сложное и позволяет восстанавливать сложные закономерности. Если это избразить с помощью диаграммы, то процесс вычисления смещения выглядит примерно так. Мы генерируем много обучающих выборок x1, ..., xn. На каждой обучаем свой алгоритм b1(x), b2(x), ..., до bn(x) и усредняем их ответы на объекте x. После чего сравниваем средний ответ с ответом идеального алгоритма y(x). Наконец, разброс — третья компонента разложения. Разброс вычисляется по похожей схеме. Мы генерируем много обучающих выборок, на каждой обучаем нашу модель, например решающее дерево, и дальше смотрим на дисперсию ответов всех этих моделей. Диаграмма получается примерно такой же: мы генерируем много выборок x1, ..., xn, обучаем на них базовые алгоритмы и дальше смотрим на дисперсию выборки b1(x), b2(x), ..., bn(x) на каких то объектах. Чем больше эта дисперсия, чем выше разброс, тем сильней алгоритм зависит от небольших изменений обучающей выборки. Например, решающие деревья обладают этим свойством. Если чуть-чуть поменять обучающую выборку, дерево меняется очень сильно. На этой картинке можно увидеть иллюстрацию смещения и разброса. Зеленая кривая — это истинная зависимость. Будем генерировать из нее 10 случайных точек, добавлять к ней небольшой шум и считать это обучающей выборкой, и будем обучать на этом полином третьего порядка. На левой картинке красными кривыми изображены полиномы, которые получаются при немножко разных обучающих выборках, сгенерированных таким образом. Видно, что средний полином, который изображен справа красной кривой, идеально угадывает истинную зависимость — зеленую кривую. Но при этом отдельные полиномы могут отличаться от этой зависимости, разброс довольно большой в каждой точке. Итак, мы здесь получили семейство алгоритмов, которые имеют низкое смещение и довольно большой разброс. Вернемся к линейным моделям. Вспомним, что их смещение может быть довольно большим. Линейные модели могут восстанавливать только линейные зависимости, но при этом в большинстве задач зависимости нелинейные, из за чего смещение большое и линейная модель в принципе не может восстановить сложные зависимости. При этом разброс маленький. У линейной модели параметров столько, сколько признаков. Это очень мало. Вряд ли они сильно изменятся, если чуть-чуть поменять обучающую выборку. Итак, у линейных моделей большое смещение и низкий разброс. Решающие деревья — это полная противоположность. У них низкое смещение. Они могут восстанавливать очень сложные закономерности, но при этом разброс очень большой, поскольку деревья очень сильно меняются даже при небольших изменениях обучающей выборки. Поговорим теперь про усреднение базовых алгоритмов. Оказывается, что если мы усредняем много базовых алгоритмов, например построенных по бутстрапированным подвыборкам, то мы не меняем их смещение. Смещение композиции, смещение среднего алгоритма совпадает со смещением отдельного базового алгоритма. Таким образом, если мы усредняем деревья, а у деревьев низкое смещение, то и у их композиции тоже будет низкое смещение. Композиции будут в состоянии восстанавливать сложные закономерности, а вот разброс меняется. Разброс композиций состоит из двух слагаемых. Первое — это разброс одного базового алгоритма, который делится на N большое — на число базовых алгоритмов. Второе слагаемое — это корреляции между двумя базовыми алгоритмами. Обратите внимание, что если базовые алгоритмы независимы, если их прогнозы не коррелируют между собой, то разброс композиции — это всего лишь разброс одного базового алгоритма, деленный на N. Это прекрасно. Если алгоритмы базовые не коррелированы и это решающие деревья, то композиция будет иметь низкое смещение, поскольку у базовых алгоритмов низкое смещение, и низкий разброс, если мы возьмем большое N. У деревьев большой разброс, мы поделим его на большое число и получим низкий разброс композиции. Итак, если базовые алгоритмы не коррелированы, то композиция может дать нам идеальный алгоритм. Но, к сожалению, это не всегда так. Базовые алгоритмы обучаются на одной и той же выборке или на подвыборках одной и той же выборки, из за чего они оказываются зависимыми, их ответы коррелированы, и очень сложно сделать их полностью независимыми. Но при этом можно попытаться хотя бы уменьшить корреляцию, и к этому есть два подхода. Первый — это то, о чем мы уже говорили: бэггинг — обучение отдельных базовых алгоритмов на случайных подвыборках объектов. За счет этого базовые алгоритмы получаются немножко разные, и при этом, наверное, чем меньше мы будем брать обучающую выборку для одного базового алгоритма, тем более независимые они будут. Но при этом не стоит здесь увлекаться. Если обучающая выборка будет слишком маленькой, то с переобучением решающих деревьев уже ничего нельзя будет поделать. Но при этом давайте вспомним, что у обучающей выборки есть два измерения: это объекты и признаки этих объектов. Если рассматривать матрицу «объекты–признаки», то по строкам записаны объекты, по столбцам — признаки, и бэггинг делает рандомизацию по строкам, то есть он выбирает случайное подмножество строк и обучает на этом подмножестве один базовый алгоритм. Но при этом можно можно брать и случайное подмножество столбцов. Это называется методом случайных подпространств. В этом случае мы сэмплируем — мы выбираем случайное подмножество признаков (столбцов) и только на этих признаках обучаем очередной базовый алгоритм. При этом два данных подхода — бэггинг и метод случайных подпространств — можно объединять. Можно сэмплировать как столбцы, так и строки матрицы «объекты–признаки» и обучать на такой подматрице каждый базовый алгоритм. При этом обратите внимание, что у метода случайных подпространств есть гиперпараметр — какую долю признаков мы выбираем? Можно от него избавиться, если тоже делать бутстрап, но это будет довольно странно. У матрицы «объекты–признаки» будут одинаковые столбцы — коррелирующие признаки, что довольно плохо. Итак, мы с вами обсудили, что такое разложение ошибки на смещение и разброс. Смещение характеризует, насколько сложно у нас семейство алгоритмов, насколько оно может восстанавливать сложные закономерности, а разброс говорит, насколько чувствительны алгоритмы к небольшим изменениям выборки. При этом мы выяснили, что усреднение алгоритмов, объединение их в такую композицию не меняет смещение и при этом уменьшает разброс. При этом чем менее коррелированы базовые алгоритмы, тем сильнее уменьшение разброса. И обсудили два подхода к уменьшению корреляции между базовыми алгоритмами: бэггинг и метод случайных подпространств. А в следующем видео мы поговорим о еще одном способе рандомизации случайных деревьев, в котором мы углубляемся и пытаемся рандомизировать сам процесс построения дерева.


[ЗАСТАВКА] В этом видео мы поговорим о случайных лесах, которые являются одним из лучших способов объединения деревьев в композиции. В прошлый раз мы выяснили, что ошибка любого алгоритма на контрольной выборке складывается из шума смещения и разброса, при этом на шум мы никак повлиять не можем. Усреднение базовых алгоритмов не меняет смещение, то есть если базовые алгоритмы были сильные, то и композиция будет довольно сильной, способной восстанавливать сложные закономерности, а вот разброс усреднение уменьшает, и при этом чем менее коррелированы базовые алгоритмы, тем сильнее будет уменьшение разброса при усреднении. Мы обсудили два способа понижения корреляции между базовыми алгоритмами, а именно бэггинг, когда мы обучаем каждый базовый алгоритм в случайном подмножестве объектов, и метод случайных подпространств, когда мы обучаем каждый базовый алгоритм на случайном подмножестве признаков. Их можно объединять, использовать одновременно и тот, и другой. Но их оказывается мало, и чтобы добиться еще более маленькой корреляции между базовыми алгоритмами, имеет смысл сделать более случайным процесс построения этих базовых алгоритмов. Давайте подумаем, как можно сделать рандомизированным процесс построения решающих деревьев. Для этого вспомним, как он устроен. Итак, решающее дерево строится жадно. Мы начинаем с одной вершины, разбиваем ее на две и далее производим ветвление уже этих двух поддеревьев, этих двух вершин до тех пор, пока не будет выполнен некоторый критерий останова. При этом разбиение мы осуществляем следующим образом. Нам нужно найти такое условие разбиения, а условия у нас очень простые, они проверяют j-тый признак и сравнивают его значение с некоторым порогом t. Если значение j-того признака меньше или равно этого порога, то объект идет в левое поддерево, если больше порога, то в правое поддерево. И нам нужно выбрать такие признак j и порог t, при которых будет достигаться минимум некоторого критерия ошибки, который характеризует, насколько хорошо разбивать данную вершину именно по такому условию, именно таким способом. И мы искали лучший признак j и порог t просто перебором, поскольку их – конечное число. Рандомизировать процесс можно следующим образом: будем искать лучший признак j для разбиения не из всех возможных признаков выборки, а из некоторого случайного подмножества признаков, и размер этого подмножества будет равен некоторой константе q. Оказывается, что этот подход действительно позволяет сделать деревья менее коррелированными. На этом графике по оси x отложено q, то есть то, из скольки случайно выбранных признаков мы выбираем лучшие при конкретном разбиении, а по оси y отложена корреляция между двумя базовыми решающими деревьями. Видно, что чем меньше q, чем меньше простор при выборе лучшего разбиения, тем меньше корреляция между решающими деревьями. Разница в корреляции при выборе абсолютно случайного признака, если q = 1, и при выборе из всего множества признаков, достигает несколько раз. Корреляция уменьшается в несколько раз при уменьшении q. Для q есть некоторые рекомендации, которые неплохо работают на практике. Если мы решаем задачу регрессии, то имеет смысл брать q = d/3, то есть 1/3 от общего числа признаков. Если мы решаем задачу классификации, то имеет смысл брать q = √d, корню из числа признаков. Итак, давайте полностью проговорим, как устроен алгоритм построения случайного леса. Мы хотим построить композицию из N решающих деревьев. Что мы делаем для построения каждого из них? Сначала мы генерируем случайную подвыборку X с волной с помощью бутстрапа. После этого мы обучаем на этой выборке X с волной очередное решающее дерево bn(x). И построение обладает двумя особенностями: во-первых, мы строим дерево до тех пор, пока в каждом листе окажется не более некоторого числа объектов n минимальное. Очень часто эту константу берут равной 1, то есть строят деревья до конца, до тех пор, пока в каждом листе не окажется по одному объекту обучающей выборки. В результате мы получим очень сложные, очень переобученные решающие деревья с низким смещением, но это нам и нужно при построении композиции. Также при выборе оптимального разбиения при построении дерева мы ищем лучший признак не из всех признаков выборки, а из случайного подмножества признаков размера q. Обратите внимание, что случайное подмножество размера q выбирается заново при каждом новом разбиении вершины. Этим подход отличается от метода случайных подпространств, где мы выбираем случайное подмножество один раз перед построением базового алгоритма, здесь же оно будет свое в каждой вершине. Далее мы объединяем построенные деревья в композицию. Если это регрессия, то мы просто усредняем их, если это классификация, то мы берем знак от среднего ответа. Одна из особенностей случайных лесов состоит в том, что они не переобучаются при росте числа базовых алгоритмов. На этом графике изображена зависимость качества случайных лесов при разном значении параметра q в зависимости от числа базовых алгоритмов. Видно, что при росте числа базовых алгоритмов ошибка на тесте уменьшается, в какой-то момент выходит на асимптоту и остается на этом уровне, не происходит роста ошибки при росте числа базовых алгоритмов. Итак, мы разобрались, что такое случайный лес. Это композиция решающих деревьев, в которой эти решающие деревья строятся довольно случайно: при каждом разбиении оптимальный признак выбирается из случайного подмножества признаков. За счет этого сильно уменьшается корреляция между деревьями, и поэтому разброс композиции получается довольно низким. Особенность случайного леса состоит в том, что он не переобучается при росте числа базовых алгоритмов. За счет этого случайный лес считается одним из самых универсальных алгоритмов. У него практически нет гиперпараметров. Мы можем брать просто довольно большое число базовых деревьев и при этом будем получать хороший, не переобученный алгоритм. В следующем видео мы поговорим о различных трюках, которые можно делать со случайными лесами.

[БЕЗ_ЗВУКА] В этом видео мы поговорим о некоторых трюках, которые случайные леса позволяют выполнять в ряде задач. В прошлый раз мы разобрались, как обучают случайные леса. Это довольно простой алгоритм, который не переобучается при росте числа базовых решающих деревьев. При этом у него есть ряд интересных особенностей, о которых мы сейчас поговорим. Первое состоит в том, что каждое решающее дерево обучается независимо от всех остальных деревьев. При обучении n-ного дерева никак не используется информация о построении остальных базовых решающих деревьев. Это можно использовать при распараллеливании. Можно обучать каждое решающее дерево независимо на своем ядре или на своем компьютере. При этом распараллеливание получается идеальное. Мы получаем линейное уменьшение времени обучения при линейном увеличении числа ядер. Вторая особенность состоит в том, что каждое дерево обучается на бутстрапированной выборке. При этом, как мы уже обсуждали, в бутстрапированной выборке оказывается примерно 63 % от общего числа объектов в обучающей выборке, а остальные никак не используются при обучении данного дерева. Это можно использовать, и подход, который так делает, называется out-of-bag. Итак, представьте, что у нас есть N большое решающих деревьев, при этом каждое из них было обучено на своей бутстрапированной подвыборке. Первое — на подвыборке x1, второе — на подвыборке x2, и так далее. При этом если мы возьмем конкретный объект обучающей выборки xi-тое, то то на нем не обучались примерно 37 % решающих деревьев. Это можно использовать. Если мы построим прогноз для данного объекта только по тем деревьям, которые не обучались на нем, мы получим некоторое предсказание. И оказывается, что оно очень неплохо характеризует обобщающую способность случайного леса. Нам не нужна дополнительная выборка или кросс-валидация, чтобы оценить качество леса, поскольку в обучающей выборке есть объекты, на которых обучались не все деревья. Итак, чуть более формально о том, как оценить качество случайного леса с помощью подхода out-of-bag. Ошибка в данном подходе вычисляется по вот такой формуле. Мы считаем сумму по всем объектам обучающей выборки от 1 до l и для каждого объекта рассчитываем ошибку некоторого хитрым образом вычисленного прогноза по сравнению с идеальным ответом yi-тое. Давайте чуть подробнее посмотрим, как вычисляется этот прогноз. В начале в нем стоит дробь, которая показывает, какая доля объектов в случайном лесе не обучалась на объекте xi-тое. По сути, в знаменателе стоит сумма по всем деревьям индикаторов того, что i-тый объект не входил в обучающую выборку для этого дерева. Итак, перед суммой стоит доля деревьев, которые не обучались на объекте xi-тое. Сумма у нас происходит по всем деревьям от 1 до N большого, и для каждого дерева мы прибавляем 0, если данное дерево обучалось на объекте xi-тое, и прогноз данного дерева на объекте xi-тое, если он не использовался при обучении данного дерева. В итоге прогноз вычисляется, как средний ответ по всем деревьям на данном объекте, которые не обучались на нем. Также с помощью оценки out-of-bag можно оценивать важность признаков. И на самом деле случайные леса очень хорошо позволяют отбирать самые важные признаки, но об этом мы будем говорить уже в следующем курсе нашей специализации. Итак, мы обсудили две особенности случайного леса. Первая состоит в том, что он идеально параллелится. Каждое дерево строится независимо, и поэтому его можно строить на своем ядре или на своем компьютере. Также при обучении каждого отдельного дерева в случайном лесе используется не вся обучающая выборка, а лишь некоторое ее подмножество, а те объекты, которые не использовались при обучении, можно использовать для оценивания качества случайного леса. Этот подход называется out-of-bag, и он позволяет избежать использования дополнительной отложенной выборки или кросс-валидации. И при этом можно показать, что out-of-bag оценка очень неплохо приближает оценку, вычисленную по кросс-валидации. На этом урок про случайные леса окончен. На следующем уроке мы поговорим о другом подходе к построению композиций — о градиентном бустинге.


[ЗАСТАВКА] Привет! В этом видео мы займемся построением модели «случайный лес» с помощью модуля sklearn.ensemble. Анализировать случайный лес мы будем на очень интересном наборе данных — это данные задачи bioresponse на kaggle. По данным характеристикам молекулы нам требуется предсказать, будет ли дан биологический ответ. Всего нам доступно 1776 характеристик молекулы, при этом матрица признаков нормализована. Для работы мы будем использовать обучающую выборку с сайта kaggle — она также доступна, называется train_csv. Для начала давайте загрузим файл и проанализируем данные. Анализировать данные будем в виде дейтафрейма, загружаем данные в виде... с помощью функции read_csv. Теперь давайте посмотрим, как они выглядят. Получили такую таблицу. В первом столбце мы видим нашу целевую переменную — она принимает значение 1 или 0, в зависимости от того, был ли дан биологический ответ. Все остальные столбцы соответствуют признакам. Давайте посмотрим размер нашей матрицы. Да, мы видим, что нам доступны данные о 3751 молекуле. Итак, давайте выведем названия колонок. Да, видим, что все данные называются большой буквой D и дальше индексом — номером признака. Так, теперь давайте отдельно отрежем целевую переменную — нам так удобней будет анализировать данные, и заодно сразу же посчитаем распределение наших объектов по классам. Ну вот видим, что задача почти сбалансирована. Теперь давайте отдельно отрежем данные, и вся подготовительная работа закончена. Теперь можно переходить непосредственно к построению модели. Построить модель RandomForest с помощью sklearn очень просто — для этого достаточно создать объект класса RandomForestClassifier с нужными параметрами. А также несложно применить и обучить эту модель — для этого нужно воспользоваться уже известными вам методами fit и predict. С другой стороны, анализировать качество модели, а то и параметров, вы тоже уже умеете. Это можно делать с помощью поиска по сетке или случайного поиска. Давайте мы не будем этого делать, а вместо этого решим другую задачу — проанализируем, как зависит качество модели от количества обучающих объектов выборки. Для этого давайте создадим нашу модель. Для начала будем строить случайный лес над 50 деревьями, каждый из которых будет иметь глубину не больше 2. Создаем такой объект. И теперь давайте построим следующий график — нам будет интересно посмотреть, как меняется качество на обучающей и тестовой выборке, в зависимости от того, на скольких объектах мы обучаемся. Для того чтобы получить такие графики, у sklearn есть специальная функция под названием learning_curve. Она позволяет нам сделать следующее — ей можно передать на вход нужный нам алгоритм, передать данные и целевую функцию, а также сказать, в каких пропорциях мы хотим обучаться, то есть на каких долях обучающей выборки мы хотим строить модель. После этого с помощью этого метода будут построены несколько моделей, мы получим оценку качества на каждом объеме обучающей выборки, и нам будут возвращены размер обучающей выборки, оценки качества на «трейне» и оценка качества на тесте. Имея такие данные, мы легко сможем проанализировать, как качество на обучении и тесте меняется от объема обучающей выборки. Вот давайте сделаем такую вещь. Мы передаем в функцию наш классификатор, который мы создали ранее. Далее передаем туда данные, которые мы также подготовили на предварительном шаге. И говорим, что мы будем обучать модель на следующих данных: сначала мы возьмем 0,1 от обучающей выборки и далее будем двигаться с шагом 0,2 до 1. Оценивать качество будем с помощью уже знакомой нам метрики accuracy и будем делать кросс-валидацию на 3 фолда. Давайте запустим. Процесс занимает некоторое время, потому что обучаются довольно много моделей. И теперь давайте посмотрим, как выглядит вывод нашей функции. Ну для начала мы видим, что train_sizes — размер обучающей выборки — был преобразован из долей в конкретное количество объектов, на которых мы обучались. То есть мы видим, что минимальное количество обучающих объектов в рамках нашего эксперимента составляет 250, максимальное — 2250. Также нам доступны оценки качества на обучении и оценки качества на тесте. Так как у нас проводилась кросс-валидация, я сразу же сделала усреднение по всем фолдам — это делается с помощью команды mean. Аргумент axis = 1 означает, что мы будем усреднять по строчкам. Вот в данном случае каждая строка — это результат измерения кросс-валидации, поэтому нам это подходит. Теперь давайте построим график. Сразу добавим на график сетку и будем строить две кривые — качество обучения на обучающей выборке и на тестовой выборке. Давайте посмотрим. Так, мы видим, что в начале качество на обучающей выборке падает — приблизительно до отметки 1250 деревьев, и дальше качество меняется очень медленно. С другой стороны, на тестовой выборке качество продолжает расти приблизительно до этой же точки, и дальше оно также перестает меняться. Какой вывод мы можем сделать из этого? Дальнейший рост обучающей выборки вряд ли скажется на качестве нашей модели. Это говорит о том, что модель данной сложности не может многое выиграть за счет того, что мы обогатим данные. Что же делать в такой ситуации? Давайте попробуем увеличить сложность модели — возможно, это приведет к улучшению ее качества. Так как мы с вами обучали модель на деревьях глубины 2, давайте увеличим глубину деревьев — это даст нам дополнительные возможности. Снова создаем классификатор RandomForestClassifier, но в этот раз указываем ему параметр max_depth = 10 — это максимально возможная глубина деревьев. Теперь давайте еще раз запустим команду learning_curve и построим кривую обучения на тесте и на обучении, при этом мы будем делать это по тем же самым точкам, по тем же самым долям обучающей выборки. [ЗВУК НАЖАТИЯ КЛАВИШИ] Итак, наши данные готовы, теперь строим график. Здесь мы видим в некоторой степени противоположную ситуацию — мы видим, что с ростом обучающей выборки, качество на тесте продолжает расти. В конце оно начинает расти несколько медленнее, но тем не менее тренд заметен. То же самое можно сказать про обучение — качество на обучающей выборке продолжает падать не очень быстро. Отсюда мы можем сделать вывод, что модель данной сложности действительно получает некоторые преимущества от того, что мы добавляем объекты в обучение. Таким образом, в данном случае имеет смысл увеличивать объем обучающей выборки. Объем обучающей выборки и сложность модели значительно сказываются на времени построения модели. С этой точки зрения строить кривые обучения очень полезно — вы можете проанализировать, имеет ли смысл добавлять больше данных в обучение. А мы на этом заканчиваем. На этом уроке мы познакомились с RandomForest, научились строить его в sklearn, а также проанализировали кривые обучения для деревьев различной глубины. На этом мы заканчиваем изучение RandomForest, а в следующем модуле вы познакомитесь с алгоритмом градиентного бустинга.