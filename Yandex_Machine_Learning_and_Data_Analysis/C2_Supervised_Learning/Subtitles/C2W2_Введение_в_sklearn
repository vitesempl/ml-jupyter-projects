[БЕЗ_ЗВУКА] Привет. В этом модуле мы начнем знакомиться с библиотекой Scikit-learn — удобным и эффективным инструментом для анализа данных и решения задач машинного обучения в Python. С помощью библиотеки Sklearn можно пройти практически все шаги по анализу данных, начиная от генерации модельных данных, заканчивая построением моделей и оценкой их качества. В этом модуле мы с вами научимся генерировать модельные данные, удовлетворяющие заданным свойствам, научимся строить разбиения данных с помощью кросс-валидации, научимся строить линейные модели, а также оценивать их качество на основе различных метрик. Начнем мы с задачи генерации модельных данных и рассмотрим модуль sklearn.datasets. По ссылке ниже приведена документация по этому модулю, мы с вами для начала его импортируем, и так как в процессе работы мы будем строить много графиков, сразу же подключим magic pylab. Модуль datasets предлагает нам набор удобных функций для построения модельных данных. Существуют такие функции как make_classification или make_regression — это такие функции, которые позволяют нам строить dataset общего вида и гибко специфицировать параметры, которые нас интересуют. А также есть довольно удобные функции, такие как make_circles или make_checkerboard и другие, которые позволяют нам генерировать красивые наборы данных, которые удобно отрисовывать на плоскости. Эти данные обычно состоят из двух признаков (x и y) и позволяют получить красивые фигуры. Вот мы именно с такой функции начнем и рассмотрим функцию make_circles. Она позволяет нам получить dataset, который на плоскости выглядит, как две вложенные окружности. Вот давайте такой объект создадим и посмотрим, как он выглядит. Ну в качестве объекта мы получили тьюпл, состоящий из двух элементов. Первый элемент — это непосредственно набор данных, список описаний наших объектов. И второй элемент — это метки классов. Вот давайте выведем их на экран и посмотрим, как они выглядят. Мы видим, что каждый объект представляется списком из двух элементов. Первый элемент — это x координата, второй элемент — это y координата. И наши метки проставляются как цифры 0 или 1 в случае бинарной классификации, с которой мы сейчас и работаем. Вот для того, чтобы наглядно посмотреть, как же наш набор данных выглядит, я предлагаю его визуализировать. Для этого нам понадобится импортировать дополнительно объект ListedColormap. Сейчас посмотрим, зачем же он нужен. Итак, чтобы отрисовать наш объект, мы воспользуемся функцией scatter. Эта функция позволяет нам отрисовывать точки, зная их x и y координаты. На входе нужно подать два списка: первый — список x координат, второй — список y координат. т Зная, как выглядит наш набор данных, мы понимаем, как же этот список получить. Очень простая комбинация функций map и lambda позволяет нам сначала достать x координаты, в данном случаем мы проходимся по нашим объектам и берем нулевой элемент из списка, и аналогично можно получить y координату, только в данном случае в lambda функции мы указываем первый элемент из списка. Дальше нам нужно указать, какими цветами отрисовывать объекты. Понятно, что для наглядности мы хотим объекты разных классов видеть разными цветами. Конечно, можно было бы сделать так: можно было бы дважды вызвать функцию scatter, первый раз отрисовать объекты нулевого класса одним цветом, второй раз отрисовать объекты первого класса другим цветом. Но представьте, если бы у нас было больше двух классов, например 20, тогда, конечно, нам было бы не очень удобно. Вот для того, чтобы этого не делать, можно воспользоваться так называемым Colormap. В таком случае в качестве цветов мы можем передать просто числа, последовательность номеров, и далее передать в эту функцию некоторый объект, который знает связь между номером и цветом. В данном случае это и есть объект Colormap. Мы создаем его вот здесь, мы говорим, что у нас есть всего 2 цвета, в данном случае я выбрала красный и желтый, и таким образом получается, что объекты с меткой 0 будут иметь красный цвет, с меткой 1 — желтый. Вот давайте все это запустим и посмотрим, что мы получим. Итак, видим, что мы получили наш рисунок, это как раз две окружности: вложенная окружность желтого цвета и внешняя окружность красного. Ну давайте сразу же обернем эти команды в одну функцию, потому что нам она в дальнейшем понадобится, чтобы запись была несколько короче. Итак, в принципе мы видим, что этот набор данных довольно простой. Что если нам захочется немножечко усложнить себе задачу и получить окружность не с такими четкими краями. Это тоже очень просто делается. Для этого нужно передать внутрь нашей функции параметр noise, означает некоторый шум, который мы добавим к нашим данным. Давайте это сделаем. Сначала добавим не очень сильный шум и посмотрим, как же изменится наша картинка. Ну вот видим, что задача с точки зрения классификации несколько усложнилась, наши границы не такие четкие. Давайте еще немножечко усложним задачу и видим, что да, теперь стало еще сложнее. Таким образом, с помощью этого параметра мы можем гибко влиять на то, насколько сложную задачу мы получили. Теперь давайте рассмотрим функцию make_classification, она позволяет гибко создавать набор данных, который нас интересует. В частности, мы можем указать количество объектов, которые мы хотим видеть в нашем dataset, количество признаков, также мы можем задать, сколько из этих признаков должно быть информативными, а сколько избыточными. Можем даже ввести повторяющиеся признаки. Для начала давайте построим очень простую задачу классификации. Для того чтобы визуализировать набор данных, а мы обязательно захотим это сделать, нам удобнее работать с двумя признаками. Самостоятельно вы, конечно, можете задать любое количество признаков. Вот давайте зададим два признака, скажем, что один из них будет информативным, и опять же будем решать задачу бинарной классификации, зададим число классов равное двойке. Теперь давайте посмотрим, как наш dataset выглядит. Ну вот видим, что получили достаточно простую задачу. Очевидно, где нужно провести разделяющую плоскость в случае классификации. Теперь давайте немножечко усложним. Будем решать задачу классификации на 4 класса. Для этого изменим значение параметра n_classes (number of classes) и скажем, что пусть в данном случае два признака будут информативными. Нам придется еще изменить объект colors, потому что нам нужно несколько больше цветов для отрисовки этого набора данных. Вот давайте на него посмотрим. Видим, что мы получили четыре облака точек. Таким образом, с помощью этой функции можно генерировать набор данных практически любой сложности и гибко решать интересующие вас задачи. А мы с вами двигаемся дальше. Помимо непосредственной генерации данных с помощью Sklearn.datasets можно загружать так называемые toy datasets или «игрушечные» наборы данных. Это такие стандартные наборы данных, которые часто используются для тестирования ваших алгоритмов. Их можно загрузить с помощью перечисленных ниже функций, например функций load_iris, load_boston или load_digits. Наверное, наиболее популярным набором данных является dataset ирисы Фишера, поэтому мы с вами на него и посмотрим. Для того чтобы его загрузить, воспользуемся функцией load_iris, и теперь давайте посмотрим на объект, который мы получили. Видим, что это некоторый dict-like object, у которого есть ключи и некоторые значения по этим ключам. Вот давайте посмотрим на набор ключей, чтобы понять, что же у нас есть в этом объекте. Вот мы видим, что нам доступны названия признаков, доступны сами данные, сама целевая переменная, также есть некоторое описание данных и известны имена классов. Давайте начнем с того, что выведем описание. Ну вот видим, что описание довольно подробное. Из него мы можем очень многое про наш набор данных понять. В частности, мы знаем, сколько нам доступно объектов, какие есть признаки, что они означают. В данном случае мы понимаем, что признаки — это метрические характеристики цветков. Нам даже доступна некоторая статистика по нашим признакам. И есть ссылка на UCI репозиторий, в котором хранится исходный набор данных, даже есть информация о некоторых references, авторы и так далее. Довольно удобно, вы практически все про набор данных можете из этой справки понять. Теперь давайте посмотрим на имена признаков и имена классов. Вот мы видим, что они также доступны. Здесь из этого мы можем понять, как называются наши виды ирисов. Теперь давайте посмотрим на данные. Чтобы запись была покороче, выведем данные по первым 10 объектам. Ну вот видим, что это ровно те 4 признака, о которых было сказано в описании, и можно посмотреть на целевую переменную. Вот видим, что классы представлены метками от 0 до 2 и объекты отсортированы по классам. В общем-то, сейчас мы уже довольно хорошо понимаем, что из себя представляет эта выборка. Ну вот давайте на нее посмотрим и визуализируем наши данные. Для этого нам будет удобно работать с данными, как с таблицей. Вспомним немножечко библиотеку pandas и давайте построим новый DataFrame. Строить будем его на основе наших данных, далее добавим в качестве названий колонок имена признаков, потому что они нам также доступны, и добавим дополнительный столбец target. Он нам нужен для того, чтобы анализировать данные, зная информацию о классах. Итак, DataFrame готов, давайте на него посмотрим с помощью команды head, и вот мы видим, что мы получили ровно то, что мы хотели — признаковое описание каждого объекта и в конце метка класса. В общем-то, в таком виде нам уже удобно работать с нашим набором данных, но так как мы хотим сейчас просто визуализировать dataset, то тогда давайте рассмотрим колонку target и заменим в ней метки классов на их имена. Это можно сделать очень просто с помощью функции apply. Передадим в нее lambda функцию, которая берет метку класса и вместо нее подставляет название. Снова посмотрим на наш набор данных. И видим, что, да, всё получилось: вместо метки класса мы теперь видим там его имя. Что же, работать с pandas очень удобно, потому что мы с вами можем очень просто строить различные графики, например, строить гистограммы распределения признаков. Вот давайте посмотрим, как первый признак выглядит... как выглядит гистограмма первого признака для класса setosa. Делаем это с помощью команды hist и получаем нужное распределение. На самом деле, мы понимаем, что нас интересует не только это распределение. В общем-то, нам интересно посмотреть на распределение всех признаков в рамках всех классов. Но нам хочется понять, вообще говоря, отличаются они между собой или нет, сможем ли мы по этим признакам каким-то образом разделить наши объекты. Вот давайте это сделаем. Понятно, что мы можем делать это аналогичным образом, построив отдельно каждый график. Но, наверное, это не очень удобно. Представьте, когда много признаков, много классов, это довольно трудоемкое занятие. Давайте получим одну картинку, на которой мы будем иметь следующий набор данных: мы хотим построить гистограммы всех признаков по всем классам. Когда у нас не очень много классов и не очень много признаков, это довольно просто делается. Для этого мы с вами снова воспользуемся pyplot'ом и в цикле по признакам и по классам будем строить некоторые графики. Для этого нам понадобится метод subplot, который позволяет нам строить такую матрицу из графиков. В этот метод мы можем передать количество строк и количество столбцов, которое мы хотим, и также для каждого конкретного графика нужно будет сдать его номер. Вот, собственно, это мы с вами и делаем. Теперь мы будем пользоваться функцией hist, но уже не функцией, не методом dataframe'а, а просто методом pyplot. И также помимо самого графика, мы будем задавать его название, будем задавать ось x и ось y, как они называются. Вот давайте построим, это занимает какое-то время, потому что графиков много, мы с вами получаем целых 12 графиков. Вот давайте посмотрим, как они выглядят. Видим, что по столбцам у нас идут графики, касающиеся разных классов, и по строчкам мы видим признаки. Вот можно проанализировать все графики. Вот, например, давайте посмотрим на последний. Видим, что распределения, в общем-то, везде получились разные и видим, что даже вот по последнему признаку у нас в классе setosa максимальное значение порядка 0,6, а в следующем классе минимальное значение 1. То есть в общем-то видно, что даже в пространстве этих признаков мы задачу решить сможем, данные можно разделить. И в качестве бонуса хочется рассказать вам еще одну библиотеку, с помощью которой довольно просто визуализировать данные. Эта библиотека называется seaborn. Ну давайте для начала ее импортируем и попробуем с ее помощью построить следующий график. Давайте отобразим наши объекты в координатах пар признаков и посмотрим, хорошо ли они разделяются. Это можно сделать с помощью метода pairplot. Передаем этому методу наши dataframe и говорим, в каком столбике находится наша целевая метка. Строим данный график и видим, что мы получили очень интересное изображение: по диагонали мы видим значение... мы видим графики с распределением наших признаков, разными цветами отмечены разные классы. В общем-то, это графики, аналогичные тем, которые мы с вами строили самостоятельно, но совмещенные на одну картинку. Возможно, кому-то так даже больше понравится. Все остальные графики показывают, отображают наши объекты в пространстве пар признаков. Вот мы можем посмотреть, что существуют такие пары признаков, в которых объекты довольно неплохо разделяются. Вот, например, по этому признаку видно, что объекты можно разделить. Но с другой стороны, есть такие признаки, по которым объекты разделяются не так хорошо. Вот график получился довольно неплохой, но мне кажется, шрифт мелкий и читать не очень удобно. Вот давайте посмотрим, как мы можем на это повлиять. Мы видим, что можно с помощью метода set задать очень много параметров. Например, мы можем указать, какой шрифт мы хотим использовать, мы можем увеличить шрифт, можем задать цвета и так далее. Вот давайте немножечко увеличим шрифт и хочется показать вам еще одну вещь. Так как dataset "iris" довольно популярен, это стандартная вещь. Библиотека seaborn содержит функцию, которая позволяет этот dataset импортировать непосредственно, поэтому вам необязательно нужно иметь уже готовую dataframe к тому моменту, когда вы хотите рисовать этот график. Следственно, теперь вы знаете два способа: можно иметь свой dataframe, можно воспользоваться функцией load_datasets из библиотеки seaborn. Вот давайте теперь для разнообразия сделаем так, единственное отличие в том, что столбец target теперь называется по-другому. Вот и заодно давайте увеличим шрифт. [ПУСТО] Вот видим, что получили практически такой же график. Шрифт достаточно... шрифт стал немножечко больше. И также можем проанализировать наши данные. Если вам библиотека seaborn понравилась, то обязательно посмотрите ссылки ниже. Эта библиотека не входит в стандартный дистрибутив анаконда, поэтому ее придется поставить отдельно. Но это очень просто. По ссылкам доступны подробные инструкции на установку и также на установку с помощью анаконды. Попробуйте, это очень легко делается. Также доступно множество примеров и руководство по этой библиотеке. А мы на этом заканчиваем. На этом уроке мы научились генерировать модельные данные и загружать стандартные dataset'ы с помощью модуля sklearn datasets. На следующем уроке мы научимся строить разбиение данных с помощью кросс-валидации.

[БЕЗ_ЗВУКА] В этом видео мы научимся делать разбиение набора данных на подвыборки. С помощью модуля Sklearn.cross_validation мы научимся делать разовые разбиения данных на обучение и тест, а также рассмотрим несколько часто встречающихся стратегий кросс-валидации. По приведенной ниже ссылке доступна документация этого модуля. Мы начнем с того, что импортируем нужные нам модули — это модуль datasets, с которым мы познакомились на предыдущем уроке, и модуль cross_validation. Также нам понадобится библиотека numpy. Теперь давайте загрузим стандартный dataset, снова воспользуемся dataset-ом ирисы Фишера. Теперь предположим, что мы хотим решить задачу классификации. Для этого нам нужно разбить данные на обучение и тест, чтобы впоследствии обучить модель на обучающей выборке и оценить ее качество на тесте. Для этого cross_validation предоставляет нам очень удобную функцию под названием train_test_split. Она позволяет нам построить разовое разбиение данных на обучение и тест. В качестве аргумента функция принимает набор данных, которые мы хотим разбить, набор меток классов, и также ей можно указать соотношение, в котором мы хотим разбивать данные. В данном случае я хочу отправить 30 % объектов в тестовую выборку и все остальные — в обучающую. Давайте это сделаем и посмотрим, что же мы получим в результате. В результате мы получили 4 объекта, train_data и test_data — это части нашей выборки для обучения и для теста, непосредственное описание объектов, и train_labels и test_labels — это метки объектов из обучающей и тестовой выборки соответственно. Теперь давайте убедимся, что наша выборка действительно разбита в заданном соотношении. Мы просто оценим, какую долю тестовая выборка составляет от всех данных. Да, действительно, видим, что 0,3 — так, как мы указали. Теперь давайте выведем размер обучающей тестовой выборки в объектах. Для этого можно оценить размер train_data или test_data и также можно оценить размер train_labels или test_labels. Понятно, что размеры должны получиться одинаковые. Да, действительно, мы видим, что соотношение вот такое, как мы хотели: 105 объектов для обучения и 45 объектов для тестирования. Ну теперь давайте выведем часть нашей обучающей и тестовой выборки и посмотрим, как они выглядят. Ну действительно, это все то же самое, это выглядит ровно так же, как наши исходные данные, просто это некоторая подвыборка из них. Ну и мы можем вывести полностью список лейблов на обучении и на тесте и убедиться, что, действительно, это те же самые лейблы (метки от 0 до 2), и в обучении и в тесте присутствуют объекты всех классов. Довольно удобно. Эта функция позволяет строить разовое разбиение на обучение и тест. Часто это полезно, если мы просто хотим оценить качество нашей модели в одной точке. Если же мы хотим получить более строгую оценку, нам нужно пользоваться кросс-валидацией. Давайте начнем с простого. Первая стратегия кросс-валидации, которую мы рассмотрим, называется KFold. Напоминаю, что в случае KFold мы разбиваем нашу выборку на K групп, при этом каждая из групп 1 раз участвует в тестировании и (K − 1) раз участвует в обучении. Для того чтобы получить такую кросс-валидацию, нам нужно использовать функцию KFold. В качестве аргументов она принимает на вход количество объектов, которое мы хотим разбивать, и количество фолдов, которое нам нужно. В отличие от функции train_test_split, функция KFold не строит нам разбиение исходных данных. Она возвращает нам пару индексов: индексы из обучения и индексы из тестов, с помощью которых мы далее можем самостоятельно разбить нашу выборку. Вот давайте применим эту функцию в случае, когда мы хотим разбить 10 объектов на 5 фолдов, и посмотрим, как выглядят наши индексы. Вот мы видим наши разбиения. Каждый фолд получился размером 2 объекта, и поэтому обучающая выборка состоит каждый раз из 8 объектов, тестовая — из 2-х. Кстати, мы видим, что наши объекты разбиты по порядку. Если посмотреть на тестовую выборку, то мы видим, что каждый раз у нас индексы расположены в исходном порядке. Не всегда это удобно, в качестве примера можете вспомнить dataset ирисы Фишера, в котором наши объекты были исходно отсортированы по метке класса. В данном случае может получиться не очень хорошая ситуация, когда у нас в обучении или в тесте присутствуют представители не всех классов. Чтобы такого избежать, нам нужно указать параметр shuffle и сказать, что он должен быть равен True, то есть мы хотим сделать shuffle с нашей выборкой, хотим перемешать элементы. Вот давайте посмотрим, как будут выглядеть индексы. В данном случае мы построили разбиение на 2 фолда, чтобы было немножечко покороче, и видим, что в данном случае объекты уже расположены не по порядку. Но при этом если мы вызовем эту функцию еще раз или еще раз, мы получаем каждый раз разные разбиения. Не всегда это удобно. Если мы хотим, чтобы результат работы нашей функции был детерминированным, то нам нужно указать параметр random_state, зафиксировать какое именно случайное разбиение мы будем использовать. Вот давайте его укажем и посмотрим. Получили некоторое разбиение, повторные запуски этой функции приводят к таким же разбиениям. Часто это удобно, если мы хотим, чтобы результаты нашей работы были воспроизводимы. Следующая стратегия кросс-валидации, которую мы рассмотрим, называется StratifiedKFold. Она очень похожа на предыдущую, но есть существенное отличие. В данном случае мы сохраняем соотношение классов в обучающих и тестовых подвыборках. Для того чтобы запустить такую функцию, нам нужно передать ей не только количество объектов, но и непосредственно метки классов на объектах, так как разбиение происходит с учетом меток. Поэтому давайте создадим набор меток классов. В данном случае мы создадим список из 10 элементов, первые 5 элементов будут равны 0, последние 5 элементов будут равны 1. Таким образом мы получили задачу бинарной классификации. И передадим этот список меток в нашу функцию. Скажем, что мы хотим сделать разбиение на 2 фолда, также укажем параметр shuffle = True и random_state. Вот давайте посмотрим. Для начала выведем наши метки, а потом индексы. Вот видим, что метки получились такие, как мы хотели, и давайте убедимся, что у нас соотношение объектов в обучающих и тестовых выборках также 50 на 50, как и в исходных метках. Ну вот видим, что объекты с индексом 3 и 4 имеют метку 0, объекты с индексом 8 и 9 имеют метку 1. Соответственно, соотношение правильное. Для примера давайте создадим другой список меток, где метки уже будут идти через одну (0 1 0 1) и посмотрим, как изменятся наши индексы. Функцию запускаем практически с такими же параметрами. Вот видим, что, да, действительно, индексы другие, и видим, что вот наш индекс 1 и 5 — это объекты с меткой 1, индексы 2 и 8 — объекты с меткой 0, то есть соотношение опять правильное, все так, как мы хотели. Следующая интересная стратегия — это ShuffleSplit. Она позволяет строить так называемые случайные перестановки. Таким образом мы можем получить очень много выборок, при этом мы можем специфицировать размер обучающей выборки, и у нас нет никаких ограничений на то, сколько раз каждый объект должен появиться в обучении или в тесте. Каждый раз мы действуем с возвращением, то есть мы получаем одно разбиение и дальше можем строить другое независимо от предыдущего. В качестве аргументов функции нужно указать количество наших объектов, сколько итераций мы хотим и размер тестовой выборки. Вот давайте разобъем данные в соотношении: 80 % — обучение и 20 % — тест, и посмотрим, как будут выглядеть наши выборки. Ну вот мы построили 10 итераций. Невооруженным взглядом видно, что у нас нет никаких ограничений на то, сколько раз объект должен встретиться в обучении или в тесте. Вот, в частности, мы видим, что объект с меткой 0 сразу несколько раз попадает в тест, разбиение действительно случайное. Но shuffle_split также можно стратифицировать. Для этого нужно использовать функцию StratifiedShuffleSplit, и в этом случае мы тоже будем получать выборки, которые имеют исходное соотношение классов. Опять же, для этого нам придется передать target и целевую метку в функцию. И давайте посмотрим, как это выглядит. Сделаем поменьше итераций, сделаем 4 и убедимся, что все правильно. Вот, действительно, каждый раз мы видим, что в тесте есть один объект нулевого класса, один объект первого класса, таким образом, очевидно, что и в обучении у нас также получилась сбалансированная выборка. И последняя стратегия, на которую мы сегодня посмотрим, называется LeaveOneOut. Наверняка вы помните, что это стратегия, которая позволяет оставить каждый объект в тесте 1 раз. Таким образом, тестовая выборка всегда состоит из одного объекта, и каждый объект из нашего набора данных 1 раз присутствует в тесте. Это очень удобная стратегия кросс-валидации, которую хорошо использовать в случае, когда мы имеем небольшую выборку данных. Давайте посмотрим, как это выглядит. Все очень просто. Функции достаточно передать только количество объектов, и легко проверить, что мы получили правильное разбиение. Каждый объект 1 раз присутствует в тестовой выборке. Конечно же, это не единственные стратегии, которыми можно пользоваться. Все остальные вы можете посмотреть в документации по предоставленной ниже ссылке. А мы с вами на этом заканчиваем изучение модуля cross-validation. Мы научились строить разовые разбиения данных с помощью функции train_test_split, а также рассмотрели наиболее популярные стратегии кросс-валидации. В следующем уроке мы перейдем к построению моделей и рассмотрим модуль Linear Models. Мы будем строить линейные модели классификации.

[БЕЗ_ЗВУКА] На предыдущих уроках мы научились генерировать модельные данные, а также строить разбиения набора данных с помощью кросс-валидации. Теперь мы можем смело перейти к построению моделей, и в этом уроке мы научимся строить линейные модели с помощью модуля sklearn.linear.model. Модуль предоставляет нам целый ряд линейных моделей: это линейная классификация и регрессия, логистическая регрессия, модель на основе стохастического градиентного спуска и другое. По ссылкам ниже доступна документация и примеры построения линейных моделей. Мы с вами начнем с того, что импортируем необходимые нам модули. С модулями cross-validation и datasets вы знакомы с предыдущих уроков, модуль linear_model нужен нам для того, чтобы использовать линейные модели, и модуль metrics пригодиться для того, чтобы оценивать качество полученных моделей. Итак, начнем с генерации данных. В данном случае в качестве модельных данных я выбрала dataset, состоящий из 2-х признаков, его также удобно визуализировать, который представляется в виде 2-х капель, или 2-х облаков точек. Построить такой dataset можно с помощью функции make_blobs. Укажем ей параметры, говорящие о том, сколько у нас будет классов, сколько будет центров и какой будет разброс. Вот теперь давайте посмотрим на то, как же выглядит наш dataset. Но ответим, что действительно получили два облака точек, которые немножечко накладываются друг на друга. Итак, теперь с помощью уже знакомой нам функции train_test_split разобьем данные на обучение и тест. Будем это делать в соотношении: 30 % — тест и 70 % — обучение. Теперь давайте строить наш классификатор. Мы будем строить линейный классификатор под названием ridge_classifier, для начала не будем указывать никаких параметров, воспользуемся моделью по умолчанию. Единственное, что мы укажем — это random_state. Он нужен для того, чтобы наши эксперименты были воспроизводимы. Итак, мы сделали объект. Теперь давайте его обучим. Для того чтобы обучить модель, нужно вызвать метод fit и передать ему в качестве аргументов данные, на которых нужно обучаться, и метки классов. В нашем случае это train_data и train_labels, которые мы сгенерировали ранее. Итак, получили обученную модель. Теперь мы можем строить наши предсказания. Для этого вызываем метод predict и передаем ему на вход тестовую выборку. Это те данные, которые в обучении не участвовали, и мы можем с помощью них проверить, насколько хорошо наша модель работает на внешних данных. Итак, получили наши предсказания, теперь давайте выведем на экран правильные метки и предсказанные метки, чтобы их сравнить и посмотреть, насколько же хорошо мы отработали. Итак, тестовые метки перед вами, метки на тестовой выборке. Теперь выводим предсказания и видим, что в целом кажется, что неплохо. Они похожи, но есть отличия. Для того чтобы формально оценить качество нашей модели, нужно воспользоваться некоторой метрикой качества. Ну давайте воспользуемся довольно простой метрикой под названием accuracy — это доля правильных ответов на тестовой выборке. Ну в данном случае мы передаем в качестве аргументов метки на тестовой выборке и наши предсказания. Итак, получаем 0,87. Получается, что в 87 % случаев мы правильно предсказали метку нашего класса. Это хорошо. А помимо качества классификации, мы можем смотреть также и на веса перед нашими признаками, которые мы получили в процессе обучения. Ну давайте их выведем. Перед нами вес перед первым и перед вторым признаком. Ну и помимо весов перед признаками мы можем вывести коэффициент перед свободным членом — свободный коэффициент. Он также перед вами. Следующая модель, которую мы обучим — логистическая регрессия. Для этого нужно воспользоваться классом LogisticRegression. Давайте создадим соответствующий объект, снова будем использовать параметры по умолчанию, и также обучим модель и построим предсказания на той же самой тестовой выборке. Обучаем модель с помощью метода fit, снова передаем ей для обучения данные и метки классов, строим предсказания, а теперь заметим, что в случае, когда мы используем логистическую регрессию, мы работаем с вероятностной моделью. Помимо меток классов, эта модель может выдать нам вероятности, с которыми каждый объект принадлежит первому и нулевому классу. Вот давайте эти вероятности тоже построим. Для этого нужно воспользоваться методом predict_proba (или predict probability). Снова в качестве аргумента передаем тестовую выборку и получаем наши вероятности принадлежности к классам. Давайте посмотрим, как это выглядит. Тестовые метки у нас не изменились. Обычные предсказания выглядят в виде меток классов. И теперь вероятностное предсказание. Видим, что для каждого объекта нам доступны 2 значения: первое — это вероятность принадлежности к классу с меткой 0, второе значение — вероятность принадлежности к классу с меткой 1. Ну и логично, что если каждую пару мы сложим, мы должны получить 1. Итак, теперь давайте оценим качество нашей модели. Снова сделаем это с помощью метода accuracy_score, посмотрим на аккуратность нашей классификации, или на долю правильных ответов. Видим, что она составляет 0,8, и напомню, что когда мы обучали ridge-классификатор, у нас доля правильных ответов была 87 %. Итак, мы с вами уже умеем обучать модели, оценивать их качество по holdout dataset-у и смотреть на полученные веса. Теперь давайте ответим на вопрос: является ли полученная оценка устойчивой? Достаточно ли такой оценки для того, чтобы сделать вывод о том, что один алгоритм работает лучше, чем другой? Давайте обучим сразу несколько моделей. Сделать это можно с помощью кросс-валидации. Проанализируем полученные результаты и попытаемся сравнить с нашей моделью. Для того чтобы обучить несколько моделей и посмотреть на полученные оценки, мы можем действовать разными способами. Например, мы можем взять функцию train_test_split, которую мы изучили ранее, построить несколько разбиений, обучить на них модели и оценить качество. Таким образом мы получим целый набор оценок, который в дальнейшем можно будет усреднить, посчитать отклонения и получить некоторую интервальную оценку на качество наших моделей. Понятно, что это не очень удобно. Нам хотелось бы иметь некоторый автоматизированный способ, для того чтобы это делать. Для этого cross-validation предоставляет нам функцию cross-val-score. Давайте посмотрим, как она выглядит. Эта функция принимает несколько аргументов. Во-первых, она принимает модель, которую мы хотим оценивать. Далее ей нужно передать данные и метки классов, а также указать, какая метрика нас интересует и каким образом мы хотим делать кросс-валидацию. Для начала давайте попробуем довольно простой запуск. Передадим классификаторы, которые мы ранее создали. Воспользуемся данными, которые нам также доступны, и укажем, что в качестве метрики мы будем использовать accuracy — долю правильных ответов, в качестве кросс-валидации будем использовать k-fold на 10 fold-ов. Если мы указываем параметр cv = 10, в данном случае мы никак не специфицируем вид кросс-валидации, работает кросс-валидация по умолчанию: это либо k-fold, либо stratified k-fold. Когда мы работаем над задачей бинарной классификации, по умолчанию работает stratified k-fold. В данном случае нас это вполне устраивает. Итак, давайте применим функцию. Сначала получили оценку для классификатора ridge-classifier, теперь давайте сделаем то же самое для логистической регрессии. И теперь по полученному скорингу, давайте также выведем его на экран, чтобы посмотреть, как это выглядит. На самом деле, это просто список оценок метрики accuracy, то же самое в случае линейной регрессии. Теперь давайте посмотрим на статистики. Ну вот выведем среднее, максимальное и минимальное значение и посчитаем разброс. Сначала для ridge-классификатора, теперь для log-регрессии. Вот на основании полученного результата мы видим, что на самом деле модель работает приблизительно одинаково. Теперь давайте немножечко усложним. Предположим, что мы хотим считать какую-то нестандартную метрику качества и хотим специфицировать нужную нам стратегию кросс-валидации. Для того чтобы передать свою метрику качества в эту функцию, нам нужно создать так называемый объект scorer. Он должен удовлетворять нужному интерфейсу. Он должен принимать 3 параметра. Первый — это модель, которую мы оцениваем. Далее — набор меток классов и набор наших предсказаний. Если мы посмотрим на синтаксис функции accuracy_score, то мы поймем, что он не такой. Эта метрика... эта функция не принимает на вход классификатор, она принимает только метки классов и наши предсказания. Для того чтобы создать правильный объект, который можно передать внутрь функции cross_val_score, нам с вами нужно его создать. Это можно делать с помощью функции make_scorer. Передаем ей в качестве аргумента нашу метрику и создаем scorer. Ну в данном случае мы передали ту же самую метрику, потому что она нам просто подходит. Теперь давайте специфицируем стратегию кросс-валидации. Ну для примера давайте будем использовать StratifiedShuffleSplit. Будем делать целых 20 итераций, и пусть доля тестовой выборки будет 30 %. Итак, scorer и стратегию мы создали, теперь давайте предадим их внутрь функции cross_val_score и снова оценим наши модели. Итак, сначала scoring для ridge-классификатора, теперь scoring для линейной, для логистической регрессии. Итак, выводим оценки и видим, что снова мы получили очень похожий результат. Понятно, что наша модель работает приблизительно одинаково. На этом мы заканчиваем. Мы научились строить линейные модели ridge_classifier и log_regression, а также научились оценивать их качество в отдельной точке и с помощью кросс-валидации. Для этого мы использовали функцию cross_val_score. На следующем уроке мы продолжим работать с модулем linear_model и научимся строить еще несколько моделей.

[БЕЗ_ЗВУКА] В этом видео мы продолжим изучение модуля sklearn.linear_model. Теперь мы будем строить модели регрессии. Для начала импортируем необходимые нам модули. Со всеми ними вы уже знакомы из прошлых видео. Теперь сгенерируем данные для обучения. Так как мы решаем задачу регрессии, нам очень подойдет функция make_regression. Она позволяет построить соответствующий dataset. В данном случае мы будем строить dataset с двумя признаками. Пускай один из них будет информативным, другой будет избыточным. Также добавим некоторый шум. Еще один параметр, который мы укажем, это параметр coef = True. Он нужен для того, чтобы мы могли с вами посмотреть на уравнение функции, которую мы приближаем. То есть мы попросим метод вернуть нам не только данные и метки, но и также уравнение. Теперь давайте попробуем отрисовать наш dataset. В данном случае мы не сможем действовать полностью аналогично тому, как мы делали в задаче классификации, потому что у нас в данном случае нет меток классов и мы просто не сможем отрисовать их разным цветом. Мы приближаем некоторую функцию. Тогда мы можем поступить другим образом — мы можем построить некоторую зависимость между нашими признаками и целевой меткой. Например, мы можем отобразить объекты в плоскости «признаки–метки», таким образом мы сможем понять, есть ли какая-то зависимость между значением признаков и значением целевой метки. Вот давайте попробуем так сделать. Сначала мы построим объекты в плоскости «нулевой признак–целевая метка», отрисуем их красным цветом, а потом мы построим те же самые объекты в плоскости «первый признак–метка» и отрисуем их синим цветом. Так как пространство значения признаков у нас совпадает, мы можем сделать это на одной плоскости, на одном графике. Вот давайте посмотрим. Да, получили вот такой график. По этому графику легко проанализировать и понять, какой же из двух признаков является информативным. Вот давайте посмотрим. Нулевой признак отображен точками красного цвета. Мы видим, что в основном с увеличением значения признака у нас растет target. Таким образом, понятно, что зависимость между этим признаком и целевой переменной есть. С другой стороны, давайте посмотрим на синие точки (это второй признак). Мы видим, что, в общем-то, изменение этого признака, например его рост, не всегда означает рост целевой функции. У нас в этом случае есть некоторые случайные изменения, поэтому понятно, что этот признак не настолько информативен. Вот теперь давайте построим модель и посмотрим на коэффициенты между этими признаками. Понятно, что мы ожидаем больший коэффициент (я имею в виду абсолютную величину) перед информативным признаком и меньший коэффициент по абсолютной величине перед избыточным признаком. Вот давайте это проанализируем. Для того чтобы построить модель и оценить ее качество, разобьем данные на обучение и тест, и теперь давайте строить модель. Воспользуемся классификатором linear_regression, обучим его на train_data и train_labels и получим наши predictions с помощью метода predict. Вот теперь давайте выведем значения функции исходная. Вот они перед вами. И теперь давайте выведем то, как мы эту функцию приблизили на тестовых объектах. Но так как довольно сложно оценить, насколько у нас хорошо получилось, вот давайте для этого введем некоторую метрику. Будем использовать среднее отклонение нашего приближения от исходного значения функции. Посчитаем. Видим, что в среднем мы ошибаемся на 4,2. Давайте сделаем оценку чуть более точной и для этого воспользуемся уже знакомой нам функцией cross_val_score. Передадим в наш регрессор данные, целевую переменную. Скажем, что будем пользоваться метрикой mean_absolute_error (та же самая метрика, которой мы только что воспользовались) и будем делать кросс-валидацию k-fold на 10 фолдов. Сразу же после этого выпишем среднее и отклонение по нашей метрике. Мы видим, что в среднем наша ошибка равна 4 с отклонением 1. Единственное, что нас здесь должно смутить, это знак «минус» перед средним. Ну, действительно, мы оцениваем метрику «среднее абсолютное отклонение», поэтому она должна быть неотрицательной. А здесь мы видим минус, потому что функция cross_val_score часто используется для подбора параметров алгоритма. В данном случае в качества scoring мы передаем метрику, которая не растет, когда модель становится лучше, а которая уменьшается, когда модель становится лучше. А так как при подборе параметров часто используется максимизация нашей метрики, то нам просто удобно иногда умножить нашу метрику на (−1) и дальше ее точно так же максимизировать. А так как функция cross_val_score понимает, что функция mean_absolute_error растет, когда модель ухудшается, то нам действительно просто умножить эту функцию на (−1). В данном случае мы с вами подбор параметров не делаем, мы ничего не оптимизируем, поэтому, конечно, нам это неудобно. Чтобы от такого избавиться, мы можем создать свой собственный scorer. Давайте это и сделаем, создадим scorer с помощью функции make_scorer и скажем, что для нашей метрики greater_is_better, то есть значение, когда наша метрика растет, это лучше. Естественно, это не так, но просто с помощью такого параметра мы с вами избавимся от искусственного умножения на (−1). Вот давайте это введем и повторим процесс. Ну вот видим, что теперь мы получили неотрицательное значение метрики. Теперь давайте посмотрим на коэффициенты нашей исходной функции. Помните, мы их получили в самом начале, когда генерировали данные. Выведем эти коэффициенты. Ну и видим, что коэффициент перед первым признаком равен 38 (это информативный признак) и перед вторым признаком признаком равен 0 (это избыточный признак). Теперь давайте посмотрим, какие же коэффициенты подобрала наша модель. Ну да, видим, что ответ довольно близок к исходному. Давайте запишем это в виде уравнения, чтобы было более наглядно. Ну вот уравнение нашей исходной функции (y — это, собственно, и есть target) и уравнение той функции, которую мы с вами обучили, которую мы с вами построили. Ну мы видим, что мы достаточно близки. Теперь давайте попробуем решить ту же самую задачу с помощью другого метода регрессии. Мы рассмотрим лассо-регрессию — регрессию с использованием регуляризации lasso или регуляризации L1. Давайте также построим модель, обучим ее на обучающих данных и и построим наше приближение. Делаем это с помощью функций fit и predict так же, как и раньше. Сразу же оценим качество модели по кросс-валидации. Будем использовать тот же самый scorer, чтобы получить неотрицательное значение метрики. Видим, что, судя по всему, качество стало немножечко хуже. Ну, понятно, что это не значимо, но тем не менее средняя ошибка немножечко больше. Теперь давайте выведем снова исходное уравнение и выведем уравнение, которое мы получили с помощью лассо-регрессии. Посмотрите. Отличие в том, что в данном случае перед неинформативным признаком мы получили 0. Здесь мы видим результат применения L1-регуляризации. На этом модельном примере очень хорошо видны особенности работы лассо-регуляризации. Мы видим, что у нас как будто бы произошел отбор признаков. Перед избыточным признаком мы получили вес 0. Таким образом, теперь вы знаете, что если в вашей задаче много избыточных признаков и вы хотите заняться отбором признаков, то есть вы хотите получить перед ними не просто маленькие веса, а действительно получить нули и отфильтровать такие признаки, то лассо-регуляризация очень хорошо подходит для этой задачи. А мы с вами на этом заканчиваем. На этом видеоуроке мы научились строить линейную регрессию и лассо-регрессию, а в следующем уроке мы поговорим про метрики качества для оценки задач классификации и регрессии.

[ЗАСТАВКА] На этом уроке мы научимся оценивать качество моделей классификации и регрессии с помощью метрик из модуля sklearn.metrics. Этот модуль предоставляет нам готовую реализацию большинства метрик, использующихся в задачах классификации и регрессии. По приведенной ссылке вы можете ознакомиться с полным набором доступных метрик. Я же продемонстрирую наиболее интересные из них. Для начала мы импортируем все необходимые модули. А теперь давайте сгенерируем данные. Так как мы будем решать сразу две задачи — классификация и регрессия, нам потребуются два набора данных. Для этого воспользуемся знакомыми нам функциями: make_classification и make_regression. Так, данные готовы. В обоих случаях мы сгенерировали датасеты, состоящие из двух признаков. Нам все еще хочется их визуализировать, поэтому нам это удобно. Однако когда мы решаем задачу классификации, мы будем строить датасет, в котором оба признака являются информативными, а для задачи регрессии мы будем использовать датасет, в котором информативным является только один признак. Итак, давайте отрисуем наши наборы данных. Начнем с данных для классификации. Вот мы видим: красным и синим цветом обозначены объекты двух разных классов. Довольно интересно они расположены. И теперь давайте отрисуем данные для задачи регрессии. В данном случае напоминаю, что мы строим объекты в координатах признак– целевая функция, то есть красным цветом у нас обозначены объекты, построенные в координатах — давайте посмотрим — первый признак–target, синим цветом объекты построены... те же самые объекты построены в координатах нулевой признак–target. Мы вот видим, что первый признак является информативным, нулевой — шумовой. Итак, теперь данные мы сгенерировали, нам остается разбить их на обучение и тест. Сделаем это с помощью функции train_test_split. И теперь можно обучать модели. Давайте в качестве модели классификаций будем использовать SGDClassifier — это линейная классификация, работающая на основе стохастического градиентого спуска. В качестве функции потерь будем использовать log loss — нам это нужно для того, чтобы наш классификатор получился вероятностным. Для некоторых метрик мы захотим использовать вероятности принадлежности объектов к нашим классам, поэтому нам нужен вероятностный классификатор. Итак, теперь обучим классификатор — это делается с помощью метода fit. И сначала сгенерируем предсказания в виде меток классов, для этого будем использовать метод predict. И сгенерируем предсказание в виде вероятности принадлежности объекта к нулевому и первому классу. Это делается с помощью метода predict_proba, и также на вход передаем наши тестовые данные. И теперь давайте выведем все это на экран, посмотрим, что у нас получилось. Для начала — правильные метки, теперь — предсказания в виде меток. Сразу видно, что мы немножечко ошибаемся в нескольких местах. И теперь — предсказания в виде вероятности принадлежности к обоим классам. Помните, что первое значение — это вероятность принадлежать к нулевому классу, второе значение — вероятность принадлежать к первому классу. Итак, вся подготовительная работа завершена, теперь можем непосредственно заняться расчетом метрик. Первая метрика, на которую мы посмотрим — это accuracy, мы с ней уже знакомы. Эта метрика соответствует доле правильно классифицированных объектов. Понятно, что метрика довольно простая, поэтому ее реализацию мы можем написать самостоятельно — достаточно просто сравнить метку... правильную метку и метку, которую мы предсказываем. Дальше посчитать, сколько, в скольких случаях мы предсказываем метку правильно, и поделить это на объем данных. Мы получаем вот такую оценку — 0,83. Ну а теперь давайте воспользуемся готовой реализацией метрики accuracy — функция называется accuracy_score. В качестве аргументов передаем ей предсказанные нашим классификатором метки и правильные метки. И теперь давайте смотреть. Да, мы видим, что наши оценки совпали. Следующий объект, про который хочется рассказать — это confusion matrix. Это матрица, размером количество классов на количество классов. В позиции i и j у нас стоит элемент, который характеризует количество объектов, которые изначально относились... имели метку i, но мы им поставили метку j. Таким образом, на диагонали у нас — элементы, характеризующие объекты, на которых мы ответили правильно, вне диагонали у нас — элементы, характеризующие количество объектов, на которых мы ошиблись. Давайте построим такую матрицу и выведем ее на экран. Она строится с помощью метода confusion_matrix. Также на вход мы передаем правильные метки классов и предсказанные метки. Вот мы с вами видим, что довольно неплохо мы отработали. На диагонали видим значения 12 и 13, то есть, ну, мы ошибаемся не так часто. Теперь давайте проверим, так ли это. Мы можем снова просуммировать объекты, на которых наша метка совпадает с предсказанной — это делается довольно просто. Получили 25 объектов. Теперь если мы сложим диагональные объекты на нашей матрице, мы тоже получили 25. Таким образом, понятно, что если сложить диагональные элементы, мы найдем количество объектов, на которых мы ответили правильно. confusion_matrix очень полезна тем, что на ее основе можно рассчитывать такие метрики, как точность, полнота и f-меры. Давайте и мы с вами это сделаем. Для начала давайте оценим точность классификации. Так как мы решаем задачу бинарной классификации, мы можем отдельно оценить точность при отнесении объектов к нулевому классу и точность при отнесении объектов к первому классу. Для того чтобы оценить точность классификации к нулевому классу, нам нужно вызвать функцию precision_score, передать ей правильные метки классов, передать предсказанные метки классов. И так как по умолчанию наш label равен 1, то нам нужно явно сказать, что в данном случае мы оцениваем точность классификации к нулевому классу. Для этого используем аргумент pos_label и говорим, что он равен 0. Итак, видим, что мы получили 0,8. Теперь давайте оценим точность классификации объектов к первому классу. Мы получили 0,87. Ну, по нашей матрице понятно, почему точность при классификации к первому классу получилась больше. Теперь давайте оценим полноту. Это делается с помощью метода recall_score, а аргументы такие же. Сначала оцениваем полноту для нулевого класса, получаем 0,86. И теперь давайте посмотрим на первый класс — 0,81. Имея оценки для precision и recall (для точности и полноты), легко получить оценки для f-меры. Давайте сначала оценим f-меру для нулевого класса, и теперь — для первого. Наши оценки готовы. Часто, когда мы решаем задачу классификации, нам интересно смотреть на все эти метрики: на precision, recall, f-меру. Причем нам интересны как значения в разрезе классов, так и значения в среднем. Для того чтобы получить такую мини-сводку по нашей модели, удобно использовать функцию под названием classification_report. Давайте ее вызовем. Для ее работы достаточно передать ей метки класса и предсказанные метки класса. Давайте посмотрим. Получили такую красивую табличку. Здесь мы видим качество модели в разрезе классов, видим precision, recall, f-score. Также видим, сколько изначально объектов было разных классов, и видим некоторые усреднения внизу — довольно удобно. А мы двигаемся дальше. Следующая метрика, про которую мы говорим — это ROC-curve и ROC — AUC. Из предыдущих уроков вы наверняка помните, что для построения ROC-curve мы сначала сортируем наши объекты по предсказанной величине, потом делим их на несколько групп по некоторым пороговым отсечениям и внутри каждой группы оцениваем True Positive Rate и False Positive Rate. Далее, когда мы таким образом получаем списки True Positive Rate и False Positive Rate, мы можем построить нашу ROC-кривую. Просто строим кривую в этих координатах. Итак, для того чтобы получить списки False Positive Rate, True Positive Rate и thresholds, нам нужно вызвать функцию roc_curve. Она принимает на вход правильные метки классов и наши ответы на этих классах. Нам интересней в данном случае воспользоваться вероятностными ответами, потому что здесь у нас задан более точный порядок. Поэтому давайте передадим в качестве второго аргумента probability_predictions — наши вероятностные предсказания. Мы с вами будем использовать вероятности отнесения объектов к первому классу, поэтому вот мы говорим, что берем все объекты, и так как, вы помните, там список из двух элементов, берем элемент с индексом 1 — это вероятность принадлежать к первому классу. Запускаем функцию, получили три аргумента. False Positive Rates и True Positive Rates нам нужны непосредственно для построения ROC-кривой, ну и понятно, что трешхолды нам для построения ROC-кривой не нужны, поэтому давайте будем здесь использовать нижнее подчеркивание, явно говоря о том, что это возвращаемое значение мы использовать не собираемся. Теперь у нас есть все данные для построения ROC-кривой. Построим ее с помощью метода plot. Передаем в качестве x-координат False Positive Rates, в качестве y-координат — True Positive Rate, и назовем нашу кривую linear model, линейная модель. Теперь давайте для сравнения построим ROC-кривую, соответствующую случайной классификации, — это просто диагональная прямая, назовем ее random. Создадим границы для оси x и оси y с помощью xlim и ylim. Далее, давайте зададим названия для наших осей и зададим имя графика. Также мы отрисуем легенду — подпись к каждой кривой, она будет находиться справа внизу. Строим график. Ну вот мы видим, что наша ROC-кривая выглядит таким образом. Теперь, для того чтобы количественно оценить качество модели, нам с вами нужно посчитать площадь под ROC-кривой. Такая метрика называется ROC — AUC, и строится она с помощью метода roc_auc_score. Понятно, что в данном случае мы снова можем передать в качестве аргументов как предсказания в виде меток, так и предсказания в виде вероятностей. Хочется понять, получатся ли они одинаковые или разные, но интуитивно кажется, что они должны быть близки — действительно, это же классификация с помощью одной и той же модели. Но понятно, что когда мы с вами имеем дело с метками, то в этом случае порядок следования объектов несколько более свободный, ну то есть те объекты, которые имеют одинаковую метку, могут идти в произвольном порядке. Случай, когда мы работаем с вероятностной классификацией, наш порядок задан более строго. Вот давайте сравним AUC в обоих случаях. Запускаем и видим, что, действительно, он у нас значительно отличается. Ну такое бывает из-за того, что в первом случае у нас действительно объекты могут в рамках группы идти в произвольном порядке. Это нормально. Еще одна метрика, на которую хочется посмотреть — это average_precision_score или presicion AUC. Для того чтобы ее посчитать, нужно вызвать функцию average_precision_score, и также передаем ей правильные метки и наши предсказания. И для оценки вероятностных классификаторов часто используется метрика log_loss, или логистические потери. В данном случае мы уже не можем передать ей метки класса, мы должны передать вероятности принадлежности к первому классу. Давайте запустим эту метрику и посмотрим. Видим, что log_loss довольно небольшой — 1,3 — очень неплохая оценка. Ну, вы помните, что log_loss — чем меньше эта метрика, тем лучше. В идеальном случае мы должны получить 0. Теперь давайте перейдем к решению задачи регрессии. Для этого нам нужно построить наш регрессор. Воспользуемся снова SGD-регрессором — это также метод регрессии, основанный на стохастическом градиентном спуске. Теперь давайте обучим модель и сгенерируем предсказания. Вот правильные значения нашей функции, вот приближенные значения. Мы видим, что да, действительно, они не везде совпадают. Теперь — как их можно оценивать. Первая метрика — это mean absolute error, средняя ошибка предсказаний, мы ей уже пользовались ранее, когда строили модель регрессии. Давайте ее посчитаем. Видим, что в среднем мы ошибаемся на 3,7. В качестве аргументов метрики передаем предсказанные значения, истинные значения, и считаем. Помимо абсолютных отклонений можно посчитать так называемую MSE-метрику, или mean squared error — это среднеквадратичное отклонение. Ну давайте запустим. Видим, что среднеквадратичное отклонение равно 24. Часто вместо среднеквадратичного отклонения используют корень из среднеквадратичного отклонения. Ну, его получить очень просто — достаточно посчитать среднеквадратичное отклонение с помощью той же самой функции и просто взять у него корень. Ну вот давайте посмотрим, как это выглядит. Получаем оценку 4,9. Последняя метрика, про которую хотелось бы сказать — это коэффициент детерминации, или r2 score. Для того чтобы ее посчитать, нужно вызвать функцию r2_score и передать ей на вход наши предсказания и правильные значения функции. Давайте запустим. И видим, что значение довольно высокое — 0,99. Итак, давайте подведем итог. Мы с вами рассмотрели целый ряд метрик для оценки моделей классификации и регрессии. На этом мы заканчиваем этот урок и заканчиваем знакомство с библиотекой sklearn. За модуль мы научились генерировать модельные данные, научились строить разбиение данных с помощью кросс-валидации. Также мы научились обучать линейные модели и оценивать их качество. Конечно, в библиотеке sklearn есть еще много интересных функциональностей, и мы обязательно вернемся к ее изучению чуть позже. А в следующем модуле вы продолжите изучать линейные модели.
