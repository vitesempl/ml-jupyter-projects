[ЗАСТАВКА] В этом модуле мы продолжим изучать возможности библиотеки sklearn. Напомню, что ранее мы уже успели обучить несколько линейных моделей, но практически всегда мы использовали параметры, заданные по умолчанию. Настало время это исправить и ответить на вопрос: как же параметры модели влияют на ее качество? И каким образом можно подбирать параметры, оптимальные для решаемой задачи? В этом видео мы будем рассматривать модуль grid_search в библиотеке sklearn и научимся подбирать параметры модели по сетке. Для начала давайте импортируем необходимые модули. И снова здесь только модуль grid_search, который мы будем рассматривать. И теперь сгенерируем данные. Воспользуемся датасетом «ирисы Фишера» и разобьем его на обучение и тест с помощью функции train_test_split. Теперь давайте выберем модель — пусть это будет SGD-классификатор — и создадим объект с параметрами по умолчанию. Теперь можно подбирать параметры. Для начала давайте посмотрим, какие же степени свободы у нас есть, какие параметры, вообще говоря, мы можем подобрать. Для этого воспользуемся методом get_params, который вернет нам словарь, и посмотрим, какие ключи у нас есть. Так, ну мы видим, что доступен целый ряд параметров, который можно подбирать. Ну, давайте будем подбирать только несколько. С одной стороны, можно выбрать вид функции потерь, будем рассматривать hinge, log, squared_hinge и squared_loss. Также давайте выберем вид регуляризации — выберем между l1 и l2. Также можем подобрать количество итераций — давайте подбирать от 5 до 10, по умолчанию у нас 5 итераций. И выберем коэффициент alpha — это множитель перед регуляризацией. По умолчанию у нас доступно значение 0,0001. Ну, вот давайте создадим отрезок от 0,0001 до 0,001, бросим на него равномерно пять точек и будем использовать их в качестве весов. Это можно сделать с помощью метода linspace. Итак, создадим наш словарь. В данном случае в качестве сетки мы будем использовать словарь, у которого ключ — это название параметра, который мы подбираем. Видите, что эти названия совпадают с названиями из get_params. А в качестве значения идет набор значений, которые мы хотим проверить. Таким образом, если мы построим декартово произведение на этих параметрах, то мы получим точки со всеми возможными наборами параметров. Собственно, это нам и хочется получить. Мы хотим в каждой из этих точек измерить качество классификации, далее сравнить, посмотреть, где качество максимальное, и сказать, что вот это есть оптимальные параметры. Это такой метод полного перебора на множестве параметров, и, в общем-то, он и называется поиском по сетке. Итак, давайте создадим стратегию кросс-валидации, с помощью которой мы будем оценивать качество. В данном случае я использую ShuffleSplit, буду делать 10 итераций, и в тестовую выборку будет идти 20 % данных. И теперь можно перейти непосредственно как бы к подбору параметров. Для начала давайте создадим объект grid_search, с помощью которого мы будем это делать. Данному объекту нужно обязательно передать модель, которую мы хотим оптимизировать — в данном случае наш SGD-классификатор. Также нужно передать сетку с параметрами, по которой мы будем бегать. Нужно указать метрику, которую мы будем проверять, и стратегию кросс-валидации. Таким образом, мы будем искать модель, наилучшую с точки зрения метрики accuracy, оцененную с помощью нашей стратегии кросс-валидации. Итак, наша сетка-объект готов, теперь давайте обучим сетку. Делается это с помощью метода fit. В процессе обучения у нас будет оценено качество в каждой из точек, и мы с вами потом сможем найти оптимальную. Вот давайте запустим. Работает это небыстро, потому что, смотрите, мы с вами перебираем четыре варианта функции потерь, две регуляризации, пять значений для количества итераций и пять значений для коэффициента alpha. Если все это перемножить... так, сколько мы получим? Получим 200 комбинаций — ну, довольно много. А еще мы делаем кросс-валидацию. Так, ну вот видим, что в несколько секунд мы уложились. Теперь давайте анализировать результаты. Ну, прежде всего, нас интересует самый лучший классификатор — его можно найти с помощью команды best_estimator. Видим, что здесь нам возвращается модель с лучшими параметрами. Отдельно можем попросить best_score, или оценку на лучшем наборе параметров, и, собственно, вывести лучшие наборы параметров в виде вот просто словаря параметров. Давайте это сделаем. Используются методы best_score и best_params. Так, смотрим, что лучшая оценка — это 0,895 для accuracy. И какие у нас наборы параметров получились? Регуляризация — l1, а коэффициент alpha — 9 итераций, и функция потерь — hinge. Так, ну, часто нам хочется посмотреть не только на лучший набор параметров, но и увидеть оценки на всех возможных наборах. Почему это полезно? Ну мы хотим проанализировать, каким образом каждый из параметров или комбинация параметров влияет на качество — собственно, для этого можно посмотреть на все остальные значения. И также можно узнать, насколько наша лучшая оценка лучше, чем, скажем, следующая. Вот получить весь набор доступных данных можно с помощью метода grid_scores. Ну, давайте мы весь выводить не будем, для примера выведем первые 10 значений. Ну, вот мы видим, что нам доступна средняя оценка качества по кросс-валидации, доступно отклонение и, конечно же, наборы параметров, на которых эта оценка была достигнута. Теперь давайте ответим на вопрос: всегда ли для нас оптимально использование такой стратегии поиска по сетке? Ну что это значит? В данном случае мы с вами работали с небольшим датасетом, обучение модели занимало не очень много времени, поэтому мы уложились в несколько секунд. Представьте себе обратную ситуацию. Если бы мы работали с большим набором данных, где было бы много признаков и много объектов, каждая модель обучалась бы значительное количество времени и мы бы хотели действительно подобрать много параметров. Тогда этот процесс занял бы существенное время, и с точки зрения вычислительной эффективности, возможно, нам было бы не так выгодно использовать полный перебор по сетке. Какие есть альтернативы? Можно использовать случайный поиск по сетке. Почему это, в принципе, может быть выгодно? Вот представьте, что мы с вами имеем большую сетку параметров. Что, если мы возьмем и оценим качество в каких-то случайных точках по этой сетке? Тогда мы сможем проанализировать полученный результат, посмотреть, как некоторые сочетания, некоторые наборы параметров влияют на качество, и на основе этого каким-то образом сузить область поиска. Например, мы можем откинуть заведомо неоптимальные параметры — параметры, которые... на которых всегда достигается худшее качество модели. После того как мы это сделаем, мы можем сузить область поиска и снова провести некоторую оптимизацию — например, опять запустить поиск по сетке или воспользоваться другим алгоритмом. Таким образом, эта стратегия кросс-валидации, вернее, стратегия подбора параметров также является очень полезной. Вот давайте посмотрим, как это можно делать. Для этого нам нужно создать другой объект, так называемый RandomizedSearchCV, которому мы также передаем те же самые параметры — обязательно нужно сказать, какую модель мы хотим оптимизировать. Передаем сетку, говорим, какая метрика нас интересует, указываем стратегию кросс-валидации, и дальше основное отличие в том, что мы говорим, сколько итераций мы хотим сделать. Фактически это означает, сколько точек из нашей полной сетки мы проверим. Вот давайте для примера проверим в 10 раз меньше точек — всего лишь 20 — и посмотрим, насколько сильно от этого ухудшится наше качество. Понятно, что это некоторый случайный результат, потому что в зависимости от того, каким образом мы инициализировали сетку, мы проверим разные точки, но в целом можно сделать некоторое приближение и понять вообще, насколько мы ухудшим качество за счет того, что сузим область поиска. Вот давайте это сделаем. Объект создан, теперь снова вызываем метод fit и заодно замерим, как долго он будет работать. Ну, вот видим, что работает он меньше секунды, очень быстро. Теперь давайте также посмотрим на оценку качества лучшей точки и на лучшие параметры. Мы видим, что наша оценка изменилась в третьем знаке, не сильно ухудшилась. И теперь давайте проанализируем параметры. Видим, что изменился только коэффициент alpha, вид функции потерь, вид регуляризации и количество итераций осталось прежним. Теперь вы можете либо продолжить процесс оптимизации из этой точки, либо остановиться на найденном наборе параметров. А мы с вами на этом заканчиваем. На этом видео мы научились подбирать параметры моделей с помощью поиска по сетке и случайного поиска по сетке. Это означает, что теперь вы умеете не только генерировать модельные данные, строить модели и оценивать их качество, но также оптимизировать модели. Это означает, что теперь мы можем собрать вместе все полученные знания и попробовать решить настоящую задачу. Именно этим мы и займемся на следующем видео.

Привет! Раньше мы изучали функциональность библиотеки Sklearn на модельных данных, а теперь настало время перейти к прикладной задаче. На этом видео мы потренируемся проходить все шаги от загрузки и предобратки данных до построения финальной модели. Работать мы будем с задачей Bike Sharing. В этой задаче по историческим данным о погодных условиях и аренде велосипедов требуется предсказать, сколько же велосипедов будет занято в заданный день и час. В исходной постановке задачи нам доступно 11 признаков, среди них есть как числовые признаки, так и категориальные, и бинарные данные. Мы будем работать с обучающей выборкой сайта соревнований kaggle. Давайте начнем с импортирования нужных библиотек. И так как мы будем строить графики, нам понадобится pylab. Фактически данные, с которыми мы собираемся работать, представляют собой матрицу «объект–признак», поэтому нам будет удобно работать с ними как с DataFrame. Давайте такой DataFrame получим с помощью функции read_csv. Итак, мы загрузили данные, теперь давайте на них посмотрим. Мы видим, что мы получили некоторый DataFrame. Первые столбцы представляют собой описание каждого объекта, а в последнем столбце мы видим значение целевой метки — количество занятых велосипедов. Также нам доступно описание данных. Мы видим, что каждый объект описывается погодными условиями, например температурой, давлением, скоростью ветра. И в конце нам дана информация о том, сколько же велосипедов было занято. Не будем пока подробно на этом останавливаться, лучше посмотрим размер наших данных. Мы видим, что нам доступно 11 000 объектов практически. И давайте посмотрим, есть ли там пропущенные значения, потому что если они есть, то нам придется задуматься о том, как же их обработать. Делаем это с помощью метода isnull. Ну вот видим, что пропущенных значений нет, теперь можно перейти непосредственно к предобработке данных. Давайте выведем некоторую информацию о нашем DataFrame с помощью метода info. Здесь мы видим, что практически все наши данные представляются цифрами — это либо целочисленный тип, либо тип с плавающей точкой. И только первый столбец, datetime, является объектом типа object. Но на самом деле мы с вами помним, что туда записана дата, поэтому логично было бы использовать эти данные в виде типа datetime. Ну почему это логично? Потому что мы сможем применять специфичные для datetime операции. Вот давайте преобразуем этот тип, сделаем это с помощью комбинации функций apply и to_datetime. И теперь, раз уж мы это сделали, давайте на основе этого столбца рассчитаем два новых признака: первый признак — это месяц, в который происходят события, второй признак — это час. Добавляем их в исходные данные, и теперь давайте посмотрим, как это выглядит. Видим, что появилось два новых столбца: месяц и час — то, что мы хотели. Далее для работы нам удобно разделить наши данные на обучение и тест. Это нужно для того, чтобы мы могли строить нашу модель на обучающей выборке и дальше честно оценить ее качество на отложенном тесте, который в обучении не участвовал. Обратим внимание, что наши данные имеют явную временную привязку — мы знаем час и день, за который мы хотим оценить количество велосипедов. В этом случае нам удобнее разбить данные по времени. Давайте будем строить модель на данных за более ранний период и оценивать ее качество на данных за более поздний период. В данном случае наши данные отсортированы по времени, поэтому для того чтобы получить такое разбиение, достаточно просто отрезать последний кусок данных, допустим последние 1000 объектов, и отправить их в отложенный dataset. Это мы сделаем с помощью метода iloc. Теперь давайте посмотрим на размеры полученных наборов данных. Видим, что действительно 1000 объектов отправляется в отложенный dataset, и остальные почти 10 000 объектов мы будем использовать для обучения. Теперь давайте проверим, что действительно вся обучающая выборка расположена в более раннем периоде времени. И вот, да, мы видим, что обучение заканчивается на час раньше, чем начинается тест. В данном случае нас это вполне устраивает. Для работы с моделями в Sklearn нам требуется отделить целевую метку от остальных данных. Давайте сделаем это для обучающей и для тестовой выборки. Мы знаем, что целевая метка находится в столбце count, поэтому давайте вырежем его из остальных данных. И также вырежем столбец datetime, потому что фактически это просто идентификатор объекта. Делаем это на обоих выборках. И теперь давайте визуализируем целевую переменную, посмотрим на ее распределение на обучении и на тесте. Ну вот, мы видим, что распределение отличается, но, в общем-то, можно сказать, что и во время обучения, и во время теста большую часть времени было занято не более 300 велосипедов. Теперь давайте сделаем следующее. В рамках этого урока мы с вами будем работать только с численными признаками, поэтому давайте сначала их отделим — создадим соответствующий список, — и теперь будем работать только с той частью обучающей и тестовой выборки, которая соответствует этим признакам. Давайте посмотрим, как выглядит получившийся набор данных. Ну да, мы видим, что он состоит только из числовых признаков — как обучение, так и тест. Теперь давайте обучим модель. Так как мы решаем задачу регрессии, давайте обучать SGDRegressor — это регрессия на основе стохастического градиентного спуска. Для начала создадим модель с параметрами по умолчанию и попробуем ее обучить на обучающей части данных. Делаем это с помощью метода fit, а дальше сразу давайте оценим качество с помощью метрики mean_absolute_error — средняя ошибка. Оценивать будем на тестовой части данных. Мы видим, что мы получили какую-то невероятно большую ошибку. Давайте посмотрим, как выглядят наши предсказания, и сравним их с правильным значением целевой функции. Сначала выведем на экран целевую функцию, теперь — наши предсказания, и мы видим, что мы предсказываем невероятно большие числа. Так быть не должно. Давайте посмотрим на коэффициенты регрессии — может быть это прольет свет на ошибку. Так, мы видим, что у нас невероятно большие коэффициенты. Как такое могло произойти? Наверняка из предыдущих уроков вы помните, что многие линейные модели чувствительны к масштабу признаков. В данном случае мы работаем с набором данных, в котором признаки могут сильно отличаться по масштабам. Для того чтобы решить эту проблему, нам нужно сначала отмасштабировать признаки в нашем наборе данных. В Sklearn это можно сделать с помощью модуля preprocessing, нам понадобится объект StandardScaler. Давайте сначала создадим такой объект, и дальше, для того чтобы применить наше преобразование, нам сначала нужно его обучить. Логично предположить, что обучать его можно только на обучающей части данных. Но почему это так? Обычно в жизни на момент, когда мы строим модель, мы еще ничего не знаем про тестовые данные. Поэтому для того чтобы не использовать информацию, которой на самом деле у нас нет — а это может привести к переобучению, — мы будем обучать наши преобразования только на обучающей выборке. Делаем это с помощью метода fit, и после того как мы получим обученное преобразование, мы применим его. Отдельно применим его к обучающей выборке, и отдельно — к тестовой. Это делается с помощью метода transform. Итак, мы получили данные, теперь можно снова обучить модель. Сразу же оцениваем ее качество. И что мы видим? Ошибка стала очень маленькой. Давайте выведем целевую функцию и наши прогнозы. Видим, что мы ошибаемся меньше, чем на один велосипед. Это очень странно. Давайте посмотрим на коэффициенты регрессии, чтобы понять, что же произошло. В таком виде их не очень удобно анализировать, давайте округлим. И что мы видим? Практически все признаки принимают очень маленькие веса, за исключением двух. Давайте посмотрим, что это за признаки. Итак, мы видим, что это признаки casual и registered. Давайте попытаемся вспомнить из описания данных, что они означают. Фактически система аренды велосипедов работает следующим образом: системой может воспользоваться как зарегистрированный пользователь, так и незарегистрированный. В данном случае наши столбцы представляют количество зарегистрированных пользователей, которые используют систему — это столбец registered. И количество пользователей, которые не зарегистрировались, но также хотят арендовать велосипед — это столбец casual. Давайте выведем значения целевой функции и увидим следующую закономерность: фактически, если значение этих двух столбцов сложить, то мы получим нашу целевую метку. Давайте убедимся, что это действительно так на всех данных. Да, это правда так. Фактически два эти столбца в сумме дают нашу целевую метку. Что же тут произошло? Мы с вами совершили стандартную ошибку начинающих data scientist'ов — мы начали анализировать данные и строить модель, детально не разобравшись в значении наших переменных. То есть фактически мы использовали в модели те данные, по которым однозначно восстанавливается целевая функция. Конечно же, так делать не следует. Давайте вырежем эти данные из нашего набора данных. Вырежем их как из обучающей, так и из тестовой выборки. Теперь давайте снова отмасштабируем признаки уже на новом наборе данных. И теперь обучим модель и оценим качество. Видим, что наша ошибка сильно выросла — теперь мы ошибаемся в среднем на 122 велосипеда. Это уже гораздо ближе к истине, эта оценка более реалистична. Для того чтобы до конца убедиться, что мы все делаем правильно, давайте выведем веса и регрессию. Ну да, теперь мы видим, что практически все признаки вносят некоторый вклад в нашу модель — эти веса больше похожи на правильные. Фактически мы с вами получили некоторую базовую модель, некоторый baseline, который мы считаем правильным. Теперь давайте попытаемся его улучшить. Напоминаю, что модель мы обучали с параметрами по умолчанию, поэтому теперь давайте подберем параметры, оптимальные для решаемой задачи. Подбор параметров мы делаем по кросс-валидации. В данном случае с этим может возникнуть проблема, потому что прежде чем обучать модель, мы хотим делать scaling — мы хотим масштабировать признаки. Однако масштабирование мы обучаем только по обучающей части выборки. Таким образом, при кросс-валидации нам придется обучить сразу несколько скейлеров — по одному на каждую итерацию кросс-валидации. Получается, что нам нужно писать неудобные циклы, и запись будет достаточно громоздкой. Однако Sklearn предоставляет нам способ, для того чтобы этого избежать — такой способ называется Pipeline. Вместо одного преобразования мы с вами можем реализовать целую цепочку преобразований. Для этого давайте импортируем Pipeline из модуля pipeline и попытаемся его построить. В данном случае мы хотим делать два шага: первый шаг — это масштабирование признаков, второй шаг — это непосредственно обучение модели. Вот давайте такой Pipeline и создадим. Передаем ему параметр steps — это список наших шагов, и далее каждый шаг представляется тьюплом, где первый элемент — это имя шага, второй элемент — это непосредственно объект, который может преобразовывать данные. Важным условием является то, чтобы у объекта были такие методы, как fit и transform, fit и predict. Вот как наш scaler, так и наш regressor этому интерфейсу удовлетворяют, поэтому мы можем смело вносить их в цепочку. Построили цепочку, и теперь давайте работать с этой цепочкой как с одиночным преобразованием. Фактически это означает, что мы можем эту цепочку обучить с помощью метода fit, а также получить предсказания с помощью метода predict. Давайте это сделаем и посмотрим на ошибку. Формально она не должна измениться, мы ожидаем увидеть ту же самую ошибку, которую мы видели ранее. Итак, да, мы видим, что все получилось — наша ошибка не изменилась. Теперь наша цепочка преобразований готова, мы убедились, что она работает правильно, поэтому давайте перейдем к подбору параметров. Параметры мы будем подбирать по сетке, с помощью перебора различных наборов параметров, поэтому давайте для начала посмотрим, как правильно к ним обращаться. Мы видим, что в случае использования Pipeline нам нужно обращаться к параметрам с помощью расширенного имени. Сначала нам нужно указать имя шага, далее — двойное нижнее подчеркивание и название самого параметра. Вот давайте создадим словарь параметров, которые мы хотим перебирать — довольно просто. Будем перебирать вид функции потерь, количество итераций, вид регуляризации, а также коэффициент перед регуляризацией. И для разнообразия давайте подберем какой-нибудь параметр для скейлинга. Ну, например, давайте подберем среднее. Теперь нужно построить нашу... наш grid, нашу сетку. Строим сетку, передаем туда Pipeline, а также передаем словарь с параметрами. Указываем, что мы будем оценивать метрику «средняя ошибка» и будем делать кросс-валидацию на 4 фолда. Теперь давайте сетку обучим. Так как мы делаем полный перебор по сетке, это может занять существенное время. Итак, сетка обучилась, давайте посмотрим на лучшие значения параметров, а также выведем лучшее значение метрики. И видим, что для обучения нам достаточно трех итераций, нужно использовать квадратичную функцию потерь, scaling нужно делать со средним 0, коэффициент перед регуляризацией должен быть 0,01 и регуляризация принимает вид L2. Теперь давайте оценим лучшую модель на отложенном тесте. Видим, что вроде бы наша ошибка уменьшилась — теперь мы ошибаемся на 120 велосипедов вместо 122, однако это оценка в некоторой точке. Давайте посмотрим, насколько наша ошибка большая относительно среднего значения целевой переменной. Итак, видим, что среднее значение у нас — 232 велосипеда, но при этом мы ошибаемся на 120. Но в общем-то, понятно, что это плохо. И с этой точки зрения 122 от 120 отличаются не очень сильно. Фактически наша оптимизация с помощью подбора параметров не помогла нам улучшить модель, важно это понимать. А для того чтобы в этом убедиться, давайте посмотрим на значения наших предсказаний. Вот для начала их получим, теперь выведем правильные значения целевой метрики, целевой метки, вернее, и выведем наши оценки — видим, что они отличаются очень существенно. Теперь давайте сделаем следующее: отобразим график наших объектов в пространстве правильных значений целевой метки и наших предсказаний. Когда мы строим график в таком пространстве, то для хорошей модели понятно, что мы ожидаем — мы ожидаем облако точек в районе диагонали. Получается, что наши предсказания должны совпадать с целевой меткой, поэтому ну вот диагональ должна получиться. В данном случае мы имеем не очень хорошую модель, давайте посмотрим, как это выглядит. Да, видим, что наши облака точек совсем не похожи на диагональ. Более того, облака точек при использовании модели без подбора параметров и облака точек при использовании модели с подбором параметров не сильно отличаются. То есть фактически наша модель является довольно слабой, и оптимизация по параметрам нам ничего не дала. На этом давайте закончим. В этом уроке мы получили некоторый baseline, некоторую начальную модель, которая работает только на числовых признаках. Также в процессе работы мы с вами научились делать scaling, мы научились строить цепочки преобразований, а на следующем уроке мы постараемся улучшить эту модель, добавив в нее все остальные признаки.

В этом видео мы продолжим анализировать задачу Bike Sharing. Напомню, что в задаче требуется по историческим данным о погодных условиях и аренде велосипедов спрогнозировать количество занятых велосипедов на определенный час определенного дня. Давайте снова импортируем нужные библиотеки и загрузим данные. Напомню, как они выглядят: это некоторая матрица, некоторая таблица, состоящая из описания объектов, и в последнем столбце мы видим значения целевой переменной. Итак, давайте снова сгенерируем признаки «месяц» и «час». И также разобъем данные на обучение и тест. Разбивать данные будем по времени, обучаться будем на более ранних данных, тестироваться будем на более поздних данных. Таким образом, в тест мы отправляем 1000 объектов за наиболее позднее время. Итак, посмотрим на размер получившихся наборов данных. Практически 10 000 данных идет в обучение, и ровно 1000 объектов идет в тест. Теперь давайте снова разделим данные на признаки и целевую метку. В качестве целевой метки используем столбец count, а из признаков вырезаем datetime, как идентификатор объектов, вырезаем столбец count, а также вырезаем два столбца — casual и registered, потому что эти столбцы в сумме дают значение целевой переменной. Делаем это для обучения и для теста. Теперь давайте поговорим про данные. В предыдущем уроке мы с вами рассматривали признаки, которые имеют численный тип. В данном уроке мы с вами хотим обработать все признаки, но при этом мы понимаем, что если признаки являются данными разного типа, то, наверное, и методы, которыми мы будем их обрабатывать, должны отличаться. Поэтому давайте сделаем следующее. Давайте разделим наши данные по типам. Тип данных мы понимаем из описания — оно нам доступно. И давайте получим следующее. Давайте получим некоторый логический индекс, который будет показывать, в каком столбце находятся данные данного типа. То есть мы хотим получить индекс, который будет принимать значение true в позиции нужных столбцов. Для начала давайте сделаем это для бинарных данных. Таких признаков всего два — это holiday и workingday. Признаки принимают значения 0 или 1, соответственно, если данные являются выходным, то первый признак принимает значение 1, в противоположном случае — 0. И если день является рабочим, то второй признак принимает значение 1, в противоположном случае — 0. Вот давайте получим логический индекс для бинарных данных. И сразу посмотрим, как это выглядит. Вот мы получаем массив, у которого true соответствует тем позициям в DataFrame, в котором мы видим эти столбцы. Аналогичные расчеты давайте сделаем для категориальных признаков. Таких признаков всего три: это сезон, погода по ощущениям и месяц. Да, получили правильный индекс и закончим процедуру на численных данных. Все индексы у нас есть. Теперь давайте строить Pipeline, давайте строить цепочку преобразований, в результате которой мы получим dataset, состоящий из обработанных данных. Сначала давайте создадим модель, которую мы хотим применять — это снова будет регрессия SGD. Однако давайте сразу же укажем некоторые параметры, которые мы подобрали в прошлый раз. Понятно, что с учетом новых данных эти параметры могут перестать быть оптимальными, но тем не менее это неплохое приближение. Создаем модель, и теперь давайте посмотрим на цепочку. В данном случае она выглядит чуть более сложной, но давайте разберемся, что же там такое. Фактически наше преобразование снова состоит из двух больших шагов: первый шаг — это обработка данных, второй шаг — это непосредственно построение модели. Давайте для начала посмотрим на построение модели, оно не отличается — здесь мы задаем имя шага и передаем нашу модель, наш регрессор. Теперь давайте разберемся с преобразованием данных. Для того чтобы по-разному обрабатывать разные части dataset, разные столбцы из набора данных, нам нужно сначала данные разбить, получить три набора данных. Дальше каждый из них обработать по отдельности и дальше собрать их вместе. Причем собирать их каждый раз нужно в одинаковом порядке — это важно, потому что фактически мы можем использовать это преобразование для кросс-валидации, для подбора параметров, поэтому нам действительно нужно получать каждый раз набор данных в заданном порядке. Вот давайте посмотрим, как такого добиться. Для того чтобы сначала разделить данные на несколько частей, а потом собрать их вместе, нам нужна функциональность FeatureUnion — это некоторый трансформер, который ровно это и делает. Вот давайте его создадим и будем работать в рамках этого трансформера, внутри него мы будем делать три преобразования, то есть фактически данные разбиваются на три части. Первое преобразование — работа с бинарными данными, второе преобразование — работа с численными данными, и последнее преобразование — работа с категориальными данными. Это и есть наш transformer list — список преобразований. Теперь смотрим на первое. Бинарные данные устраивают нас в таком виде, в котором они уже есть, мы не хотим никак их изменять. Поэтому что мы здесь делаем? Мы просто используем FunctionTransformer, которому мы передаем наш логический индекс — логический индекс, соответствующий бинарным признакам, и говорим, что нам достаточно просто эти данные выбрать, отделить их от всего dataset и, не изменяя, положить их отдельно. Вот ровно это здесь и происходит. Теперь следующий тип данных — эти числовые признаки. Здесь нам придется применить целую цепочку преобразований, потому что фактически их два. Сначала эти данные нужно отделить от остальных, потом нужно их отмасштабировать с помощью scaler. В данном случае нам придется объявить внутри Pipeline, в отличие от предыдущего случая, но это также несложно. Объявляем Pipeline и говорим, что он состоит из двух шагов: первый шаг — это трансформер, то есть фактически мы передаем сюда логический индекс для числовых данных и отделяем их от всех остальных. И дальше к данным, которые мы отделили, мы применяем scaling — делаем масштабирование. На этом преобразование числовых признаков заканчивается. Последний тип признаков, с которыми мы работаем — это категориальные данные. Здесь также будет цепочка преобразований, поэтому создаем Pipeline. Шаги следующие: первый шаг — это отделение этих данных, то есть фактически мы передаем сюда логический индекс для категориальных признаков и отделяем нужную часть набора данных. И дальше применяем методику hot encoding — помните, что если у нас есть категориальный признак, внутри которого доступно несколько значений, то после кодирования hot encoding мы получаем n признаков, каждый из которых является бинарным. Фактически он, каждый из новых признаков, соответствует отдельному значению старого категориального признака — принимает значение 1 на тех объектах, где достигается это значение, и 0 на всех остальных. Вот ровно такое преобразование мы здесь и применяем. На этом наша цепочка преобразований заканчивается, и последнее, что нужно сделать — это собрать данные вместе и применить второй шаг, обучить модель. Вот давайте такую цепочку создадим, и попробуем для начала просто ее обучить, и оценить качество. Делаем это с помощью метода fit, обучение закончено. И теперь оцениваем качество. Вот получаем, что мы ошибаемся на 120 велосипедах — приблизительно то же самое, что и раньше. То есть фактически преобразование данных не помогло нам принципиально улучшить модель. Но давайте посмотрим: может быть, подбор параметров способен на это повлиять? Давайте посмотрим, как в случае такой сложной цепочки обращаться к параметрам. Делаем это с помощью комбинации методов get_params и keys. И видим, что да, но не очень удобно, потому что нужно использовать расширенные имена, нужно указывать имя шага, потом имена всех промежуточных шагов и так вплоть до параметра. Но, в общем-то, это тоже можно сделать. Вот давайте мы с вами не будем перебирать слишком много параметров, а для того чтобы ускорить процесс, переберем только два: это коэффициенты alpha и eta0. Создадим такой словарь. И теперь давайте снова воспользуемся поиском по сетке, будем делать полный перебор. Метрика, которую мы оцениваем — это средняя ошибка. И делаем кросс-валидацию на четыре фолда. Итак, создаем сетку, и теперь давайте ее обучать. В этот раз все должно быть довольно быстро. Да, видим, что процесс занял меньше секунды. И теперь давайте посмотрим на лучшую оценку и на лучшую комбинацию параметров. Ну вот видим, что ошибка вновь довольно большая и мы получили правильные веса. Теперь давайте воспользуемся этой лучшей моделью и оценим ее качество на отложенном dataset. Итак, строим наше предсказание, теперь считаем метрику. Видим, что мы ошибаемся в среднем на 125 велосипедов. Это не очень хороший результат. Давайте выведем метки и выведем наши результаты. Метки и результаты. Да, видим, что, действительно, ошибаемся мы очень сильно. Но давайте теперь построим следующий график. Отобразим наши объекты в координатах исходных значений целевой метки и наших прогнозов. Вот мы получили график. Во-первых, мы видим, что область точек достаточно далека от диагональной области, которая получилась бы в случае хорошей модели. А с другой стороны, мы понимаем, что наш график очень сильно похож на тот график, который мы получили на предыдущем уроке. Это означает, что все наши преобразования — генерация новых признаков, вновь подбор параметров — не привели к улучшению модели, мы не смогли ее улучшить. Как же нам быть в такой ситуации? Но давайте для начала проанализируем, почему это так. Мы с вами строим линейную модель — это говорит о том, что мы предполагаем некоторую линейную зависимость между признаками и целевой переменной. На самом деле это не совсем так. Ведь мы понимаем, что не всегда количество занятых велосипедов линейно зависит, например, от времени. Понятно, что на каком-то отрезке это будет так, допустим, рост времени будет приводить к росту занятых велосипедов, но на другом отрезке это может быть наоборот. И те же самые рассуждения более-менее справедливы, более-менее применимы к другим признакам, например таким как температура, или номер месяца, или давление. Соответственно, что мы может сделать? С одной стороны, чтобы помочь нашей линейной модели, мы можем сгенерировать другие признаки — такие признаки, на которых мы такую зависимость будем предполагать. Другой вариант — мы можем воспользоваться другой моделью, более сложной, которая умеет учитывать нелинейные зависимости между признаками и целевой функцией. Процесс генерации новых признаков является творческим, поэтому, конечно же, вы можете самостоятельно изучить набор данных и придумать новые признаки. Попробуйте, это довольно интересно. А мы с вами исследуем другой подход. Давайте обучим другую, нелинейную модель. Вы такие модели еще не проходили, но это будет некоторым анонсом к следующей неделе. Возможно, это только подогреет ваш интерес. Модель, которую я предлагаю строить, называется «случайный лес». Она находится в модуле ensemble. Вот давайте создадим случайный лес, зададим туда некоторые параметры, не будем пока акцентировать на них внимание. И сделаем следующее: построим точно такую же цепочку преобразований с единственным изменением — подменим наш регрессор с линейной модели на случайный лес, на random forest. Итак, получили цепочку, теперь давайте модель обучим — делаем это с помощью метода fit. И даже не будем подбирать никакие параметры, просто обучим модель и посмотрим на ее ошибку. Видим, что случайный лес обучается несколько дольше, чем линейная модель. Но чуть позже вы узнаете, почему это происходит. Итак, наша модель обучена, оцениваем ее качество. Так, видим, что ошибка сильно уменьшилась. Теперь мы ошибаемся в среднем на 80 велосипедов — ну, кажется, что это сильно лучше, чем было, но непонятно — это улучшение принципиальное или нет? 80 и 120 — большая ли это разница? Давайте для начала выведем наши предсказания и правильные значения целевой функции. М-да, кажется, что стало получше, но все еще непонятно. Давайте попробуем снова построить аналогичный график — отобразим объекты в координатах «значение целевой метки» и «наше предсказание» и посмотрим, приблизились ли эти объекты к диагональной области. А заодно давайте сравним график для «случайного леса» и для линейной модели. Итак, наш график готов. Теперь я думаю, что разница очевидна. Видим, что в данном случае наши объекты очень близко подошли, достаточно близко подошли к диагональной области. Получается, что с помощью этой модели у нас получилось установить зависимость гораздо лучше. А мы на этом заканчиваем. На этом видео мы научились обрабатывать признаки разных типов, а также научились строить сложные вложенные цепочки преобразований. Мы построили несколько моделей на данных Bike Sharing — это линейная модель и модель random forest, а также сравнили их качества. Я надеюсь, что модель random forest вас заинтересовала, потому что уже на следующей неделе вы перейдете к изучению новых семейств алгоритмов, среди которых random forest обязательно будет.

