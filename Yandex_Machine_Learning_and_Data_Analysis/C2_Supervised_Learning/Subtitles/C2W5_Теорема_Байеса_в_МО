[БЕЗ_ЗВУКА] Здравствуйте! Меня зовут Дмитрий Ветров, я являюсь главой исследовательской группы байесовских методов, и сегодня мы предсказуемо поговорим о теореме Баейса. Но сначала вспомним некоторые ключевые понятия из теории вероятности. Итак, если у нас есть две случайные величины x и y, которые между собой взаимозависимы, то можно ввести условное распределение или условную плотность распределения (если мы говорим о непрерывных величинах). Например, условная плотность распределения на y при условии x по определению равна отношению совместной плотности распределения x и y к безусловной или к маргинальной плотности p(x). Совместное распределение всегда можно таким образом представить в виде произведения условного распределения y при условии x на безусловное p(x). И из соображений симметрии то же самое можно сделать в виде произведения p(x) при условии y на p(y). Отсюда мы получаем так называемую формулу обращения условной вероятности, то есть можем выразить условное распределение на x при условии y в виде произведения p от y при условии x на p(x) делить на p(y). Теперь если мы возьмем от обоих частей интеграл по dx, в левой части мы получим 1, поскольку это интеграл от плотности распределения относительно x, а в правой части можно вынести знаменатель за знак интеграла. Теперь если мы перенесем его в левую часть, мы получим так называемую формулу полной вероятности. p(y) = интеграл от условной плотности p от y при условии x на p(x). Или, перефразируя это, можно сказать, что безусловная или маргинальная плотность распределения на y — есть результат матожидания по всевозможным иксам от условных плотностей p от y при условии x. Такое свойство часто называют правило суммирования вероятности. По сути, мы получили способ считать безусловное распределение от условных распределений. Если мы применим правило суммирования вероятности и правило произведения вероятности, мы получим знаменитую теорему Байеса. Условное распределение на y при условии x равно произведению p от x при условии y на p(y) разделить на нормализованную константу, которой является интеграл от числителя, то есть p от x при условии y на p(y)dy. В концептуальной форме теорема Баейса позволяет нам переходить от априорных распределений на неизвестную величину (в данном случае на y) к так называемым апостериорным распределениями на y при условии x. То есть если мы пронаблюдали какую-то косвенную характеристику, которая как-то связана с интересующей нас случайной величиной — с y, мы имеем возможность учесть эту информацию и уточнить наши представления о неизвестной величине y. Для того чтобы понять геометрический смысл условного и безусловного распределения, рассмотрим простой пример. Рассмотрим двухмерную случайную величину x y, в которой компоненты взаимозависимы. Такую случайную величину, точнее линию уровня ее плотности, можно отобразить на таком двухмерном графике. Безусловным распределением p(x) будет являться просто проекция этого двухмерного графика на ось x. Очевидно, математическое ожидание p(x) будет совпадать с матожиданием x в совместном вероятностном распределении. Если теперь мы пронаблюдали значение y (y = y0), мы можем посчитать условное распределение на x при условии y0. Этому распределению будет отвечать сечение нашего двухмерного графика гиперплоскостью y = y0. Обратите внимание, что у условного распределения, во-первых, изменилось матожидание, оно теперь не нулевое, во-вторых, уменьшилась дисперсия. Ну и, наконец, давайте вспомним ключевой метод статистического оценивания из классической статистики, известный как метод максимального правдоподобия. Стандартная задача матстатистики, как известно, — оценить значение неизвестных параметров распределения по заданной выборке из этого распределения. Итак, пусть нам задана выборка X из n объектов (x1, ..., xn), каждый из которых независимо одинаково распределены и сгенерированы из одного и того же распределения p от x при условии θ, то есть распределения, которое нам известно с точностью до параметров θ. Нашей задачей является восстановить эти неизвестные значения параметров θ. Метод максимального правдоподобия достаточно прост и заключается в следующем: мы составляем функцию правдоподобия нашей выборки, то есть произведение плотностей p от xi при условии θ по всем i от 1 до n. xi-тые нам известны, и теперь мы рассматриваем эту функцию как функцию от θ. Если мы найдем максимум этой функции, то аргмаксимум и будет являться так называемой оценкой максимального правдоподобия. На практике, как правило, максимизируют все-таки не сами функции правдоподобия, а их логарифмы, потому что в этом случае наше произведение по объектам выборки переходит в сумму логарифмов p(xi) при условии θ. В математической статистике для метода максимального правдоподобия получен ряд интересных теоретических результатов. В частности известно, что оценки максимального правдоподобия, которые вообще-то являются функциями от случайной величины от нашей выборки, а значит сами являются случайной величиной. Так вот, оценка максимального правдоподобия является асимптотически несмещенной, то есть матожидание оценки максимального правдоподобия при достаточно больших объемах выборки совпадает с истинным значением параметра θ. Кроме того, при n, стремящимся к бесконечности, оценка максимального правдоподобия сходится к истинному значению θ — это свойство состоятельности. Наконец, оценка максимально правдоподобия среди прочих несмещенных оценок обладает наименьшей возможной дисперсией — это свойство эффективности. Также известно, что оценки максимального правдоподобия асимптотически нормальны при больших n, имеют нормальное распределение с матожиданием, равным истинному значению параметра и ковариационной матрицей, связанной с матрицей информации Фишера. Тем не менее у оценок максимального правдоподобия есть один весьма существенный недостаток: большинство теоретических результатов, которые гарантируют корректность и даже оптимальность оценивания по методу максимального правдоподобия, получены при объемах выборки, либо стремящихся к бесконечности, либо достаточно больших. В то же время на практике часто приходится работать с выборками ограниченными. Под объемом выборки в данном случае я имею в виду не просто количество объектов, которые мы пронаблюдали из заданного распределения, а отношение числа объектов к количеству оцениваемых параметров θ. То есть если θ, допустим, это d-мерный вектор, то под эффективным объемом выборки я понимают отношение n к d. Так вот если это отношение немного больше 1, оказывается, что многие результаты из метода максимального правдоподобия перестают быть корректным. В этом видео мы кратко вспомнили основные понятия математической статистики: что такое условное и безусловное распределение; какой у нее геометрический смысл; что такое теорема Баейса; а также поговорили о методе максимального правдоподобия, какие у него есть достоинства и недостатки. В следующем видео мы поговорим об альтернативном подходе к теории вероятности — так называемом баейсовском подходе, и о том, какие альтернативы для статистического оценивания существуют относительно метода максимального правдоподобия.

В этом видео мы рассмотрим, чем же байесовский подход к теории вероятности, к математической статистике отличается от классического или частотного подхода. На самом деле, ключевое различие между частотным подходом, который многие из вас изучали в вузах, и байесовским подходом заключается в том, как трактовать случайность. С точки зрения классического подхода случайная величина — это величина, значение которой мы принципиально не можем предсказать, то есть некоторая объективная неопределенность. В то же время с точки зрения байесовского подхода случайная величина на самом деле является детерминированным процессом, просто часть часть факторов, которые определяют исход этого процесса, для нас неизвестны. Именно поэтому мы и не можем предсказать конкретный исход данного испытания с данной случайной величиной. Из этого сразу вытекают некоторые следствия. Ну, например, с точки зрения байесовского подхода любую неизвестную величину можно интерпретировать как случайную и использовать аппарат теории вероятности, в частности, вводить на нее плотность распределения. При этом, коль скоро случайные величины для нас кодируют субъективное незнание, у разных людей неопределенность на одну и ту же случайную величину может быть разная. Именно поэтому и плотности распределения на эту случайную величину будут отличаться для разных людей, обладающих разной информацией о факторах, влияющих на эту случайную величину. С точки зрения классического подхода величины четко делятся на случайную и детерминированную. И бесмысленно применять аппарат теории вероятности к детерминированным случайным величинам или параметрам. С точки зрения байесовского подхода все величины, значения которых неизвестны, можно интерпретировать как случайные. Соответственно, можно вводить плотность распределения и выполнять байесовский вывод. Основным методом оценивания в классическом подходе является метод максимального правдоподобия, который мы с вами рассмотрели в предыдущем видео. При байесовском подходе к статистике основным выводом является теорема Байеса. Соответственно, результатом оценивания в классическом подходе обычно являются точечные оценки, как правило, это оценки максимального правдоподобия, либо реже — доверительные интервалы. При байесовском же подходе результатом вывода является апостериорное распределение на оцениваемые параметры. Метод максимального правдоподобия является оптимальным при n стремящемся к бесконечности, соответственно, большинство теорем в теории вероятности, которые обосновывают корректность применения этого метода, доказывают предположение, что объем выборки, по которой мы оцениваем неизвестный параметр, много больше 1. В то же время байесовский подход можно использовать при любом объеме выборки, даже если объем выборки равен 0. В этом случае результатом байесовского вывода и апостериорного распределения просто будет являться априорное распределение. В то же время, если объем выборки, а именно отношение n к d, где n — это количество объектов, а d — это размерность оцениваемых параметров, много больше 1, результат байесовского вывода начинает стремиться к результату, оцениваемому с помощью метода максимального правдоподобия. Тем самым все теоретические гарантии, которые известны для метода максимального правдоподобия, применимы и к результату байесовского вывода. Одним из преимуществ байесовского подхода является возможность объединения разных вероятностных моделей, которые отражают те или иные косвенные характеристики оцениваемой неизвестной величины. Например, представим себе следующую ситуацию. У нас есть m различных измерений, каждое из которых каким-то образом характеризует неизвестную величину x. Для каждого измерения у нас есть своя вероятностная модель, которая показывает, насколько данное значение j-го измерения вероятно, если случайная величина приняла то или иное значение. Нашей задачей является оценить скрытую неизвестную величину x по наблюдениям y1 yn. Зафиксируем наше исходное незнание о величине x в виде априорного распределения p(x). После чего применим первую вероятностную модель, которая увязывает x и y1. Применив формулу Байеса, можем получить апостериорное распределение на x при условии, что мы пронаблюдали y1. Теперь, если мы начнем анализировать результат второго измерения, которое может быть никак не связано с первым измерением и получено из совершенно другой вероятностной модели, то мы снова можем применить байесовский вывод, только теперь в качестве априорного распределения на x мы положим апостериорное распределение, полученное после измерения y1. То есть в качестве априорного распределения мы поставим p(x) при условии y1. Применив теорему Байеса, мы получаем апостериорное распределение на x, но уже при условии y1y2. Действуя так m раз мы в итоге получим апостериорное распределение на x при условии y1, ..., ym, которое отражает максимум информации, которую мы могли извлечь о величине x при условии, что мы пронаблюдали y1, ..., ym. Если бы мы использовали точечные оценки вместо апостериорных распределений, мы бы оказались в положении слепых мудрецов из известной притчи, которые пытались изучать слона путем различных тактильных ощущений. Как известно, в притче мудрецы не смогли прийти к единому мнению, в то же время, если бы они оперировали байесовским аппаратом и получали бы апостериорное распределение, скорее всего, они смогли бы прийти к мнению относительно того, что же они изучают. В этом видео мы с вами изучили, что такое байесовский подход к теории вероятности, чем он отличается от классического подхода, и посмотрели, как благодаря использованию теоремы Байеса можно объединять несколько вероятностных моделей в более сложную модель. В следующем видео мы посмотрим, как же байесовский подход может быть использован к машинному обучению, и какими преимуществами он обладает.

[БЕЗ_ЗВУКА] В этом видео мы рассмотрим, как байесовские методы могут быть применены к машинному обучению, и рассмотрим, какие же преимущества есть у байесовских методов относительно обычных алгоритмов машинного обучения. Задачу машинного обучения можно интерпретировать как задачу восстановления зависимости между наблюдаемыми и скрытыми компонентами. При это зависимость восстанавливается по обучающей выборке, в которой предполагается, что мы знаем и наблюдаемые, и скрытые компоненты. Если мы оперируем в рамках вероятностного подхода или, более конкретно, в рамках байесовского подхода, в качестве модели зависимости между наблюдаемыми и скрытыми компонентами мы используем совместные вероятностные распределения над наблюдаемыми, скрытыми компонентами, и если мы говорим про байесовский подход, то еще и над параметрами, которые настраиваются в ходе процедуры обучения. Рассмотрим для примера задачу линейной регрессии. В линейной регрессии в качестве наблюдаемых параметров у нас выступают признаки объекта, в качестве скрытых параметров выступают значения целевой переменной, а в качестве параметров, которые настраиваются в ходе процедуры машинного обучения, выступают веса линейной регрессии. Тогда на байесовском языке такую модель линейной регрессии можно сформулировать в виде совместного вероятностного распределения на t и w при условии x. Строго говоря, если мы действуем с помощью байесовских методов, нам необходимо вводить совместное распределение на t, w и x. Но поскольку мы всегда предполагаем, что xi-тое у нас будет известно как в обучающей, так и в тестовой выборке, моделировать на них распределение излишне. Такие модели называются дискриминативными моделями. В противовес им рассматривают так называемые генеративные модели, в которых совместные распределения вводятся как на скрытые величины t и настраиваемые параметры w, так и на наблюдаемые параметры x. В рамках генеративных моделей помимо прочего возможно решать задачу по генерации новых объектов. В рамках же дискриминативных моделей, в которых мы не модулируем дополнительно распределение на x, класс задач, который мы можем решать, ограничивается задачами прогноза скрытой компоненты t при условии наблюдаемой компоненты x. Итак, рассмотрим следующую дискриминативную модель: совместное распределение на t и w при условии x по правилам произведения разложим на функции правдоподобия (то есть распределение на t при условии x, w) и априорное распределение на w. В качестве функции правдоподобия будем брать нормальное распределение на t с матожиданием в точке w транспонированной x (то есть в точке линейной комбинации), и некоторой заданной дисперсии сигма квадрат. А в качестве априорного распределения на w положим нормальное распределение с нулевым матожиданием и единичной ковариационной матрицей. Предположим, что нам задана обучающая выборка, состоящая из n объектов, с известной наблюдаемой и скрытой компонентой xi-той, где i меняются от 1 до n, и попробуем найти максимум апостериорного распределения. Выполнив байесовский вывод, легко показать, что максимум апостериорного распределения будет совпадать с максимумом по w числителя в формуле Байеса, потому что знаменатель формулы Байеса не зависит от w. Числитель формулы Байеса в свою очередь представляет из себя произведение функции правдоподобия обучающей выборки на априорное распределение. Перейдя к логарифму, это же можно записать в виде суммы логарифмов отдельных правдоподобий, то есть логарифмов правдоподобий каждого объекта плюс логарифм априорного распределения на w, и подставив туда конкретные значения для наших плотностей из вероятностной модели, мы получим, что это просто сумма квадратов отклонений ti от xi транспонированное на w плюс сигма квадрат на квадрат нормы w. Фактически мы получили хорошо известный метод наименьших квадратов с L2-регуляризатором. Таким образом, известный метод наименьших квадратов с L2-регуляризатором может быть переформулирован на языке байесовских моделей и соответствует достаточно простой вероятностной модели, в которую мы просто ввели гауссовское априорное распределение с нулевым матожиданием на веса линейной регрессии. Какие же преимущества дает использование байесовских моделей в машинном обучении? Первое преимущество мы уже рассмотрели с вами на предыдущем видео, когда говорили о возможности построения сложных вероятностных моделей из более простых, как мы строим стену из кирпичиков. Это становится возможным благодаря тому, что результат байесовского вывода в одной модели (то есть апостериорное распределение) можно использовать в качестве априорного распределения в следующей вероятностной модели. Тем самым происходит зацепление разных вероятностных моделей. Еще одним преимуществом байесовских методов является возможность обработки массива данных, которые поступают последовательно. В самом деле, используя апостериорное распределение в качестве априорного при поступлении новой порции данных, можем легко произвести обновление апостериорного распределения без необходимости повторного обучения модели с нуля. Если бы мы использовали точечные оценки на настраиваемом параметре, это сделать было бы невозможно, нам пришлось бы заново обучать модель. Еще одним преимуществом байесовских методов является возможность использования априорного распределения, которое предотвращает излишнюю настройку неизвестных параметров под обучающую выборку. Это в свою очередь позволяет избежать эффекта переобучения, который часто свойственен даже задачам, в которых присутствует гигантский объем обучающих выборок, ну в ситуации, когда и количество настраиваемых параметров у нас тоже достаточно велико. Благодаря использованию априорных распределений мы можем регуляризовывать нашу модель машинного обучения и предотвращать эффект переобучения. Наконец, одним из ключевых достоинств байесовских методов является возможность работы с не полностью размеченными, частично размеченными, а то и вовсе не размеченными обучающими выборками. То есть в ситуациях, когда в обучающих выборках нам известна наблюдаемая компонента, а скрытая компонента известна не для всех объектов, либо для многих объектов известно не точное значение скрытой компоненты, а лишь некоторое допустимое подмножество скрытых компонент. Такие выборки называются частично размеченными. Оказывается, что байесовский формализм, байесовское моделирование позволяет абсолютно корректно работать с такими моделями и извлекать из них максимум имеющейся информации о неизвестных значениях параметров. В этом видео мы с вами рассмотрели пример применения байесовской модели к известному методу машинного обучения, а именно к линейной регрессии, а также поговорили о том, какими преимуществами обладают байесовские методы относительно классических подходов к машинному обучению.