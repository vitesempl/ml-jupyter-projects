В этом уроке мы будем с вами заниматься методами, позволяющими проанализировать взаимосвязь между одним признаком и большим количеством других. Давайте рассмотрим следующий пример. Пусть нас интересует: влияет ли употребление алкоголя на успеваемость школьников? Лучший способ это понять — провести эксперимент. Возьмем случайную выборку школьников. Каждому из них назначим случайную еженедельную дозу алкоголя. По окончании учебного года измерим корреляцию между назначенной дозой и успеваемостью школьников. Наш эксперимент идеален, поскольку доза назначается случайно, наша выборка автоматически балансируется по всем возможным типам школьников, которые только могут быть. Еще лучше этот эксперимент был бы только, если школьники бы сами не знали какое количество алкоголя они принимают, но это достаточно сложно обеспечить. Еще один более существенный недостаток этого эксперимента заключается в том, что нам никогда не дадут его провести — это неэтично. Такие ситуации возникают достаточно часто. Мы никогда не сможем исследовать взаимосвязь между уровнем насилия в видеоиграх и агрессивностью детей в жизни, поскольку мы не можем заставить детей играть в видеоигры с высоким уровнем насилия какое-то продолжительное количество времени, если они сами этого не хотят. Иногда проведение эксперимента не только неэтично, а попросту невозможно. Например, если вы хотите понять, как влияет средняя дневная температура на вероятность возникновения лесного пожара, у вас нет никакого способа провести эксперимент, потому что средней дневной температурой в лесу вы управлять никак не можете. Единственное, что остается в таких условиях, когда нельзя провести эксперимент, — это использовать обзервационные данные. То есть данные, которые собраны каким-то образом просто путем наблюдения за выборкой. В задаче исследования успеваемости школьников мы можем, например, взять данные по 633 ученикам старших классов двух португальских школ, для которых известно большое количество разных демографических показателей и показателей успеваемости. В частности, среди всех показателей есть уровень потребления алкоголя по выходным и финальная оценка по португальскому языку. Если мы посмотрим только на эти две оценки, мы увидим вот что. Здесь на горизонтальной оси отложено потребление алкоголя, от 1 до 5 (увеличение), а по вертикальной оси — средняя оценка, опять же, естественно, чем больше она, тем лучше. Мы видим, что эти две величины друг с другом отрицательно коррелированы. Эта корреляция значима. Значит ли это, что потребление алкоголя влияет на успеваемость старшеклассников? Значит ли это, что чем больше алкоголя они потребляют, тем хуже они учатся? Чтобы точнее ответить на этот вопрос мы можем использовать еще 29 признаков, которые у наших школьниках описаны. Эти признаки потенциально влияют на успеваемость гораздо сильнее, чем употребление алкоголя. Например, возраст учеников или доход их родителей... Это вещи, которые определяют успеваемость гораздо более явно. Если мы учтем влияние этих признаков, останется ли у потребления алкоголя предсказательная сила? Можно ли утверждать, что потребление алкоголя вызывает снижение оценки по португальскому языку? То есть можно ли утверждать, что между этими двумя признаками есть причинно-следственная связь? Оказывается на такие вопросы можно отвечать с помощью линейной регрессии. Задача линейной регрессии: у нас есть n объектов, на которых измерены значения k признаков x и, кроме того, на них известно значение отклика y. Мы ищем какой-то вектор константы β такой, что y примерно приближается линейной комбинации x с весами β. Когда мы строим регрессию, мы строим наилучшее линейное по x приближение условного математического ожидания y при таких x. В линейной регрессии коэффициент βj показывает, насколько в среднем увеличивается отклик y, если xj увеличивается на 1, а все остальные x зафиксированы. Таким образом, используя регрессию, мы можем изолировать эффект интересующей нас переменной и посмотреть на него отдельно. Иногда этот эффект можно даже интерпретировать как причинно-следственную связь, при выполнении некоторых особенных условий. Строить обычную линейную регрессию очень просто. Мы все это очень хорошо умеем. Однако если мы хотим по построенной модели делать какие-то выводы с использованием статистических методов, необходимо приложить дополнительные усилия. Именно этому и будет посвящен этот урок.

[БЕЗ ЗВУКА] Из этого видео вы узнаете, какими свойствами обладает решение задач регрессии методом наименьших квадратов, а также какие предположения должны выполняться, для того чтобы мы по этому решению могли делать какие-то выводы. Мы решаем задачу линейной регрессии, приближая условное матожидание y по x линейной комбинации x. Для того чтобы больше не думать про коэффициент b0 просто добавим в нашу матрицу объекты-признаки x единичный столбец. Теперь эта матрица размера n на k + 1. Задачу регрессии мы будем решать методом наименьших квадратов без всяких регуляризаторов. Точное решение этой задачи известно. β с крышкой задается вот такой аналитической формулой. Соответственно, можно посчитать и y с крышкой, то есть предсказание нашей модели на объектах, на которых она обучается. Чтобы посчитать качество решения, получаемого методом наименьших квадратов, определим величину TSS (Total Sum of Squares), как разброс y относительно своего среднего. Оказывается, что этот разброс можно поделить на две части. Одна из частей, объясненная сумма квадратов — это сумма квадратов отклонений среднего y от предсказанных y. Вторая часть, остаточная сумма квадратов, RSS — это сумма квадратов отклонений предсказанных y от их истинных значений. По этим величинам, ESS и TSS, мы можем составить меру r², которая называется коэффициентом детерминации — это отношение ESS к TSS, по сути, это доля объясненной дисперсии отклика во всей дисперсии отклика. Давайте посмотрим какие предположения необходимо сделать для того, чтобы решение метода наименьших квадратов обладало интересующими нас свойствами. Во-первых, мы будем предполагать, что истинная модель y действительно линейна, то есть y можно представить в виде X * β + какая-то ошибка ε. Во-вторых, мы будем предполагать, что наблюдения, по которым мы оцениваем нашу модель, случайны, то есть наши объекты дают независимую выборку из пар xi, yi. В-третьих, нам нужно, чтобы матрица X была матрицей полного столбцового ранга, то есть ни один из признаков не должен являться линейной комбинацией никаких других признаков. Поскольку среди столбцов есть константа, никакой из признаков в нашей выборке не должен быть и константой тоже. Далее, мы будем предполагать, что ошибка случайна, то есть ее условное математическое ожидание по x должно быть равно 0. Уже из этих четырех предположений можно вывести полезное свойство оценок, получаемых методом наименьших квадратов. Если они выполняются, то оценки β с крышкой являются не смещенными и состоятельными оценками истинных β. То есть их математическое ожидание совпадает с истинным β и с ростом объема выборки вероятность отклонения от математического ожидания постепенно уменьшается. Добавим к четырем предположениям еще пятое предположение гомоскедастичности ошибок. Мы будем считать, что дисперсия ошибки не зависит от значений признака, то есть условная дисперсия ε по x равна константе σ². Вместе эти пять предположений называются предположениями Гаусса-Маркова. Теорема Гаусса-Маркова утверждает, что если эти предположения выполняются, то наши МНК-оценки имеют наименьшую дисперсию в классе всех оценок β линейных по y. То есть оценки методом наименьших квадратов уже при выполнении этих 5 предположений в каком-то смысле являются оптимальными. Из сделанных предположений вытекает вот такое выражение для дисперсии МНК-оценок. Дисперсия βj с крышкой определяется тремя компонентами. Первая компонента — это σ², дисперсия шума. Чем больше σ², тем больше дисперсия нашей оценки βj. Чем больше в данных шума, тем менее точно мы можем оценить нашу модель. Вторая компонента — TSSj, это разброс j-го признака x относительно своего среднего. Эта величина стоит в знаменателе дисперсии. То есть чем сильнее признак варьируется в нашей выборке, тем меньше дисперсия у коэффициента при этом признаке в нашей моделе. Наконец, третья компонента — (1 − Rj²), где Rj² – это коэффициент детерминации при регрессии xj на все остальные x. Таким образом, чем лучше наш признак xj объясняется линейной комбинацией всех остальных x, тем больше дисперсия нашей оценки. Чем бесполезнее признак, чем лучше он объясняется всеми остальными x, тем хуже при этом признаке мы можем определить коэффициент. R²j по предположению о полноте столбцового ранга матрицы x не может быть в точности равно 1, но оно может быть очень близко к 1. И эта ситуация называется мультиколлинеарностью. В матричном виде выражение для дисперсии вектора оценок β с крышкой выглядит вот так: это произведение σ² * (X транспонированное X) в −1. Если матрица X содержит столбцы, которые почти линейно зависимы, то X транспонированное X будет плохо обусловлено, и при ее обращении будет получаться численная неустойчивость. Поэтому дисперсия оценок βj с крышкой будет велика. Обратите внимание, что определение «мультиколлинеарности» не включает случай, когда столбцы полностью линейно зависимы. Мультиколлинеарность — это когда признаки почти линейно зависимы. Добавим к 5 предположениям Гаусса-Маркова еще одно — предположение о нормальности ошибки ε. Таким образом, ошибка ε будет иметь нормальное распределение с нулевым средним и дисперсией σ². Это то же самое, что сказать, что распределение y по x условное, нормальное, со средним x на β и дисперсией σ². Если выполняются эти 6 предположений, то оценки, даваемые методом наименьших квадратов совпадают с оценками максимального правдоподобия. Это открывает нам доступ к прекрасным свойствам оценок максимального правдоподобия. Из этих 6 предположений вытекает, что оценки метода наименьших квадратов, во-первых, имеют наименьшую дисперсию среди всех несмещенных оценок β. Во-вторых, имеют нормальное распределение со средним β и дисперсией σ² * (X транспонированное X) в −1. Далее, дисперсию шума σ² можно оценить с помощью RSS. Для этого достаточно всего лишь поделить RSS на правильное число степеней свободы n − k − 1. Кроме того, отношение RSS к истинной дисперсии шума σ² будет распределено по χ квадрат с числом степеней свободы n − k − 1. Наконец, следующее очень сильное свойство. Для любого вещественного вектора длины k + 1 справедливо следующее утверждение: вот такое выражение, задающее какое-то отклонение β от β с крышкой, его произведение на наш произвольный вектор c, распределено по Стьюденту с числом степеней свободы n − k − 1. Что из этого можно выжать? Если выполняются предположения (1)-(6), то мы можем строить доверительный интервалы для коэффициентов βj, можем строить доверительные интервалы для среднего отклика, для матожидания y при таких x. Кроме того, мы можем строить предсказательные интервалы для значения y при таких x. Все это мы с вами научимся делать. Итак, в этом видео мы разобрались с тем, какие предположения нужны для того, чтобы делать выводы по регрессионной модели. К сожалению, никаких регуляризаторов мы использовать не можем. Теория построения выводов при использовании регуляризаторов только развивается, и в ней нет еще никаких готовых решений. В следующем видео мы научимся строить доверительные интервалы и проверять гипотезы.

[БЕЗ ЗВУКА] В этом видео мы научимся для коэффициентов регрессии строить доверительные интервалы и проверять о них гипотезы. Из предыдущего видео, напомню, мы узнали, что, если мы делаем шесть необходимых предположений, из этого вытекают очень полезные свойства, что, во-первых, отношение RSS к истинной дисперсии ошибки σ² распределено по χ², во-вторых, распределение коэффициентов, получаемых при регрессии методом наименьших квадратов β с крышкой, нормальное. И в-третьих, справедливо вот это вот утверждение достаточно общее о том, что вот такая синтетическая конструкция распределена по Стьюденту. Давайте это немедленно используем. Во-первых, доверительный интервал для дисперсии шума σ² можно построить через отношение RSS к квантилям распределения χ². Во-вторых, чтобы построить доверительные интервалы для коэффициента βj, возьмём последнее утверждение с распределением Стьюдента и в качестве вектора c выберем вектор, состоящий из всех нулей, в котором на j позиции стоит 1. Позиции мы нумеруем от 0 в соответствии с нумерацией коэффициентов в векторе β. Тогда доверительный интервал для коэффициента βj задаётся следующим образом. Это наша точечная оценка βj с крышкой ± квантиль распределения Стьюдента порядка 1 − α/2 Распределение берётся с числом степеней свободы n − k − 1, умноженное на корень из дисперсии, который представляет собой произведение σ с крышкой и j-го диагонального элемента матрицы (X транспонированное X) в −1 под корнем. Чтобы построить доверительный интервал для математического ожидания отклика y на новом объекте, задаваемом вектором x₀, просто в качестве c возьмём этот самый вектор x₀. Доверительный интервал готов. Он равен x₀ * β с крышкой ± квантиль распределения Стьюдента * σ с крышкой * √x₀ транспонированное (X транспонированный X) в − 1 x₀, готово. Чтобы построить предсказательный интервал для значения отклика на этом же самом объекте, нам нужно всего лишь дополнительно учесть ещё дисперсию ошибки. Формула для предсказательного интервала отличается от формулы для доверительного интервала для матожидания только вот этой единицей, стоящей под корнем. Где доверительные интервалы, там и проверка гипотез. Для проверки гипотезы о том, что коэффициент βj = 0, можно использовать вот такой T-критерий Стьюдента. Гипотеза о равенстве нулю коэффициента βj переводится на человеческий язык как признак xj не влияет на отклик y. Нулевую гипотезу βj = 0 можно проверять против любой односторонней и двусторонней альтернативы. Это делается с помощью статистики T равной отношению βj с крышкой к его дисперсии. И если справедлива нулевая гипотеза, эта статистика имеет распределение Стьюдента числом степеней свободы n − k − 1. Давайте посмотрим на примере, как это можно применять. Пусть у нас есть 12 испытуемых, и x — это результат прохождения ими составного теста на скорость реакции. А y — это их результат на симуляторе транспортного средства. y получать долго и дорого, поэтому ставится задача в предсказании y по x. Можно ли это делать? Строим вот такую регрессионную модель, проверяем гипотезу о том, что x значимо лучше предсказывает y, чем ничего. Нулевая гипотеза β₁ = 0 против двусторонней альтернативы β₁ ≠ 0 критерием Стьюдента отвергается. Достигаемый уровень значимости 2 * 10 в −5. Если мы хотим проверять гипотезу о том, что сразу несколько коэффициентов в нашей модели равны 0, мы будем использовать не критерий Стьюдента, а критерий Фишера. Поделим нашу матрицу объекта признаки X на две части. В первую часть X₁ поместим все признаки, в которых мы уверены. Обязательно нужно оставить там же и константу. Во вторую часть X₂ перенесём все признаки, гипотезу о значимости влияния которых на отклик мы хотим проверить. За β₁ и β₂ обозначим соответствующие куски вектора параметров модели β. Проверяем нулевую гипотезу о том, что все компоненты вектора β₂ равны нулю, против общей альтернативы H₀ неверна. Это делается с помощью статистики F, которая определяется через соотношение двух RSS. Первый RSS — это RSS сокращённой модели, RSS с нижним индексом r. Это модель, в которой признаки из X₂ вообще не используются. Вторая RSS — это RSS полной модели, в которой есть признаки и X₁, и X₂. Если нулевая гипотеза справедлива, то вот такая статистика F, составленная из этих двух RSS, имеет распределение Фишера с числом степеней свободы k₁ и n − k − 1. Пример. Пусть у нас есть 1191 ребёнок, про которых мы знаем: их вес при рождении, среднее число сигарет за один день беременности, которые выкуривала мать, номер ребёнка у матери, среднемесячный доход семьи, а также длительность получения образования в годах матерью и отцом. Мы хотим проверить гипотезу о том, что образование родителей не является значимым предиктором при предсказании веса ребёнка при рождении. Используем критерий Фишера. Проверим нулевую гипотезу о том, что β₄ и β₅ равны 0 против общей альтернативы. Критерий Фишера даёт достигаемый уровень значимости примерно 0,2. То есть данные не позволяют отклонить такую гипотезу. Если k₁ = 1, критерий Фишера даёт нам достигаемый уровень значимости абсолютно такой же, как дал бы нам критерий Стьюдента для этого же самого признака при использовании двусторонней альтернативы. Если k₁ > 1, могут возникать разные неоднозначные ситуации. Например, критерий Фишера может говорить, что гипотеза незначимости признаков X₂ отвергается. При этом критерий Стьюдента не может отвергнуть никакую из гипотез о признаках, лежащих внутри X₂. Мы получаем странную ситуацию. Все вместе признаки значимого определяют отклик, но ни один из них значимо на отклик не влияет. Это можно объяснить двумя способами. Способ номер 1. Такая ситуация может возникать, если отдельные признаки из X₂ недостаточно хорошо объясняют наш отклик. Но совокупный их эффект при использовании, при прогнозировании y значим. Второе объяснение. Признаки из X₂ мультиколлинеарны. Мультиколлинеарность приводит к численной неустойчивости критериев Стьюдента и Фишера, поэтому их достигаемые уровни значимости могут быть неадекватными. Теперь противоположная ситуация. Пусть критерий Фишера не отвергает гипотезу о незначимости признаков из X₂, а критерий Стьюдента по отдельным компонентам X₂ какие-то из гипотез отвергает. То есть все вместе признаки у нас незначимы, а какие-то из них по отдельности оказываются значимыми. Для этого тоже может быть два объяснения — хорошее и плохое. Первый вариант. Незначимые признаки из X₂ маскируют влияние значимых. Это вполне возможно. Второй вариант. Значимость отдельных признаков из X₂ — это просто результат эффекта множественной проверки гипотез. Действительно, критерии Фишера проверяют всего одну гипотезу, а критерии Стьюдента проверяют целую серию из k₁ гипотез. Какие-то из них могу отклониться просто случайно. Критерий Фишера имеет особенный вид, если мы хотим проверить гипотезу о том, что все признаки x для предсказания y не нужны. То есть лучшее предсказание для y — это его константа. Проверяется, таким образом, гипотеза о том, что все β с 1 по k-й равны 0. В этом случае статистика критерия выглядит просто и определяется просто через коэффициент детерминации, нормированной на число степеней свободы. Нулевое распределение статистики точно такое же – это распределение Фишера. Давайте в предыдущей задаче с весом детей при рождении проверим гипотезу о том, что модель, которую мы построили, вообще имеет хоть какой-то смысл. Проверяем гипотезу о том, что все β = 0 против общей альтернативы, критерием Фишера она уверено отвергается. Достигаемый уровень значимости порядка 6 * 10 в −9. Итак, в этом видео мы научились строить доверительные интервалы для коэффициентов регрессионной модели. А также доверительные и предсказательные интервалы для среднего значения отклика и просто для значения отклика на новом объекте. Кроме того, мы научились проверять гипотезы о коэффициентах модели с помощью критериев Стьюдента и Фишера. В следующем видео мы поговорим о том, как проверить, выполняются ли предположения 1.6, которые лежат в основе всей этой машины статистических критериев, которые мы в этом видео изучили.

В этом видео мы узнаем, как проверять шесть предположений, лежащих в основе всей статистической машины, с помощью которой мы проверяем значимость коэффициентов регрессии. Предположения перед вами. Давайте пойдем по этому списку по порядку. Первое предположение — это предположение о линейности отклика. Мы утверждаем, что y в действительности представляет собой линейную комбинацию X с какой-то случайной ошибкой ε. Естественно, это предположение в точности не выполняется никогда. Трудно ожидать, что ваш отклик y в действительности — это линейная комбинация рассматриваемых вами признаков x. Линейная модель, как и все остальные, неверна, но очень полезна, и кроме того, устойчива к небольшим отклонениям от линейности. Поэтому единственное, что мы будем проверять, это нет ли каких-то огромных отклонений от линейности y по x. Для этого мы будем использовать остатки. Построим регрессию y на x, посчитаем разность между истинными y и предсказываемыми y (y с крышкой). Это и есть остатки. Будем строить для остатков графики. По горизонтальной оси будем откладывать значение каждого из признаков xj, по вертикальной оси — остатки, и будем смотреть, как выглядит это облако точек. Если, например, оно выглядит вот так, представляет собой какую-то параболу, то скорее всего это значит, что ваш отклик y зависит от квадрата признака x. Такую зависимость можно учесть, просто добавив в матрицу X столбец, соответствующий квадрату признака xj. На таком графике можно обнаруживать и другие какие-то осмысленные функциональные зависимости, если вы их видите, просто добавляйте в матрицу X соответствующий столбец. Следующее предположение — предположение случайности выборки. Мы хотим, что выборка наша была независимой и одинаково распределенной. Это предположение может нарушаться несколькими способами. Первый способ более тяжелый: если объекты, на которых измерены наши признаки и отклик, зависимы, то всё плохо. Дисперсию ошибки ε и коэффициентов βj с крышкой по формулам из предыдущего видео мы недооцениваем, и все статистические критерии, которые на этом основаны, перестают работать корректно. Еще это предположение может нарушаться, если выборка отобрана из генеральной совокупности не случайно, а каким-то образом отфильтрована. Фильтровать генеральную совокупность по какому-то признаку z можно только в случае, если условное матожидание y по x совпадает с условным матожиданием y по x и z, то есть z не добавляет никакой новой информации об y. Если мы отфильтровали как-то по-другому, например, просто по одному из признаков, содержащихся в x, то выводы, построенные по такой модели, можно распространять только на отфильтрованную генеральную совокупность. Например, если в нашей выборке испытуемые только младше 50 лет, мы не можем ничего сказать об испытуемых в генеральной совокупности, которым больше 50 лет. Следующее предположение: матрица X должна иметь полный столбцовый ранг, то есть ее ранг должен быть равен k + 1. Если в вашей выборке есть линейно зависимые признаки, то дисперсия оценки коэффициентов при таких признаках будет бесконечной. Это не очень удобно при построении доверительных интервалов: это будут доверительные интервалы бесконечной ширины. И кроме того, гипотезы тоже так особо не проверишь. Если вы столкнулись с такой проблемой, это значит, что от каких-то признаков в вашей модели придется избавиться. Это означает, помимо всего прочего, что если у вас есть категориальные переменные, вы не можете на них использовать one-hot encoding, которым мы пользовались в предыдущих курсах. Дело в том, что если вы каждый уровень фактора кодируете своей бинарной переменной, в сумме они дают 1, а 1 уже в нашей матрице X есть, поэтому столбцы у нас линейно зависимы. Вместо этого нужно использовать другой способ кодирования: dummy-кодирование. Если признак xj принимает m различных значений, то его нужно кодировать m − 1 эффективной переменной. Давайте посмотрим, как это работает, на примере. Пусть y — это уровень заработной платы, а x — это занимаемая человеком должность. Пусть у нас x принимает три значения: рабочий, инженер и управляющий. Эти три значения мы будем кодировать двумя фиктивными переменными — x1 и x2. Должность рабочего соответствует двум 0. Инженер — 1 и 0, управляющий — 0 и 1. В полученной регрессионной модели два признака — x1 и x2, и коэффициенты β1 и β2 при них кодируют среднюю разницу в уровнях зарплат инженера и рабочего и управляющего и рабочего. В регрессионных моделях с dummy-кодированием интерпретация коэффициентов β модели всегда ведется относительно уровня фактора, который закодирован всеми 0. Вы можете менять кодировку dummy, используя какие-то свои соображения о том, какая из моделей будет удобнее интерпретироваться. Продолжим двигаться по списку предположений. На очереди предположение о случайности ошибки. Условное математические ожидание ε по x должно быть равно 0. Гипотезу о том, что оно равно 0, можно очень легко проверить по данным. Строите регрессию y на x, считаете остатки, проверяете гипотезу о том, что среднее значение остатков равно 0. Это можно сделать, например, критерием Стьюдента. Пятое предположение — предположение гомоскедастичности ошибки. Дисперсия ε условная по x должна быть константой вне зависимости от того, какие x мы берем. Это предположение можно проверять двумя способами. Первый, нестрогий, — это визуальный анализ. Вы строите графики зависимости остатков от всех признаков xj и смотрите, выглядят ли точки на этом графике как горизонтальная полоса. Если вместо горизонтальной полосы вы видите что-то расширяющееся или сужающееся, наоборот, значит, у вас есть гетероскедастичность, предположение гомоскедастичности не выполняется. Формально это предположение можно проверять с помощью критерия Бройша-Пагана. Нулевая гипотеза о константной дисперсии проверяется этим критерием против общей альтернативы с помощью очень интересной статистики — это коэффициент детерминации, полученный при регрессии квадратов остатков на признаки x. Вот этот коэффициент детерминации, умноженный на n, если справедлива нулевая гипотеза, имеет распределение хи-квадрат с числом степеней свободы k. Наконец, шестое предположение. Это предположение нормальности. С тем, как проверять нормальность, мы уже разбирались. Есть способ визуальный: вы строите ку-ку график и смотрите, лежат ли на этом графике точки более-менее на одной прямой. И есть способ формальный: вы можете использовать статистические критерии для проверки нормальности, среди всего разнообразия критериев рекомендуется использовать критерий Шапиро-Уилка. Итак, в этом видео мы обсудили все необходимые инструменты, которые нужны для проверки шести лежащих в основе статистических критериев, связанных с регрессией, предположений. В следующем видео мы обсудим, в каких случаях для построенных регрессионных моделей возможна причинно-следственная интерпретация.

[БЕЗ_ЗВУКА] В этом видео мы с удивлением обнаружим, что линейные регрессионные модели иногда могут иметь причинно-следственную интерпретацию. Давайте рассмотрим вот такой пример. Посмотрим, как связаны между собой уровень физической активности человека и уровень холестерина у него в крови. Перед нами 10 тысяч испытуемых, на графике по горизонтальной оси отложен уровень физической активности, по вертикальной — уровень холестерина. Мы видим, что эти два признака положительно коррелированы, похоже, поскольку облако вытянуто вдоль главной диагонали. Давайте проверим гипотезу о том, что по уровню физической активности можно предсказывать каким-то образом уровень холестерина. Построим вот такую линейную регрессионную модель с одним-единственным признаком и проверим гипотезу о том, что коэффициент β1 в этой модели равен нулю. Можно проверить ее против альтернативы, что β1 > 0. Критерий Стьюдента говорит, что гипотеза нулевая такая против этой альтернативы отвергается с достигаемым уровнем значимости очень-очень маленьким — 2 × 10 в −16-й. Даже если мы бы и взяли здесь двухстороннюю альтернативу, мы получили бы достигаемый уровень значимости 4 × в −16-й — это тоже очень-очень мало. Давайте теперь посмотрим, как эти же самые данные выглядят в разрезе возраста наших испытуемых. На этом графике размечены пять возрастных групп — от левого нижнего угла к верхнему правому располагаются группы 10–20 лет, 20–30, 30–40, 40–50 и 50–60. В каждой возрастной группе уровень холестерина и количество физических упражнений друг с другом связаны отрицательно, но при этом в каждой следующей группе оба признака, которые мы исследуем — и уровень холестерина, и уровень физической активности, — растут с возрастом. Давайте построим теперь линейную регрессионную модель с двумя признаками — будем предсказывать уровень холестерина по возрасту и количеству физических упражнений. Проверим нулевую гипотезу о том, что количество физических упражнений значимо, хорошо предсказывает уровень холестерина. Будем проверять гипотезу β1 = 0 против альтернативы β1 < 0. Критерий Стьюдента дает достигаемый уровень значимости точно такой же маленький — порядка 2 × 10 в −16-й, то есть нулевая гипотеза снова отвергается. У нас есть две модели, в первой из этих моделей мы видим, что физические упражнения положительно влияют на уровень холестерина, то есть чем больше вы упражняетесь, тем больше холестерина у вас в крови. Во второй модели мы видим ровно противоположное: если мы знаем ваш возраст, то чем больше вы упражняетесь, тем ниже уровень холестерина у вас в крови. Какой из этих двух выводов мы примем как финальный? В этой задаче можно включить здравый смысл и, исходя из здравого смысла, понять, что именно вторая модель верна, потому что кажется, что физические упражнения улучшают ваше здоровье, поэтому, наверное, уровень холестерина должен как-то снижаться. Но не во всех задачах такая опция доступна, попробуем не использовать здравый смысл. Можно рассуждать так: вторая модель более подробна, она содержит больше признаков, значит, наверное, она богаче, и, возможно, благодаря этому ее выводы более правильные. То есть модель, в которой больше признаков, лучше, наверное, чем модель, в которой признаков меньше. Пусть теперь у нас первый признак — он на этом графике отложен по горизонтальной оси — это средний балл выпускника из школы. А второй признак, отложенный по вертикальной оси — это результат выпускника на мотивационном тесте во время собеседования при поступлении в вуз. Эти два признака, если мы посмотрим на вот облако точек, которое мы видим на этом графике, кажется, что вообще никак не связаны друг с другом. Красные точки на этом графике — это школьники, которые в вуз поступили, а синие — это те, которые не поступили. Мы видим, что правила приема в вуз устроены как-то достаточно просто, то есть поступают ученики, у которых или высокий средний балл, или хорошие результаты на тесте по мотивации. Давайте подумаем, влияет ли на результаты теста по мотивации средний балл. По конфигурации облака точек кажется, что не влияет, но мы можем формально это проверить — вот построим такую простую регрессионную модель, будем как бы предсказывать результат теста на мотивацию по среднему баллу, проверим гипотезу о том, что коэффициент β1 = 0 против двухсторонней альтернативы. Критерий Стьюдента дает достигаемый уровень значимости 0,15, нулевую гипотезу мы отвергнуть не можем, то есть мы не можем утверждать, что средний балл влияет на результат теста по мотивации. Давайте теперь в эту регрессионную модель добавим еще один признак, этот признак будет «поступил ли человек в вуз?». В такой регрессионной модели снова проверим нулевую гипотезу о том, что коэффициент β1 = 0 против двухсторонней альтернативы. И на этот раз критерий Стьюдента уверенно эту нулевую гипотезу отвергает, то есть мы утверждаем, что вот в такой регрессионной модели результат теста на мотивацию значимо лучше предсказывается средним баллом студента, чем в отсутствие этого признака. Как это можно интерпретировать? У нас снова две регрессионные модели, и в первой мы получаем, что признак значимо на отклик не влияет, а во второй получается, что признак значимо влияет на отклик, причем в отрицательную сторону. Вот чем меньше средний балл — мы видим на регрессионных прямых, которые нарисовались в нашей модели, — чем меньше средний балл, тем выше результат теста на мотивацию. Чем отличаются эти две задачи, с холестерином и с поступающими? Дело в том, что признаки, которые в этих задачах используются, связаны друг с другом совершенно разными причинно-следственными конфигурациями. В первой задаче признак, который для нас побочный — возраст, влияет на оба признака, которые нас интересуют: и на отклик — уровень холестерина, и на признак — количество физических упражнений. Такая конфигурация называется вилкой. В задаче с поступающими конфигурация противоположная. Дело в том, что и признак, который нас интересует — средний балл, и отклик — мотивация, наоборот, влияют на третий побочный признак — факт поступления в вуз. Вот такая конфигурация со стрелочками, направленными в противоположную сторону, называется коллайдером. Оказывается, что для того чтобы коэффициент при признаке в регрессионной модели можно было интерпретировать с точки зрения причинно-следственной связи, нужно чтобы все остальные признаки в модели были предками икса, то есть влияли на x, и не были ни в коем случае потомками икса, которые также одновременно являются потомками игрека, то есть все побочные признаки в регрессионной модели не должны быть вершинами-коллайдерами. В линейной регрессионной модели β1 с крышкой — это оценка среднего эффекта, то есть среднего изменения y от увеличения x1 на единицу. Вот этой оценке можно в некоторых случаях давать причинно-следственную интерпретацию, то есть утверждать, что если мы проведем эксперимент, в котором зафиксируем все возможные факторы, которые могут на y влиять, и поменяем только один из них — x1, то именно так изменится y. Вот условие, при котором такую причинно-следственную интерпретацию давать можно, следующее: линейная регрессионная модель должна содержать все признаки, являющиеся причинами x1. Кроме того, она не должна содержать признаков, которые являются следствиями одновременно x1 и y. То есть в регрессионной модели должны быть все предки x в причинно-следственном графе и не должно быть ни одной вершины коллайдера по отношению к паре x и y. Итак, мы обсудили, что линейная регрессия иногда может позволять оценивать причинно-следственные связи, но на самом деле это можно делать только в некоторых достаточно строгих предположениях, то есть линейная модель должна быть подобрана правильно, она должна содержать признаки правильные и не содержать признаков неправильных. Вот обо всем этом необходимо обязательно отдельно думать, если вы хотите делать причинно-следственную интерпретацию для вашей модели. Плохо подобранные признаки могут привести к противоположным выводам, то есть если вы включаете бездумно коллайдеры или, наоборот, не учитываете признаки, которые являются предками x, то вы можете сделать выводы, противоположные настоящим выводам. Интересно, что на сегодняшний день существуют методы, которые по обсервационным данным позволяют восстанавливать структуру предполагаемых причинно-следственных связей между признаками в этих данных. Но к сожалению, эти методы достаточно сложные, и, кроме того, реализация в Python, которая существует для этих методов, она находится еще в какой-то альфа-версии, поэтому мы не можем ее обсуждать и изучать, эту тему. А далее вас ждет пример построения и интерпретации регрессионной модели — обязательно посмотрите его, там будут некоторые важные на практике нюансы, которые мы в теоретической части не обсуждали.

В этом видео мы оценим влияние внешней привлекательности на уровень заработной платы с помощью линейной регрессии. А заодно, по ходу, разберём некоторые полезные практические трюки. Давайте посмотрим, какие у нас есть данные. У нас есть 1260 опрошенных, и про каждого из них мы знаем, сколько в час они зарабатывают в долларах, какой у них опыт работы в годах, сколько лет они получали образование, насколько они привлекательны внешне в баллах от 1 до 5. Эти оценки выставляли независимые участники исследования. И кроме того, мы знаем ряд демографических показателей, таких как пол, семейное положение, состояние здоровья (хорошее либо плохое), членство в профсоюзе, цвет кожи, занятость в сфере обслуживания — все эти признаки в нашей выборке бинарные. Мы хотим понять, насколько внешняя привлекательность влияет на средний уровень заработной платы с учётом всех остальных факторов. Давайте загрузим данные, посмотрим, как они выглядят. Вот примерно так. Вот наш целевой признак — wage, именно его мы будем предсказывать с помощью всех остальных, в том числе и признака looks, который оценивает внешнюю привлекательность. Ну давайте начнём с того, что посмотрим на данные. Только четыре признака из всех, что у нас есть, количественные. Поэтому довольно легко построить графики. Используем матрицу диаграмм рассеяния. Здесь в каждой клеточке построен график, где каждая точка — это один испытуемый. И по одной из осей отложен один признак, по другой — другой. На диагонали у этой матрицы стоят гистограммы. Что мы здесь сразу видим? Ну что признак образования достаточно дискретный, то есть есть достаточно немного уровней, которые он может принимать. Это логично, поскольку это количество лет, которые опрошенный наш получал образование, ну оно не может варьироваться абсолютно произвольно. Видно, что распределение уровня заработных плат достаточно скошенное. Наверное, с этим надо будет что‐то сделать. Кроме того, на этом графике с уровнем привлекательности видно, что у нас довольно много испытуемых со средним уровнем привлекательности 3, и достаточно мало испытуемых с уровнем привлекательности 1 и 5, то есть привлекательности намного выше среднего и намного ниже среднего. По всем остальным признакам категориальным давайте оценим сбалансированность выборки. Просто посчитаем количество раз, которое каждый из уровней вот этих всех категориальных факторов встречается в выборке. Это делается для того, чтобы, например, убедиться, что в нашей выборке не три человека, которые состоят в профсоюзе, из 1260. Потому что, если это так, то вряд ли наша регрессия может на такой признак правильно и хорошо настроиться. И может быть, лучше его вообще удалить. Ну мы видим, что здесь это не так, здесь по каждому из признаков — членство в профсоюзе, здоровье, цвет кожи, пол, семейное положение, работа в сфере обслуживания, по каждой есть достаточно много объектов на каждом уровне фактора. Прежде чем строить регрессию, давайте немного предобработаем данные. Посмотрим ещё раз на распределение нашего целевого признака — уровня заработной платы в час. На левом графике сам уровень заработной платы, на правом — он же, но в логарифмической шкале. Что мы здесь видим важного? Во‐первых, вот на левой гистограмме видно, что есть достаточно немного наблюдений, далеко отстоящих от всей выборки, с большим значением уровня заработной платы. На самом деле, это всего лишь один человек. Один человек в нашей выборке получает 77,72 доллара в час, а остальные все — меньше 45. Вот этого одного человека лучше удалить, потому что регрессия на него перенастроится, и это наблюдение какое‐то достаточно нетипичное. Убираем его. Давайте теперь посмотрим на распределение оценок привлекательности ещё раз. Ну вот мы снова видим, что у нас здесь довольно мало людей с привлекательностью 1 и 5. Мы можем поступить так. Давайте создадим вместо вот этого числового признака looks два бинарных признака. Один из них, выше среднего, будет равен 1, если привлекательность у опрошенного выше среднего. Второй, ниже среднего, будет равен 1, если привлекательность опрошенного ниже среднего, то есть 3. Таким образом, у нас появляется два новых признака, именно влияние их на уровень заработной платы мы будем исследовать, а исходный признак looks, ну пожалуй, можно удалить. Вот так теперь выглядят данные, которые мы будем анализировать. Давайте начнем строить на них линейную регрессию. Давайте построим обычную линейную регрессионную модель по всем имеющимся у нас признакам и начнём её постепенно модифицировать, анализируя по ходу. Строить эту линейную регрессию мы будем с помощью функции OLS из пакета statsmodels. Мы будем использовать именно его, потому что эта функция позволяет автоматически рассчитывать большое количество разных диагностических метрик, которые полезны для понимания того, как именно устроена модель, которую вы построили. На вход эта функция принимает вот такую формулу: здесь слева стоит название признака, который в нашей регрессии является откликом. А справа через «+» перечислены все признаки, которые мы хотим использовать при прогнозировании нашего отклика. Хорошо. Давайте посмотрим на диагностику этой вот простой, обычной линейной модели. Что здесь печатается полезного? Во‐первых, мы видим R². Коэффициент детерминации этой модели не слишком высок — 0,26 всего лишь. Здесь есть статистика и достигаемый уровень значимости критерия Фишера при проверке гипотезы о том, что модель вообще имеет какой‐то смыл. То есть о том, что все коэффициенты при всех признаках модели, на самом деле, можно приравнять к 0 и оставить одну только константу. Вот эта гипотеза, как мы видим, по достигаемому уровню значимости отвергается достаточно уверенно. То есть как бы наша модель ни мало дисперсий исходного отклика описывала, она намного лучше, чем ничего. Ниже у нас таблица, в которой выписаны все коэффициенты получившейся регрессии. В первом столбце сами коэффициенты, во втором — их стандартные отклонения, в третьем — значения статистики t-критерия Стьюдента при проверке гипотезы о том, что этот коэффициент равен 0. В следующем столбце — достигаемый уровень значимости, соответствующий этим статистикам, ну и далее — доверительный интервал для коэффициентов. Чтобы понять, насколько наша модель хороша, давайте посмотрим на распределение её остатков. Слева – ку-ку график, а справа — гистограммы этого распределения. Что же мы здесь видим? Мы видим, что точки на ку-ку графике лежат довольно сильно не на прямой. Распределение остатков не очень‐то похоже на нормальное. Это проблема, потому что мы хотим использовать значения достигаемых уровней значимости критериев Стьюдента и Фишера, и на их основании как‐то модель модифицировать, отбирать значимые признаки. Если предположения о нормальности ошибки в модели не выполняются, то критерии Стьюдента и Фишера перестают работать. С этим можно что‐то сделать. Например, можно поступить так. Мы видели, что наш отклик тоже имеет какое‐то одностороннее распределение, то есть у него такой достаточно длинный правый хвост, и оно обрезано в нуле с левой стороны. Остатки точно так же имеют скошенное распределение. В таких случаях часто помогает логарифмирование, мы просто будем делать регрессию не исходного признака, а его логарифма. Ну давайте это попробуем. Повторим всё то же самое, только на этот раз будем строить регрессию не на сам признак, а на его натуральный логарифм. Давайте посмотрим на получившуюся модель. Во‐первых, у неё довольно сильно вырос R², коэффициент детерминации теперь 0,38. Мы видим, что распределение остатков, кажется, стало больше похоже на нормальное: на ку-ку графике точки лежат уже более‐менее на прямой. Всё ещё сохраняется небольшой левый хвост, который мы видим на распределении остатков на гистограмме, но он стал существенно меньше, чем в предыдущем примере. Давайте теперь посмотрим, как остатки меняются в зависимости от разных значений непрерывных признаков, которые стоят в регрессии. Непрерывных признаков у нас всего два: это опыт работы и образование. И вот по ним графики. На графике для опыта работы мы видим интересный эффект: кажется, что точки на этом графике лежат вот на такой дуге. Это означает, что в наших остатках есть какая‐то квадратичная зависимости от признака «опыт работы». Когда мы видим, что что‐то такое происходит, мы можем просто в нашу линейную регрессию подставить дополнительно какие‐то преобразования от наших исходных признаков. Например, в данном случае мы можем подставить квадрат «опыта работы». Давайте это сделаем. Добавляем. Единственное, что при этом меняется, это вот в нашей формуле для модели, которая строит линейную регрессию, добавляется ещё одно слагаемое — вот это. Посмотрим на результат. R² ещё увеличился. Теперь нам удаётся объяснить 40 % дисперсии исходного признака. Распределение остатков не слишком изменилось, но, по крайней мере, теперь в остатках больше нет зависимости от квадрата опыта работы: как мы видим вот по этом графикам, она полностью ушла. Прежде чем переходить к интерпретации достигаемых уровней значимости и критерия Стьюдента для всех признаков и делать, таким образом, отбор признаков, давайте убедимся, что выполняется предположение о гомоскедастичности, без которого критерий Стьюдента будет работать непредсказуемо плохо. Используем критерий Бройша – Пагана, вот эта функция возвращает его достигаемый уровень значимости, мы видим, что он очень маленький. То есть гипотеза о гомоскедастичности остатков отвергается. Что можно в этом случае делать? Оказывается, можно сделать на это специальную поправку, которая называется поправка Уайта. Она пересчитывает значения дисперсии коэффициентов регрессии с учетом того, что в модели присутствует гетероскедастичность. Казалось бы, почему не использовать этот метод всегда, если он устойчив в гетероскедастичности? Ну дело в том, что он просто менее точен, если на самом деле гетероскедастичности нет. Поэтому прежде чем его включать, лучше проверять, действительно ли в этом есть какая-то необходимость. Давайте сделаем эту поправку Уайта. Это очень просто, делается вот так. Когда вы делаете fit, в качестве параметра cov_type вы указываете HC1, например. HC — это гетероскедастичность, а 1 — это номер типа поправки Уайта. Их есть, если я не ошибаюсь, 4 или 5. Ну можно всегда использовать номер 1, например, потому что никто не знает, какая из них равномерно лучше. Давайте посмотрим, что от этого изменилось. На каком-то верхнем, самом грубом уровне регрессионная модель осталась практически такая же, потому что R² у нее не изменился, и достигаемый уровень значимости критерия Фишера все такой же. Это неудивительно, потому что от использования поправки Уайта на гетероскедастичность, меняются у нас только оценки дисперсий наших коэффициентов регрессии. И если мы сравним вот эту таблицу с предыдущей, мы увидим, что они действительно немного отличаются. Это значит, что наша модель в каком-то смысле устойчива к гетероскедастичности, или гетероскедастичность в данных была не слишком сильной, потому что когда мы на нее скорректировали, мы получили практически то же самое. Ну небольшие отличия все-таки есть, достигаемые уровни значимости немного поменялись, и теперь, когда мы уверены в том, что их можно правильно интерпретировать, потому что критерии, которые их считают, действительно работают, предположения для них выполняются, давайте сделаем на их основе отбор признаков. Что мы здесь видим? Какие признаки у нас незначимые? Это признаки: состояние здоровья, раса, семейное положение и признак привлекательность выше среднего. Первые три мы удалим, а самый последний оставим. Несмотря на то, что он в модели незначим, мы все равно его будем там держать, потому что мы хотим в самом конце интерпретировать коэффициент перед ним. Ну неважно в таких случаях, что признак незначим. Давайте построим следующую регрессионную модель, удалив незначимые признаки: цвет кожи, здоровье и семейное положение. Можно ожидать, что в целом качество нашей модели немного ухудшится, поскольку какие бы признаки бессмысленные ни были, когда мы их просто удаляем, у нас R², например, всегда понижается. Но видно, что здесь R² уменьшился совсем немножко, всего на 0,003, так что, видимо, эти признаки действительно мало что добавляли в нашу модель. Давайте посмотрим теперь на значимость признаков. Ну и мы видим, что достигаемые уровни значимости для всех из них, кроме опять же привлекательности выше среднего, очень маленькие. То есть признаки действительно, по всей видимости, хорошие, то есть Y они помогают предсказывать. Давайте теперь убедимся с помощью критерия Фишера, что удаление сразу трех незначимых по Стьюденту признаков не привело к значимому ухудшению модели. Критерий Фишера в данном случае вызывается вот такой функцией. Это compare_f_test, и здесь вы в качестве первой модели указываете модель бо́льшую, а в качестве второй, вот здесь, в скобках — модель, которая меньше и вложена в эту первую бо́льшую. Мы видим, что достигаемый уровень значимости здесь 0,18, то есть мы не можем отвергнуть гипотезу о том, что признаки, которые мы выбросили, были действительно не нужны. Давайте сделаем еще одну диагностику: посчитаем влиятельность всех наблюдений в нашей выборке. Дело в том, что регрессия склонна перенастраиваться на нетипичные наблюдения. Если у нас есть какие-то точки, которые лежат далеко от основного облака, из-за того, что в регрессии минимизируются квадраты отклонений, если мы проведем регрессионную прямую через основное облако, вот эта далекая точка даст огромный вклад в минимизируемую нами ошибку, в среднеквадратичное отклонение. Поэтому регрессионная прямая будет стремиться пройти как можно ближе вот к таким далеко лежащим точкам. В результате, на большей части точек мы получим плохое качество предсказаний. Для того чтобы убедиться, что таких точек в нашей выборке нет, построим следующий график. На этом графике по горизонтальной оси отложены квадраты остатков, то есть квадрат ошибки предсказания на каждой точке, а по вертикальной оси отложена влиятельность наблюдений. Вот чем она больше, тем сильнее это наблюдение влияет на регрессионное уравнение. Как эта штука вычисляется? Очень просто. Мы просто берем нашу исходную выборку и по очереди из нее удаляем одну точку, одно наблюдение. Перестраиваем регрессионное уравнение по вот этой вот выборке без одной точки и смотрим, насколько оно похоже на регрессионное уравнение, построенное по полной выборке. Вот разница между этими двумя уравнениями как раз и определяет влиятельность точки. Что мы видим на этом графике? Мы видим, что у нас нет точек, которые лежат как-то экстремально далеко от основного облака по влиятельности. Ну вот точка с самой большой влиятельностью — это точка 1122. Точка, на который мы сильнее всего ошибаемся — это точка 269. Давайте на них посмотрим и подумаем, действительно ли с ними что-то не так. Первая точка с самой большой влиятельностью соответствует темнокожей замужней женщине с небольшим уровнем образования и привлекательностью выше среднего и очень большим опытом работы. Вот, по всей видимости, влиятельность этого наблюдения определяется как раз нетипичностью значения опыта работы на этой точке. Точка, на которой мы сильнее всего ошибаемся — это женатый белый мужчина, достаточно образованный, с большим опытом работы (но не слишком большим), привлекательностью выше среднего и практически максимальной в нашей выборке заработной платой. По все видимости, для него мы предсказываем заработную плату недостаточно высокой, то есть мы недопредсказываем в этом случае. Но тем не менее, поскольку на графике мы видели, что все точки лежат более-менее плотно, нет каких-то точек экстремально влиятельных, мы можем сказать, что наша модель уже достаточно хороша и на этом остановиться. Давайте теперь сделаем по ней выводы. Наша итоговая модель объясняет 40 % вариаций логарифма нашего отклика. На следующих двух графиках: на первом отложен отклик, то есть заработная плата, а по вертикали — экспонента от наших предсказаний, на втором графике отложен логарифм заработной платы и наши предсказания этого логарифма. Ну мы видим, что на самом деле это облако точек не слишком диагонально. То есть действительно наша модель неидеальна, она объясняет всего лишь 40 % вариаций нашего отклика. Но по всей видимости, эта задача — достаточно сложная. Предсказать уровень заработной платы точно по таким признакам, которые у нас есть, сложно. Поэтому 40 % — это результат достаточно неплохой. На самом деле, нас интересует не вся модель, а только коэффициенты при привлекательности выше среднего и ниже среднего. Давайте на них посмотрим и проанализируем их. При привлекательности ниже среднего стоит коэффициент −0,13. Поскольку это коэффициент в регрессии на логарифм отклика, его можно интерпретировать как прирост в процентах. То есть когда наш признак – привлекательность ниже среднего – меняется с 0 на 1, наш отклик уменьшается на 13 %. Таким образом, выводы, которые можно сделать по построенной регрессионной модели, выглядят следующим образом. Представители генеральной совокупности, из которой взята наша выборка, с учетом рассмотренных нами дополнительных признаков, получают в среднем на 13 % меньше, если их привлекательность ниже среднего. Достигаемый уровень значимости — 0,001. 95% доверительный интервал для вот этого прироста — от 5 до 21 %. Этот доверительный интервал можно взять из диагностики последней построенной нами модели выше. А вот люди с привлекательностью выше среднего получают столько же, сколько и люди со среднем уровнем привлекательности. Достигаемый уровень значимости при проверке значимости этого признака для предсказания нашего отклика очень большой — 0,97. 95% доверительный интервал для изменения среднего уровня заработной платы при повышении привлекательности до выше среднего — от −6 до 6 %. Он очень большой и содержит 0, то есть этот признак полностью неинформативен. В этом видео мы построили регрессионную модель и, интерпретируя ее, проанализировали взаимосвязь между двумя интересующими нас признаками. Модель, которую мы строили, мы постепенно улучшали. Но интересно, что шаги, которые мы делали, чтобы ее улучшить: отбор признаков, использование гетероскедастичной поправки Уайта — все эти шаги не слишком сильно повлияли на наши итоговые выводы. Коэффициент в регрессии при признаке, который нас больше всего интересовал, практически не изменился. Это хорошо. Если модель не слишком сильно меняется при добавлении/ удалении признаков, то значит, что она достаточно устойчива и, возможно, действительно как-то правильно описывает механизм, лежащий в основе явления, которое мы с ее помощью анализируем.

