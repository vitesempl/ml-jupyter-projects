Независимо от того, в какой сфере бизнеса вы работаете, вам постоянно хочется улучшать ключевые показатели вашего бизнеса. Для этого часто появляются разнообразные идеи, касающиеся того, как же эти показатели улучшить. Но откуда могут браться такие идеи? С одной стороны, они могут браться из анализа рынка, анализа поведения пользователей и анализа их потребностей. С другой стороны, идеи могут возникать, исходя из того, как устроен ваш бизнес, и как вы видите его развитие в будущем. Возникает логичный вопрос: как же такие идеи проверять, ведь их много, и хочется применять только самые успешные из них? >> Как бы вам ни нравилась ваша идея, в любом случае ее нужно проверять, потому что всегда может оказаться, что вы не учли какую-нибудь мелкую деталь, и всё пошло не так, как вы задумывали. Для проверки таких идей существует много разных методов. Для начала все идеи должны проходить проверку на какой-то элементарный здравый смысл: вы рассказываете вашу идею коллегам, которые тоже понимают в этом что-то, и вы обсуждаете с ними и вместе приходите к решению, стоит ли этой идеей дальше заниматься или можно отказаться от неё уже сейчас. >> Наряду с обсуждением вашей бизнес идеи с коллегами, полезно бывает протестировать эту идею на реальных пользователях. Такие проверки обычно стоят дороже, потому что вам требуется привлекать пользователей, которые тратят на проверку свое время, однако они позволяют сделать более релевантную оценку, потому что вы тестируете идею на тех людях, которые потом ею будут пользоваться. К таким проверкам существует целый ряд требований. С одной стороны, хочется, чтобы они были достоверными, то есть условия, в которых вы проверяете идею, были максимально приближены к реальным. С другой стороны, нам не хочется тратить на такие проверки очень много времени и очень много денег. А в качестве такой проверки может быть опрос пользователей, когда вы заранее подготавливаете какие-то вопросы и дальше просите людей на них ответить. С другой стороны, мы можем использовать такой механизм, как «фокус группы». >> Если вы устраиваете фокус группу, вы собираете какую-то небольшую группу пользователей и очень подробно расспрашиваете у них все детали, всё, что они чувствуют по отношению к вашему новому баннеру, как именно они на него кликают, и вы смотрите, как долго их взгляд задерживается на каких-то деталях вашего сайта. То есть вы узнаете очень много, но, поскольку это дорого, вы не можете так сделать для большого количества людей. По сравнению с опросами, фокус группы глубже, но их можно сделать только на небольшой выборке. >> Казалось бы, фокус группы — почти идеальный механизм проверки ваших бизнес идей. Однако, есть некоторый нюанс в том, что не всегда удается с точностью воссоздать те самые условия, в которых пользователи будут использовать ваш продукт. Например, есть известный случай с тестированием количества сахара в напитке Coca-cola. В рамках исследования количества сахара, содержащегося в «Кока-Коле», была собрана фокус группа, на которой людям предложили на выбор два напитка: со стандартным содержанием сахара и с увеличенным содержанием сахара. Людям просто нужно было попробовать оба напитка и выбрать тот, который им больше понравился. В результате этого исследования выяснилось, что большее количество людей предпочитает Кока-Колу с увеличенным количеством сахара. Ну, в результате увеличили количество сахара, этот напиток предложили более широкой аудитории, но неожиданно продажи упали. После этого эксперты стали разбираться, ну как же так произошло, ведь в рамках фокус группы было принято решение о том, что больше сахара — это вкуснее, но выяснилось, что это исследование проходило не в тех самых условиях, в которых люди употребляют «Кока-Колу» в жизни. Выяснилось, что, если речь идёт всего лишь об одном стакане, то, действительно, более сладкий напиток интереснее. Однако если сладкий напиток употребляется постоянно в больших количествах, то больше сахара — хуже. >> Просто сложнее выпить больше вот этого очень сладкого напитка. >> Кажется, что это логично. Поэтому, когда проводятся фокус группы, всегда нужно следить за тем, чтобы условия были максимально приближены к настоящим. Однако это не всегда возможно. >> Полностью проблема приближения условий к настоящим решается в рамках другого способа оценки ваших идей — это «A/B testing». Вот именно об этом мы и будем говорить в этом уроке. «A/B testing» — это способ проверки ваших идей непосредственно в боевых условиях. То есть вы предлагаете вашим пользователям какую-то новую функциональность или новый элемент дизайна и именно в тех условиях, в которых они взаимодействуют с вашим продуктом. >> Очевидный плюс A/B теста в том, что, с одной стороны, условия больше похожи на настоящие — это фактически те же самые условия, в которых ваш продукт будет существовать, С другой стороны, вы максимально снижаете ваши финансовые риски, ведь это эксперимент — мы никогда не знаем, получится он или нет, понравится ли новый продукт пользователю. При этом A/B testing проводится на небольшой группе пользователей, поэтому вы можете заранее спрогнозировать максимальные потери. >> В следующем видео мы обсудим, в каких сферах вообще технология A/B testing может применяться.

Давайте поговорим о том, где A/B-тесты могут применяться. >> Ну, если говорить об области IT, то A/B-тесты используются практически повсеместно. Любые сайты, независимо от их профиля, используют механизм A/B-тестирования для принятия решения о своих изменениях. А что насчет других сфер? >> Ну, вообще, я думаю, это в IT справедливо и для приложений, игр и, вообще, более-менее для всего, что взаимодействует с конечным пользователем. И во всех этих сферах нужно тестировать такие вещи, как, например, какие-то изменения в дизайне или изменения функциональности, или, может быть, какие-то новые возможности для пользователя, которые появляются. >> Да, всё действительно так, но есть нюанс: мы можем тестировать изменения разных типов. Это могут быть как изменения дизайна, например, внешнего вида вашего сайта, с другой стороны, это могут быть изменения алгоритмов. Такие изменения пользователю не нужны, но они отражаются на том, как функционирует ваш сайт. С другой стороны, это могут быть изменения типа введения новых функций, тех функций, которых раньше не было. Здесь есть проблема в том, что, с одной стороны, хочется как можно меньше времени тратить на эксперименты и проверить множество идей сразу, с другой стороны, изменения разных типов плохо тестируются одновременно. >> Ну да, может оказаться, что разные изменения просто технически несовместимы друг с другом. Или, например, одно из изменений действует на ваш бизнес-показатель положительно, а другое — отрицательно, и когда вы применяете их вместе, вы не можете разделить эти два эффекта. Вообще, интересно, мне кажется, что такие техники применяются далеко не только в IT, но и в некоторых довольно неожиданных местах, например, для оптимизации работы государственных органов. При правительстве США и при правительстве Великобритании есть небольшая группа, которая с использованием психологов-бихевиористов порождает какие-то гипотезы о том, как люди взаимодействуют с государством, и на основании вот таких гипотез проводятся небольшие эксперименты. Например, какие-то небольшие изменения в дизайне налоговой формы или изменения в способе записи на донорство, трансплантацию органов. Вот какие-то такие вещи небольшие, которые легко и очень дешево можно поменять, оказывается, приводят к тому, что государство может сэкономить себе миллионы и миллионы, и совершенно бесплатно. Удивительные вещи. Действительно, звучит очень интересно. Если говорить о механизме A/B-тестирования в целом, то можно сказать, что весь процесс распадается на две большие части. Первая часть — это планирование эксперимента, планирование того, как именно будет выглядеть A/B-тест, как вы будете делить пользователей на группы, сколько будет длиться A/B-тест и многие другие вопросы, связанные с этим. И вторая часть — это непосредственно проверка гипотез, принятие решения о том, положительно или отрицательно влияют ваши изменения на ваш бизнес в целом. В следующих видео мы с вами разберем эти части подробнее.

Итак, у вас есть идея. Вы хотите показать, что она хорошая. Для этого вам нужно провести эксперимент, в котором вы эту идею применяете, и что-то у вас должно значимо улучшиться. Вот то, что именно должно у вас значимо улучшиться, надо выбрать, перед тем как эксперимент проводить. И оказывается, что эта задача достаточно нетривиальная. >> Здесь речь идет о выборе метрик. Часто для того чтобы показать, что состояние вашего бизнеса улучшилось, а именно его ключевые показатели изменились в ту сторону, в которую вы ожидаете, вам нужно выбрать некоторые метрики, связанные напрямую с состоянием вашего бизнеса. Но чаще всего это бывают метрики, связанные непосредственно с деньгами, часто это бывают аудиторные метрики, и с ними есть некоторая сложность с тем, что не всегда их возможно сразу корректно рассчитать. >> Часто такие метрики, во-первых, сложно померить, во-вторых, они бывают достаточно грубыми и не реагируют практически на ваши небольшие изменения в функциональности или дизайне. Кроме того, во многих случаях метрики, которые вас интересуют, нужно просто очень долго измерять. Например, если вы хотите, внеся изменения в ваш сайт с арендой квартир, каким-то образом увеличить количество людей, которые порекомендуют ваш сервис своим знакомым, вам может понадобиться, допустим, год на то, чтобы измерить этот показатель, потому что люди не так уж часто переезжают, и знакомые у людей тоже переезжают не так уж часто. >> Это важное замечание приводит нас к идее использования так называемых прокси, или промежуточных, метрик. Это такие метрики, которые, с одной стороны, достаточно чувствительные, чтобы измерять их в рамках A/B-тестинга или в рамках эксперимента, а с другой стороны, они хорошо согласуются с теми бизнес-показателями, которые вы хотите в реальности измерять. Например, в нашем примере с арендой квартир в качестве такой метрики может быть использована метрика «среднее количество визитов на сайт в день», «среднее количество уникальных пользователей» или «количество шеров вашего сайта в социальных сетях». Эти метрики достаточно чувствительны, чтобы измерить их в рамках A/B-теста, но с другой стороны, они согласованы с той метрикой, которую мы хотим использовать в реальности. >> То есть у нас есть бизнес-показатели, которые нас больше всего волнуют, есть метрики, которые мы используют вместо них, потому что мы не можем быстро померить деньги. Еще какие-нибудь варианты? >> Да, конечно. Важным этапом при принятии решения о том, следует ли запускать ваше изменение в A/B-тестинге, является оффлайн-тестирование. В рамках оффлайн-тестирования вы можете проверить, как ваши изменения сказываются на поведении пользователей по историческим данным. Давайте рассмотрим следующий пример. Допустим, мы изменили алгоритм ранжирования результатов по запросам пользователей при поиске квартир на нашем сайте недвижимости. В этом случае мы можем поступить следующим образом: мы можем проанализировать запросы пользователей за прошлое, если мы с вами знаем, о чем пользователи спрашивали нас на сайте ранее, и эмулировать новую выдачу новым алгоритмом. Таким образом, с одной стороны, мы будем иметь информацию о том, что пользователи искали и на какие позиции они кликали, какие ответы показались им релевантными, с другой стороны, у нас будет новое ранжирование, то есть новые результаты поиска. Если мы с вами совместим эти данные и измерим, на какие позиции теперь приходятся клики пользователей, мы можем сравнить, например, такие метрики, как «средняя позиция клика». Скажем, если эта метрика возрастает, то, наверное, наше изменение хорошее. Значит, имеет смысл протестировать его в онлайне. А если, например, наш новый алгоритм ранжирования совсем не находит те результаты, которые пользователи посчитали релевантными, то, возможно, есть некоторая проблема. >> Получается, у нас возникает такая иерархия метрик: есть предварительные метрики, которые мы мерим до начала эксперимента, есть непосредственно экспериментальные, на которые мы смотрим и принимаем решение о том, хорошее изменение или плохое. А есть бизнес-метрики, которые вот там где-то в самом конце, и мы надеемся, что наши изменения в конце концов повлияют именно на них. >> Таким образом, мы с вами более-менее разобрались в том, на каких уровнях можно мерить метрики: оффлайн-метрики, онлайн-метрики в рамках A/B-тестинга и ключевые бизнес-показатели. А в следующем видео мы с вами рассмотрим то, как нужно строить наш эксперимент или поговорим о дизайне эксперимента.

[БЕЗ ЗВУКА] Итак, мы определились с тем, что мы хотим измерять. Давайте теперь выберем, как именно мы будем это измерять. Нам нужна какая-то небольшая группа пользователей, которой мы предъявим наши новые изменения. Для того чтобы результаты, полученные на этой небольшой группе можно было обобщать на наших пользователей в целом, эта небольшая группа должна быть репрезентативной, то есть ее структура, ее... люди, которые ее составляют, должны быть в той же пропорции в нашей группе, что и в группе всех пользователей вообще. Например, если мы знаем, что 2/3 пользователей нашего продукта — женщины, то в нашей экспериментальной группе должны ровно быть 2/3 женщин. Таким образом при построении экспериментальной группы мы можем выделять какие-то важные свойства пользователей: по возрасту, еще какие-то вещи, которые нас интересуют, и искусственно делать так, чтобы в экспериментальной группе были ровно такие же доли по разным подгруппам, как и в группе пользователей в целом. Такой подход называется стратификацией. Другой подход, противоположный ему в каком-то смысле, это подход рандомизации. Если вы набираете в вашу экспериментальную группу пользователей абсолютно случайно, то в среднем она получается такого же состава, как и вся генеральная совокупность пользователей. Дополнительный плюс рандомизации заключается в том, что при рандомизации ваша экспериментальная группа пользователей выравнивается со всей генеральной совокупностью не просто по тем показателям, которые вам показались важными заранее — вы их выбрали, а по всем возможным показателям вообще. Это преимущество рандомизации. В некоторых случаях оказывается важным померять как на пользователя влияет несколько воздействий сразу. То есть у вас по каждому пользователю должны быть данные о том, как он реагирует например на сайт без изменений и на сайт с изменениями. Такой дизайн называется парным или связанным. Выборки, которые вы получаете, они не зависимые, а связаные, и это очень выгодно в ситуациях, когда показатель, который вы измеряете, имеет большую индивидуальную дисперсию, то есть если пользователи у вас очень сильно отличаются по измеряемому вами показателю, то выгодно измерять на каждом пользователе значение показателя и в контрольной группе, и в экспериментальной. При связанном дизайне зачастую оказывается важным, в каком порядке пользователю предъявляются разные варианты. Для того чтобы снять влияние вот этого порядка, можно использовать дизайн крест-накрест — когда вы половине пользователей показываете сначала новый вариант, а потом старый, а другой половине — сначала старый, а потом новый. Мы уже говорили о том, что экспериментов можно проводить одновременно большое количество. Но все хорошо в этой ситуации, только если каждый пользователь участвует только в одном эксперименте. Если существует вероятность, что каждый пользователь попадает сразу в несколько экспериментальных групп, нужно внимательно следить за тем, чтобы эти эксперименты друг другу не противоречили. Например, очень известная история о том, как Google однажды тестировал 41 оттенок синего в цвете ссылок в своей поисковой выдаче. Если допустить, что одновременно еще проводился бы эксперимент о том, как выбрать цвет страницы или цвет фона, на фоне которого показываются эти ссылки, очевидно, что они не должны быть тех же самых цветов, что и текст ссылок, иначе просто пользователь не сможет ничего прочитать. То есть пример подбора одновременно цвета текста и цвета фона, на котором он показывается, это пример эксперимента, в который друг с другом как раз не сочетаются. Итак, при дизайне экспериментов вы можете пользователей либо стратифицировать, либо рандомизировать. Вы можете делать группы, в которых люди видят разные экспериментальные условия независимыми и связанными. Кроме того, если они связаны, вы можете думать про то, в каком порядке они должны демонстрироваться. Вот это важные вещи, о которых нужно думать при дизайне ABT-эксперимента.

Одно из важнейших требований к A/B тестингу, которое обязательно должно быть заложено в визуальные эксперименты — это требование устойчивости. В данном случае под устойчивостью мы понимаем два простых пожелания. Во-первых, мы, конечно, не хотим видеть значимых изменений там, где их на самом деле нет. С другой стороны, обратное тоже верно: если какие-то значимые изменения есть, то мы хотим их увидеть на метриках. Это очень просто понять на примере. Предположим, мы выкатываем в эксперимент две одинаковые версии сервиса. Ну, например, у нас нет никаких изменений. В данном случае, конечно же, нам на всех метриках хочется видеть одинаковый результат, нам хочется быть уверенными, что если изменений нет, то мы их не увидим. С другой стороны, если есть некоторые значимые изменения, то конечно, мы хотим увидеть это на метриках. Мы хотим убедиться, что по всем метрикам есть некоторый значимый прирост. Казалось бы, это очень простые и логичные требования, которые должны всегда выполняться. Но часто на практике получается, что это не так. Давайте рассмотрим следующий пример. Допустим, у нас есть некоторый сайт, который позволяет делать поиск по некоторому специфичному контенту. Ну, например, мы ищем по объявлениям о недвижимости. В данном случае мы можем изменить дизайн и проверить то, как новый дизайн влияет на поведение пользователей. Правда ли, что он им нравится больше, они начинают больше искать и более активно пользоваться сервисом. Мы можем сделать некоторое очень простое изменение, ну, например, перекрасить кнопку поиска из синего цвета в зеленый. В данном случае легко понять, как будет выглядеть наш A/B тестинг. Мы разобьем пользователей на тестовую и контрольную группу. Одной группе пользователей будем показывать кнопку синего цвета (старый дизайн), а другой группе пользователей будем показывать кнопку зеленого цвета, это наш новый дизайн. Дальше мы с вами посчитаем некоторые метрики, которые можно считать в онлайне в данном случае (количество нажатий на эту кнопку), и посмотрим, как же они изменились. Часто в таких экспериментах можно наблюдать следующий эффект. В контрольной группе, то есть в группе, где мы изменили дизайн, количество кликов будет больше. Но что же это означает? Действительно ли дизайн стал лучше? В данном случае возможна двоякая трактовка. Те пользователи, которые привыкли пользоваться сайтом и уже привыкли к тому, что кнопка имеет синий цвет, могут удивиться тому, что что-то изменилось и захотеть проверить, изменился ли только дизайн или, может быть, как-то изменилось поведение. Соответственно, они могут начать чаще нажимать на кнопку просто из любопытства, чтобы понять, что же изменилось. В данном случае важно убедиться, что те метрики, которые мы считаем, не учитывают это изменение как значимое. Для того чтобы обезопасить себя от ситуации, в которой мы принимаем незначимые изменения за значимые, мы можем поступить следующим образом. Предположим, мы провели классический A/B тестинг и увидели, что новый дизайн лучше, то есть кнопка зеленого цвета больше нравится пользователям. По тем метрикам, которые мы меряем, например, доля кликов, или длина сессий, или, может быть, что-то другое, мы видим значимые улучшения. Тогда по логике мы должны поступить следующим образом: мы должны выбрать новый дизайн и раскатить его на всех пользователей, то есть показывать всем пользователям кнопку зеленого цвета. Однако мы можем поступить несколько хитрее. Давайте выберем небольшую группу пользователей, ну, например, меньше 1 %, и будем продолжать показывать им старый дизайн после выкатки нового. То есть у нас будет отдельно существовать некоторая группа пользователей, которая еще какое-то существенное время будет видеть старый дизайн. Это позволит нам сделать следующее: мы сможем продолжить рассчитывать те же самые метрики, которые мы считали в процессе A/B тестинга, более длительное время. По прошествии более длительного времени мы с вами сможем снова сравнить поведение пользователей, которые видят новый дизайн, с поведением пользователей, которые видят старый дизайн. В данном случае, если мы не увидим значимых изменений, то мы сможем сделать вывод о том, что во-первых, нам дизайн эксперимента не позволяет нам отличать незначимые изменения от значимых, потому что иначе мы бы увидели это в результате первоначального A/B тестинга, а с другой стороны, мы поймем, что на самом деле мы можем оставить любой дизайн, который нам больше нравится. Пользователи их не отличают. Вот такая техника называется обратным экспериментом. Ее идея заключается в том, что после классического A/B тестинга мы с вами продолжаем проводить эксперимент, однако мы выделяем некоторую маленькую группу пользователей, которые продолжают видеть старое решение. Это дает нам возможность убедиться в том, что наши изменения действительно значимы и приводят к тому эффекту, который мы ожидаем. Казалось бы, технология обратного эксперимента способна решить все наши проблемы и помочь нам очевидным образом отличать значимые изменения от незначимых, но представьте себе следующую ситуацию. Мы часто проводим A/B тестинг, видим значимые изменения на целевой и контрольной группе, запускаем эти изменения в технологию «обратный эксперимент» и понимаем, что на самом деле изменений нет. Конечно же, эта ситуация является для нас крайне нежелательной, потому что мы тратим довольно много времени на проведение A/B тестинга и далее на проведение обратного эксперимента. Возникает вопрос: можем ли мы заранее убедиться в том, что наш дизайн эксперимента позволяет нам отличать значимые изменения от незначимых? В частности, убедиться в том, что размер наших контрольных и целевых групп позволяет нам это делать, а также в том, что длительность эксперимента тоже подобрана правильно. Для того чтобы эту задачу решить, существует технология A/A тестинга, или А/А тестов. Это работает следующим образом. Предположим, что мы приняли решение о том, что мы хотим провести A/B тестинг с новым алгоритмом, ну, например, алгоритмом поиска или алгоритмом рекомендации. Как в этом случае выглядел бы классический A/B тестинг? Мы бы разделили пользователей на контрольную и тестовую группу, и показывали бы разные алгоритмы в разных группах, после чего сравнили бы те метрики, которые нас интересуют, рассчитанные на разных группах. Давайте перед тем как запускать классический A/B тестинг, поделим пользователей на группы точно так же, как мы бы это сделали в рамках A/B тестинга, но только будем показывать разным группам один и тот же алгоритм, то есть не будем в тестовой группе показывать наше новое решение. Какое-то время будем проводить этот эксперимент, на самом деле ровно такое время, которое мы бы хотели делать A/B тестинг, и дальше посмотрим, видим ли мы значимые изменения на тех метриках, которые нас интересуют. В том случае если мы значимые изменения не видим, то это хорошая ситуация, потому что кажется, что наш дизайн эксперимента не пытается показать нам значимые изменения там, где их нет. С другой стороны, в противоположной ситуации, если мы вдруг увидим значимые изменения при том, что мы с вами знаем, что обе группы видят одно и то же решение, то это наводит нас на мысль о том, что что-то в нашем дизайне эксперимента не так. Например, эксперимент длится недостаточно долго или мы неправильно разбили пользователей на группы. В любом случае, это повод задуматься о дизайне вашего эксперимента и каким-то образом его провалидировать. Таким образом, мы с вами обсудили две методологии, которые позволяют вам убедиться в устойчивости вашего дизайна эксперимента: это обратный эксперимент и А/А тест. А в следующем видео мы поговорим о том, как же нам оценить необходимый объем выборки и длительность эксперимента.

[БЕЗ_ЗВУКА] Когда мы выбрали метрику, дизайн эксперимента, метод его анализа и убедились, что вся экспериментальная инфраструктура устойчива в достаточной степени, дальше мы фактически готовы к запуску эксперимента. Единственный вопрос, на которой нам остается ответить, это сколько эксперимент должен идти; сколько пользователей должно быть в нашей тестовой выборке, для того чтобы мы с уверенностью могли ответить на интересующие нас вопросы. Задача определения необходимого нам объема выборки тесно связана с тем, какой именно статистический инструмент мы будем для ее анализа использовать. Для каждого конкретного критерия вот этот подбор необходимого объема выборки делается каким-то своим способом, по какой-то своей формуле. Для того чтобы понять, какой объем выборки нам нужен, нам нужно зафиксировать некоторые вещи. Во-первых, минимальный размер эффекта, который мы хотим померить. То есть для нашей метрики, насколько большие отклонения от показателя, который мы предполагаем, сохранится по умолчанию, если наши изменения вообще никак не влияют на пользователей, какой размер вот этого отклонения мы хотим замечать в эксперименте. Эта штука называется «размер эффекта». Далее, следующий показатель, который надо зафиксировать, — это допустимые вероятности ошибок первого и второго рода. В A/B-тестах, как правило, мы проверяем гипотезы о том, что никакие наши примененные изменения не повлияли на пользователей вообще никак, и проверяем ее против альтернативы, что как-то повлияли. Ошибкой первого рода в этой ситуации будет отвержение неверной нулевой гипотезы, то есть принятие не влияющих на самом деле на пользователей изменений. Ошибка второго рода — это, наоборот, отклонение действительно хороших и влияющих на пользователей изменений. Вот мы должны, для того чтобы рассчитать необходимый объем выборки, зафиксировать допустимые вероятности ошибок первого и второго рода. В статистике, как правило, используется вероятность ошибки первого рода — 0,05, а вероятность ошибка второго рода — 0,2. В вашем конкретном эксперименте стоимости ошибок первого и второго рода могут быть какими-то существенно разными, поэтому часто может оказаться выгодно вручную выбрать эти пороги на вероятности ошибок первого и второго рода. Наконец, когда вы зафиксировали размер эффекта и допустимой вероятности ошибок, вы можете поступить следующим образом: вы берете название метода, который вы планируете использовать для сравнения ваших контрольных групп и экспериментальных групп, например Z-критерий или T-критерий, и вы используете калькулятор мощности этого критерия. Вообще, для всех статистических критериев между собой связаны сложными взаимосвязями несколько величин: тип альтернативы, размер эффекта, размер выборки и допустимые вероятности ошибок первого и второго рода. Если вы какие-то из этих величин фиксируете, вы можете рассчитать оставшиеся. То есть если вы фиксируете конкретный критерий и фиксируете конкретный тип альтернативы, вероятности ошибок первого и второго рода и минимальный интересующий вас размер эффекта, вы можете вычислить объем выборки, который для этого нужен. Для того чтобы это сделать, нужно использовать калькулятор мощности. Вы просто гуглите его, и для каждого конкретного критерия вы легко найдете десятки различных реализаций, в том числе не требующих никакого знания программирования. На этом мы заканчиваем говорить про дизайн экспериментов для A/B-тестинга. Следующие несколько уроков посвящены методам его анализа. Интересно, что про дизайн мы говорим меньше, а про методы анализа больше, несмотря на то, что в реальной жизни дизайн, как правило, занимает больше времени и он более важен, чем методы анализа. Проблема однако в том, что методы анализа сложнее, и важнее их правильно понимать. Поэтому мы уделим им больше времени.