В этом видео мы узнаем, что такое матричное разложение. Идея матричных разложений очень проста. Иногда бывает удобно представить матрицу как произведение каких-то других матриц, обладающих некоторыми интересными свойствами. Первый пример — спектральное разложение матрицы. Если матрица X симметрична, то ее можно представить в виде произведения S (транспонированное) * D * S, где матрица D диагональная, а матрица S ортогональная. Все матрицы имеют тот же размер, что и матрица X. Элементы диагональной матрицы неотрицательные и являются собственными числами матрицы X. Зачем такое может быть нужно? Ну, например, часто встречаются функции вида f(y) = y (транспонированное) * X * y, где y — некоторый вектор. Такие выражения называются квадратичными формами, и вы можете их встретить, например, при анализе многомерного нормального распределения или при анализе матрицы ковариации. С этими объектами вы познакомитесь ближе в уроках про теорию вероятности и статистику. Оказывается, что если мы воспользуемся спектральным разложением матрицы, то мы сможем провести некоторую очень простую замену переменной, введя вместо переменной y переменную z = S * y, и теперь квадратичная форма будет иметь намного более простой вид. Это будет просто взвешенная сумма квадратов координат новой переменной. Оказывается, что в таком виде анализировать намного проще. Следующий пример — это сингулярное разложение матрицы. В этом случае мы представляем матрицу X как произведение трех матриц: U * D * V. Матрицы U и V — ортогональные, матрица D — диагональная. Но теперь мы не требуем от матрицы X симметричности. Геометрический смысл здесь довольно прост. Исходная матрица задает некоторое линейное преобразование, и матрицы, которые используются в произведении, тоже задают некоторые преобразования. И мы просто вместо того чтобы рассматривать какое-то одно сложное преобразование, представляем его как сначала некоторое ортогональное преобразование, ну то есть какой-нибудь поворот, инверсии, а затем растяжение вдоль осей с помощью диагональной матрицы и затем снова поворот. Ну то есть повернули, растянули, снова повернули. Применений у сингулярного разложения масса. На самом деле очень часто, выводя какие-то выражения для разных методов машинного обучения, мы будем сталкиваться с тем, что подставить сингулярное разложение — это очень хорошая идея, которая приводит к каким-нибудь способам избежать разные сложные ситуации. Одно из самых колоритных применений сингулярного разложения — в рекомендательных системах. Если у нас есть матрица, в которой записаны числа, которые являются просто оценками фильмов или каких-то других объектов пользователями, то, применив сингулярное разложение к этой матрице, мы можем построить достаточно неплохие простенькие рекомендации. Но об этом мы узнаем подробнее в следующих видео. Пока подведем итог. Порой бывает удобно представить матрицу как произведение нескольких других матриц, которые обладают некоторыми хорошими свойствами. Такое представление называется матричным разложением. Мы рассмотрели два простых примера: спектральное разложение и сингулярное разложение. Сингулярное разложение особенно понадобится нам в дальнейшем, в частности в рекомендациях. А в следующем видео мы познакомимся с еще одной интерпретацией понятия «матричные разложения», которое будет еще более приближать нас к применению матричных разложений в рекомендациях.

В этом видео мы увидим матричные разложения немного с другого ракурса. А начнем мы говорить про приближение матрицы, некоторой другой матрицы меньшего ранга. Для начала давайте сообразим все же, что такое ранг. Ну с этим понятием мы уже знакомились, и интуитивно понятно, что матрица задает некоторые отображения, и ранг каким-то образом описывает «сложность» этого отображения. Если более формально, то ранг матрицы — это максимальное количество линейно независимых столбцов или строк матрицы. Можно ввести ранг и по-другому. Можно сказать, что ранг — это максимальный размер подматрицы с ненулевым определителем. Также полезно заметить, что если размер матрицы X — m на n, то ранг матрицы будет точно не больше, чем минимум из этих двух чисел, что в целом понятно из определения. В случае, если мы рассматриваем произведение двух матриц, ну например, матрица A имеет размер m на k, матрица B имеет размер k на n, то их произведение конечно имеет размер m на n. Однако, если число k очень маленькое, ну то есть меньше m и n, то ранг этого произведения тоже не может быть больше, чем k. Итак, зачем нам может быть нужно приближать матрицу матрицей меньшего ранга? А у нас может быть предположение, что матрица, которую мы видим в действительности, не совсем правильная. Ну например, это могут быть какие-нибудь рейтинги фильмов или какие-нибудь признаки. И мы можем думать, что там есть какой-то шум. И нам хочется восстановить более простую матрицу, которая будет достаточно близка к той, которую мы пронаблюдали. Один из способов это делать заключается в том, что мы не просто пытаемся подобрать матрицу меньшего ранга k, а мы представляем ее как произведение двух матриц U и V транспонированное таких размеров, чтобы ранг произведения был не больше, чем k. Но что значит «приблизить»? Что значит, что X будет примерно равно U умножить на V транспонированное? Ну просто нам нужно ввести некоторую норму на матрицах, сказать, что норма разности X − U * V транспонированное должна быть минимальной и подбирать матрицы U и V таким образом. Как ввести норму матрицы? На самом деле есть много способов. Один из них — это норма Фробениуса. Выглядит достаточно интуитивно. Нужно просто взять и просуммировать квадраты всех элементов матрицы и извлечь из этого всего корень. Таким образом, с нормой Фробениуса задача принимает следующий вид: U и V должны просто минимизировать сумму по всем i, j квадратов отклонения значений в матрице xᵢⱼ от значений в произведении матриц. Пример первый: преобразование признаков. Если исходная матрица X — это матрица с признаками некоторых объектов, то новая матрица U содержит столько же строчек, сколько и в ней, и тоже может интерпретироваться как матрица признаков объектов. При этом, если число k у нас намного меньше, чем n (исходное количество признаков), то мы еще и выполняем задачу понижения размерности пространства признаков. Так как по матрице U можно довольно неплохо восстановить матрицу X с помощью матрицы V, то мы выполняем задачу понижения размерности таким образом, что теряем как можно меньше полезной информации. Ну то есть практически все нужное нам сохраняем. Другой пример — задача рекомендаций. Пусть X — это матрица с оценками xᵢⱼ, поставленными пользователем i фильму j. Какие-то значения матрицы нам известны, а какие-то остаются для нас тайной. Возьмем простую модель. Будем приближать xᵢⱼ произведением uᵢ на vⱼ, где uᵢ отражает интересы пользователя, а vⱼ отражает некоторые признаки фильма. Возникает следующая идея: давайте настроим u и v таким образом, чтобы хорошо прогнозировать известные значения xᵢⱼ, а неизвестные значения спрогнозируем как произведение уже настроенных векторов признаков. Тогда задача немного модифицируется. Теперь мы берем сумму не по всем элементам ij, а только по тем, которые нам известны. Подведем итог. Ранг матрицы — это некоторый способ описать «сложность» матрицы. Матричными разложениями называют не только представление матрицы произведением других, но и приближение исходной матрицы некоторым произведениям, как правило, более низкого ранга. В следующем видео мы узнаем, как с этими матричными разложениями связано сингулярное разложение из прошлого видео.

[БЕЗ_ЗВУКА] Теперь давайте узнаем, как сингулярное разложение матриц связано с низкоранговыми приближениями. Итак, если мы приближаем исходную матрицу (матрицу меньшего ранга), то мы просто пытаемся найти некоторую матрицу X с крышкой ранга не больше, чем k, которая будет не сильно отличаться от исходной по какой-то норме. В случае матричного разложения мы рассматриваем произведение матриц U х V транспонированное как матрицу, которую мы приближаем, исходную. Давайте вспомним, что такое SVD, или сингулярное разложение матриц. Сингулярное разложение матриц — это представление исходной матрицы произведением трех других: ортогональной, диагональной и снова ортогональной. Оказывается, что если мы рассматриваем матрицу Xk, которая представляет из себе произведение трех матриц, получаемых из SVD, она будет обладать некоторыми особыми свойствами. Что же это за три матрицы? Давайте из первой матрицы, из SVD, возьмем k столбцов первых, из второй матрицы возьмем квадрат размера k × k, он должен соответствовать самым большим диагональным числам, и из третьей матрицы возьмем k строк. И вот эти вот матрицы и перемножим, получив матрицу Xk. Так что же за удивительное свойство, которым она будет обладать? Оказывается, матрица Xk будет наилучшим приближением исходной матрицы X по норме Фробениуса. Это совершенно нетривиальный на первый взгляд факт, но его можно с успехом использовать в разных прикладных ситуациях. Да, напомню, что норма Фробениуса — это просто корень из суммы квадратов координат матрицы. Соответственно, норма Фробениуса разности матриц — это просто корень из суммы квадратов разностей. Таким образом, SVD позволяет нам сделать наилучшее приближение исходной матрицы по норме Фробениуса. В рекомендательных системах, используя матричные разложения, мы тоже ищем наилучшее приближение. Наверняка это неспроста. Есть такой вариант: давайте возьмем SVD, как-то распределим диагональную матрицу между двумя другими и будем получившийся результат использовать как матричное разложение, например в рекомендательных системах. Первый недостаток такой мысли в том, что SVD не то чтобы очень просто сделать. Второй недостаток заключается в том, что SVD мы делаем для всей матрицы, а некоторые значения нам неизвестны, значит вместо них нам все равно придется подставить какие-то искусственные значения — нули или среднее значение. Однако так сложилось, что очень долго в рекомендациях любое матричное разложение (на две матрицы, не на три) называли SVD. Ну, видимо, из-за вот этого свойства SVD. Однако мы призываем все же говорить «матричное разложение матрицы предпочтения», а не SVD. Итак, как же нам сделать рекомендации? Настал момент, когда мы можем с этим разобраться. Первый вариант будет не очень хорош по качеству, но он будет значительно проще. Мы можем сделать SVD разложение исходной матрицы предпочтений, воспользовавшись какой-нибудь готовой реализацией SVD, и матрицу UkDk использовать как матрицу профилей пользователей, а матрицу Vk — как матрицу профилей фильмов. А произведение профилей будет нашей оценкой для того, насколько понравится фильм пользователю. Ну на самом деле матрицу D мы можем распределять и как-то по-другому. Другой вариант более правильный. Мы можем никак не использовать SVD как метод, а просто подбирать U и V, минимизируя функционал. Обратите внимание: даже в SVD мы можем по-разному распределять диагональную матрицу по двум другим, а если мы просто рассмотрим некоторое матричное разложение, ну то есть пусть некоторая матрица X = матрице A * матрицу B, то домножение матрицы A на некоторую матрицу R (справа) и домножение матрицы B на некоторую матрицу R в минус первой (слева) не приведет к изменению произведения. Таким образом, мы просто найдем еще один способ разложить матрицу X в произведение двух матриц. И если у этих матриц также будут те же хорошие свойства, что и у матриц A и B, то это будет просто еще одним решением задачи разложения исходной матрицы. Таким образом, задача в принципе не имеет однозначного решения в общем случае, и с этим нужно как-то бороться. О том, как с этим быть, мы поговорим еще в курсе про прикладные задачи. Подведем итог. Сингулярное разложение матрицы позволяет построить наилучшее приближение некоторой матрицы матрицей ранга k по норме Фробениуса. Это свойство сингулярного разложения привело к распространению термина «SVD» в рекомендательных системах, хотя там, конечно, речь о двух матрицах, а не о трех. Это же свойство может быть использовано и просто для понижения размерности пространства признаков. На этом мы заканчиваем с вами знакомство с матричными разложениями. Мы с вами узнали, что матричные разложения можно интерпретировать по-разному, что они бывают тоже разные. И даже достаточно плотно познакомились с сингулярным разложением матриц и с его применениями в рекомендательных системах.