[БЕЗ_ЗВУКА] Привет, это снова Евгений! Добро пожаловать на урок по статистике! Мы будем говорить о том, как по конечным выборкам оцениваются законы распределения случайных величин, из которых они взяты. Основной инструмент статистики — это статистики. Пусть у нас есть выборка из случайной величины X объема n. Будем обознать ее за X с верхним индексом n. X1, X2, ..., Xn можно считать независимыми одинаковыми копиями исходной случайной величины X. Поэтому часто говорят, что выборка представляет собой совокупность независимых одинаково распределенных случайных величин. В англоязычной литературе это длинное словосочетание часто заменяется аббревиатурой i.i.d. Так вот статистикой называется любая функция от этой выборки. Вообще говоря, вместо «любая» нужно говорить «измеримая», но мы договорились в этом курсе не упоминать о теории меры. Всё! У нас есть все инструменты для того, чтобы начинать что-то оценивать. Давайте посмотрим, какие статистики используются для оценок по выборкам законов распределения случайных величин различных классов. Если мы имеем дело с дискретной случайной величиной, все довольно просто. У нас есть множество ее значений, и распределение задается функцией вероятности, то есть вероятностями, с которыми дискретная случайная величина принимает все свои значения. Если у нас есть выборка из этой случайной величины, лучшие оценки для вероятностей из функции вероятности — это частоты соответствующих событий на выборке. Спасибо закону больших чисел — нам не нужно придумывать ничего очень сложного. С непрерывными случайными величинами все немного сложнее. Если случайная величина задается с помощью функции распределения, оценить ее можно с помощью эмпирической функции распределения, которая представляет собой среднее значение по всем элементам выборки индикаторов того, что элемент выборки не превосходит аргумента функции x маленькое. Эмпирическая функция распределения достаточно хорошо оценивает теоретическую функцию распределения, особенно если выборка большая. Естественно, чем больше выборка, тем лучше ваша оценка. Вот так выглядит теоретическая функция стандартного нормального распределения — красная линия. Стандартным называется нормальное распределение со средним 0 и дисперсией 1. А синяя ступенчатая линия — это эмпирическая функция распределения, построенная по выборке объема 100. Как вы видите, эти две линии достаточно похожи. А еще непрерывные случайные величины могут задаваться своими плотностями. Плотности — это, если помните, такие функции, что интеграл от них по любому отрезку от a до b равен вероятности попадания случайной величины в этот интервал. Чтобы оценить плотность, разобъем область определения случайной величины на интервалы одинакового размера. Тогда число объектов выборки в каждом интервале будет пропорционально среднему значению плотности на этом интервале. Именно так устроена гистограмма. Перед вами живая гистограмма, составленная из студенток университета Висконсин. Они выставлены по росту, который измерен с точностью до дюйма. Рост написан на листочке у девушки, стоящей в первом ряду. Как видите, в этой выборке больше всего девушек ростом 5 футов 4 дюйма (это примерно 165 см) и гораздо меньше девушек, рост которых намного больше среднего или намного меньше среднего. Это неудивительно, поскольку рост — это типичный пример нормально распределенной случайной величины. Рост определяется большим количеством случайных факторов, которые действуют независимо, и именно такие случайные величины хорошо описываются нормальным распределением. А вот так выглядит гистограмма обычная. Признак, который на ней показан, — это продолжительность жизни крыс на строгой диете в днях. По гистограмме прекрасно видны все особенности распределения данных. Это распределение бимодальное. Основной его пик приходится примерно на 1000 дней. Но есть крысы, которые живут существенно меньше — около 400 дней. Важный аспект работы с гистограммами — это правильный выбор числа интервалов. Если вы возьмете интервалов слишком мало, они будут слишком большие и гистограмма получится грубой. По ней вы не сможете понять, что происходит в данных. То же самое может произойти и в обратном случае. Если вы возьмете слишком много интервалов, в большую часть из них не попадет ни одного объекта выборки, и гистограмма получится разреженной и тоже не очень информативной. Этого недостатка лишены гладкие оценки плотности. Это другой способ оценки плотности распределения. Для их построения используется ядерное сглаживание. Для того чтобы сделать такую оценку, вы должны взять окно ширины h и, двигая это окно по числовой оси, вычислять в нем значение функции, которая называется ядром. Если вы не поняли ни слова из того, что я сказал, не расстраивайтесь. В следующем курсе мы будем очень подробно говорить про ядерное сглаживание. Пока вам нужно только знать, как выглядят ядерные оценки для плотности распределения. Перед вами оценка, построенная на тех же самых данных продолжительности жизни крыс. Все особенности распределения на этой оценке также отражены. Мы видим, что она бимодальная. Соотношение пиков такое же, как и на гистограмме. А вот все виды оценок распределения для нашей выборки из стандартного нормального распределения. На верхнем графике мы видим теоретическую плотность (это красная линия) и гладкую оценку плотности, построенную по выборке объема 100 (это синяя линия), а на нижнем графике — гистограмма. Вообще говоря, гладкие оценки плотности и гистограммы несовершенны. Никакой из этих инструментов не идеален, поэтому я рекомендую вам, когда вы визуализируете данные, использовать и тот и другой способ. Итак, в этом видео мы дали определение статистике и узнали, какие статистики используются для оценок функции вероятности, функции распределения и плотности распределения. Далее вы получите ноутбук, в котором будут функции, используемые для построения статистик, о которых мы говорили в этом видео. А после этого мы поговорим о важных параметрах распределения и о том, как их оценивать по выборкам.

В предыдущем видео мы узнали, как по выборкам оцениваются законы распределения случайных величин, будь то функция вероятности, функция распределения или плотности. Часто интерес представляет не распределение в целом, а какие-то его отдельные важные параметры. В этом видео мы обсудим, какие параметры самые важные, а в следующем узнаем, как именно они оцениваются по выборкам. Первый и наиболее часто используемый класс параметров — это среднее. Понятие среднего — нестрогое, его можно формализовать разными способами. Способ № 1 — математическое ожидание. Для дискретной случайной величины математическое ожидание определяется как сумма всех значений, которые она принимает, взятая с весами, равными вероятностям этих значений. Непрерывная случайная величина принимает несчетное множество значений, поэтому ее математическое ожидание определено как интеграл по всей области определения случайной величины от x, умноженного на f(x) — плотность распределения. Следующий способ формализации понятия среднего — это медиана. Чтобы определить медиану, введем сначала понятие квантиля. Квантилем порядка α называется такая величина Xα, что наша случайная величина X принимает значения слева от нее на числовой оси с вероятностью не меньше α и справа с вероятностью не меньше, чем (1 − α), а медиана — это вовсе не то, к чему вы привыкли, а всего лишь квантиль порядка 0,5, то есть такое значение, что наша случайная величина слева и справа от него попадает с вероятностями, близкими к 1/2. Третий способ формализации понятия среднего — это мода. Модой дискретной случайной величины называется наиболее вероятное ее значение. То есть значение, которое она принимает с наибольшей вероятностью. Для непрерывной случайной величины мы моду так определить не можем, поскольку каждое из значений она принимает с вероятностью 0, поэтому мода — это точка, в которой максимума достигает плотность распределения непрерывной случайной величины. Если мы имеем дело со случайной величиной из нормального распределения с параметрами μ и σ квадрат, то ее математическое ожидание, медиана и мода в точности совпадают и равны μ. Это одно из волшебных свойств нормального распределения. Но так происходит далеко не всегда. Например, если наша случайная величина распределена равномерно на отрезке ab, то ее математическое ожидание и медиана равны (a + b) пополам, то есть середине отрезка, а мода такой случайной величины однозначно не определена, поскольку у плотности, которую вы видите перед собой, нет максимума. То есть модой может быть любая точка на отрезке от a до b. В общем случае математическое ожидание, мода и медиана распределения не обязаны совпадать. Например, если вы имеете дело с распределением с вот такой плотностью, график которой двугорбый, или бимодальный, то ее мода — это точка, определяющая положение наибольшего горба, а математическое ожидание и медиана сдвинуты в сторону второго горба, причем математическое ожидание сдвинуто сильнее, чем медиана. Еще один класс параметров распределений — это параметры, описывающие разброс случайной величины, то есть то, насколько сильно они концентрируются вокруг своего математического ожидания. Наиболее часто используемый из таких параметров — это дисперсия, определяющаяся как математическое ожидание квадрата отклонения случайной величины от своего математического ожидания. Корень из дисперсии часто используется сам по себе и имеет свое название — среднеквадратическое отклонение. Еще один параметр, оценивающий разброс распределения, — это интерквартильный размах. Это всего лишь разность 75-процентного и 25-процентного квантилей, или квартилей, как они еще называются, потому что расположены по четвертям. Если вы имеете дело со случайной величиной, распределенной по закону Пуассона с параметром λ, то дисперсия ее равна λ. Интересно, что ее матожидание тоже равно λ. Это важное свойство распределения Пуассона — его математическое ожидание и дисперсия совпадают. Для нормально распределенной случайной величины с параметрами μ и σ квадрат дисперсия равна σ квадрат. Таким образом, мы теперь знаем значения обоих параметров нормального распределения. Первый отвечает за математическое ожидание, а второй — за дисперсию. Различные параметры разброса не совпадают друг с другом даже для нормального распределения. Чтобы посмотреть, как это работает, давайте на графике плотности нормального распределения отложим интервалы, кратные среднеквадратическому отклонению σ. В интервал (μ ± σ) попадает 68 % вероятностной массы нормального распределения, то есть в этом интервале случайная величина, нормально распределенная, реализуется с вероятностью 68 %. В то время как в интервале, соответствующем интерквартильному размаху вокруг среднего, случайная величина из нормального распределения реализуется с вероятностью 50 %, просто по определению интерквартильного размаха. В интервал (μ ± 2σ) наша нормально распределенная случайная величина попадает с вероятностью 95 %. Это правило называется правилом двух сигм. Другой его вариант — правило трех сигм — говорит, что в интервал (μ ± 3σ) попадает 99,7 % вероятностной массы нормального распределения. То есть нормально распределенная случайная величина лежит на (μ ± 3σ) с вероятностью, довольно-таки близкой к единице. Итак, в этом видео мы разобрались, какие параметры оценивают среднее и разброс распределений, а также мимоходом узнали, как работают правила двух и трех сигм. В следующем видео мы разберемся с тем, как эти параметры распределений оцениваются по выборкам.

[БЕЗ_ЗВУКА] В этом видео мы поговорим о том, как характеристики центрального значения и разброса случайных величин оцениваются по выборке, а также поговорим немного о коварной природе статистик. Начнем с оценки средних. Оценка матожидания случайной величины — это выборочное среднее, то есть буквально среднее арифметическое всех реализовавшихся значений. Довольно просто. Чтобы построить выборочную медиану, которая оценивает истинную медиану, нам нужно проделать несколько более сложную манипуляцию. Возьмем нашу выборку и составим из нее вариационный ряд, то есть отсортируем все ее элементы по неубыванию. i-тый элемент вариационного ряда называется i-той порядковой статистикой. Так вот, выборочная медиана — это центральный элемент вариационного ряда, то есть если мы имеем дело с выборкой, объем которой — нечетное число, то есть n представляется в виде 2k+1, то выборочная медиана — это просто k-тый элемент вариационного ряда. Если длина выборки четная, то для того, чтобы посчитать выборочную медиану, мы берем среднее арифметическое между k-тым и (k+1)-м элементом вариационного ряда. Выборочная мода, в свою очередь, оценивается по максимуму оценки плотности распределения. Как оценивать плотность мы говорили несколько видео назад. Следующий пример взят из книги 1954 года «Как врать с помощью статистики?». В этом примере рассматривается популяция из 25 человек, для каждого из которых известен годовой доход. В выборке есть 10 человек, годовой доход которых равен 2000 долларов, один человек, годовой доход которого равен 3000 долларов, и так далее. И на самом верху этой пирамиды находится человек, годовой доход которого — 45000 долларов. Среднее арифметическое годовых доходов на этой выборке — 5700 долларов, медиана — 3000, мода — 2000. Проблема здесь заключается в том, что все эти величины называются словом «средние», поэтому если вы пишете какой-то неформальный отчет по исследованию, сделанному по этой выборке, вы можете использовать любую из них в зависимости от того, какое впечатление вы хотите произвести. Если вы хотите написать что-то оптимистичное, вам подходит среднее арифметическое, если вы хотите произвести пессимистичное впечатление на вашего читателя, вы берете моду и говорите, что большая часть популяции имеет средний доход 2000 долларов. Посмотрим теперь на оценки разброса распределения. Выборочная дисперсия оценивает дисперсию и представляет собой среднее арифметическое квадратов отклонения от выборочного среднего. Единственный нюанс заключается здесь в том, что в знаменателе перед суммой стоит не n, а n−1. Мы не будем говорить о том, почему так происходит. Для того, чтобы построить выборочную оценку интерквартильного размаха, нам нужно определить выборочный квантиль. Выборочным квантилем порядка α будем называть порядковую статистику, порядок которой равен целой части от α*n. Тогда выборочный интерквартильный размах — это разность соответствующих порядковых статистик. Предыдущий пример про годовой доход вымышленных людей показал, что разные статистики могут характеризовать выборку по-разному. В качестве следующего примера рассмотрим выборку людей, реально существующих. Перед вами распределение годового дохода членов Американской ассоциации юристов. Оно имеет два выраженных пика: один в районе 168 000 долларов, другой — в районе 45 000. Среднее значение, подсчитанное по этой выборке, равно 82 000 долларов. Проблема в том, что это среднее значение распределение абсолютно не характеризует. В выборке крайне мало людей, которые получают сумму, похожую на среднее. Проблема в том, что мы редуцировали всю информацию, содержащуюся в распределении, до единственного числа. И это число недостаточно хорошо характеризует то, что происходит в сырых данных. Еще один известный пример подобного рода — это квартет Энскомба. Это синтетический пример, в котором рассматриваются четыре искусственно сгенерированные пары выборок x и y, характеристики которых в каждом из четырех случаев полностью совпадают. У этих выборок равны выборочные средние и выборочные дисперсии с точностью до двух знаков после запятой. Однако если мы посмотрим на диаграммы рассеяния по этим четырем парам выборок, мы увидим, что в каждом из четырех случаев здесь происходят явно совершенно разные вещи. То есть даже совокупность статистик, которые мы взяли, выборочные средние, выборочные дисперсии, не позволяет нам полностью понять данные. Всегда, когда вы анализируете данные, обязательно смотрите на графики, рисуйте распределение, смотрите на гистограммы оценки плотности, потому что очень легко ввести себя в заблуждение, просто глядя на цифры. Итак, в этом видео мы научились оценивать по выборке меры центральной тенденции и разброса случайных величин, а также рассмотрели на нескольких примерах ситуации, когда статистики не справляются с тем, чтобы отразить всё, что происходит в данных. В следующем видео мы познакомимся с центральной предельной теоремой.

[БЕЗ_ЗВУКА] Это видео про центральную предельную теорему, одну из важнейших теорем статистики. Пусть у нас есть некая случайная величина X с функцией распределения F. И мы имеем ее выборку объема n. Посчитаем по этой выборке выборочное среднее — X с чертой. Будем использовать нижний индекс n, чтобы подчеркнуть, что выборка именно такого объема. Какое распределение имеет эта новая случайная величина — выборочное среднее? И как оно связано с исходным распределением F? Именно на эти вопросы мы и будем сейчас отвечать. Давайте проведем эксперимент. Возьмем случайную величину вот с таким странным, ни на что не похожим распределением. Это распределение абсолютно синтетическое, это смесь двух равномерных и одного треугольного распределения. И будем делать следующим образом. Будем из этой случайной величины брать выборку объема n, считать по этой выборке выборочное среднее, записывать это полученное значение. Повторим этот эксперимент много-много-много раз, например, миллион. И построим гистограмму полученных выборочных средних. Посмотрим, на что она похожа. Перед вами гистограммы, построенные по выборкам объема 2. Мы видим, что по сравнению с исходной плотностью случайной величины эта гистограмма выглядит более гладкой, острые углы на ней начинают постепенно расплываться. И этот процесс продолжается с увеличением выборки. Вот так выглядит гистограмма выборочных средних, построенных по выборкам объема 3. На ней уже практически не осталось острых углов. При объеме выборки 5 гистограмма окончательно становится унимодальной. А дальнейшее увеличение выборки не влияет на форму гистограммы, она становится только более узкой, более сконцентрированной вокруг среднего значения — нуля. Не напоминает ли вам это распределение что-нибудь знакомое? [ЗВУК] Действительно, распределение выборочных средних неплохо можно описать нормальным, именно это и утверждает центральная предельная теорема. Если у вас есть случайная величина X из практически любого распределения и у вас есть выборка объема n из нее, то выборочное среднее, построенное по этой выборке, можно приблизить нормальным распределением со средним значением, которое совпадает с математическим ожиданием исходной случайной величины и с дисперсией, которая равна дисперсии исходной случайной величины, поделенной на n. Чем больше n, тем точнее эта нормальная аппроксимация, тем лучшее распределение описывается нормальным. Интересно, что этот результат справедлив не только для непрерывных распределений, но даже для дискретных. Давайте посмотрим на примере биномиального распределения, как это работает. Перед вами функция вероятности биномиального распределения с параметрами 15 и 0,4. То есть вы как будто проводите 15 независимых испытаний Бернулли, в каждом из которых вероятность успеха равна 0,4. Повторим эксперимент аналогично предыдущему эксперименту со странной случайной величиной. Вот распределение выборочных средних, построенное по выборкам объема 2. Объем 3, объем 5, 10, 30. Происходит ровно все то же самое. Распределение становится все более и более гладким и все более и более похожим на нормальное. Центральная предельная теорема работает. Еще один эксперимент. Возьмем ту же самую биномиальную случайную величину, но с параметром p, равным не 0,4, а 0,01. Функция вероятности этой случайной величины выглядит вот так: высота столбиков в правой части гистограммы здесь настолько маленькая, что на графике ничего не видно, поэтому давайте будем смотреть только на левую часть графика. Вот эта гистограмма. Прекрасно. Давайте повторять эксперимент. Вот так выглядит распределение выборочных средних, построенное по выборкам объема 2, 3, 5, 10, 30. Обратите внимание, что исходное распределение не позволяет распределению выборочных средних быстро сходиться к нормальному. Даже по выборке объема 30, как вы видите, гистограмма выборочных средних не очень хорошо может быть описана нормальным законом. Во-первых, потому что с левой стороны мешает граница. Во-вторых, даже относительно середины, относительно максимума этой гистограммы, распределение не очень симметрично. Центральная предельная теорема хорошо работает, если исходное распределение не слишком скошено. Чем сильнее оно скошено, тем больший объем выборки нужен для того, чтобы достичь хорошего качества нормальной аппроксимации. Есть простое эмпирическое правило: если распределение не очень скошенное, достаточно иметь выборку объема 30, для того чтобы точность нормальной аппроксимации была хорошей. Естественно, это правило очень неточное. Во-первых, не очень понятно, что значит «не слишком скошено», во-вторых, непонятно, что значит «хорошо» в этом контексте. Но это правило, которым можно с той или иной точностью пользоваться на практике. Итак, мы познакомились с центральной предельной теоремой. Центральная предельная теорема — это важнейшая теорема статистики. В следующем видео мы узнаем, как ее можно использовать, и будем с ее помощью строить доверительные интервалы.

[БЕЗ_ЗВУКА] В этом видео мы узнаем, что такое доверительные интервалы и научимся их строить. Представьте, что у вас есть некий продукт, и вы знаете, кто его целевая аудитория. Вы хотите узнать, насколько хорошо ваша целевая аудитория с ним знакома, то есть понять, какая ее доля слышала о продукте. Давайте определим вот такую случайную величину x. Она равна единице, если член целевой аудитории знаком с вашим продуктом, и нулю, если он с ним не знаком. Эта случайная величина имеет распределение Бернулли с параметром p. Вот этот параметр p — это и есть узнаваемость продукта, которую вы хотите каким-то образом померить. Как это можно сделать? Можно провести опрос. Допустим, в вашем опросе n участников. Тогда на выходе вы получаете выборку Xn из нулей и единичек. И оценкой узнаваемости по этой выборке будет выборочное среднее, то есть доля единиц в полученной выборке. Представьте, что вы провели первый вопрос, опросили 10 человек и узнали, что 6 из них с вашим продуктом знакомы. Таким образом, ваша оценка параметра p — это 0,6. Вы провели еще один опрос среди 100 членов вашей целевой аудитории. 44 из них знакомы с вашим продуктом. То есть ваша другая оценка параметра p — 0,44. Какая из этих двух оценок лучше? Интуитивно кажется, что вторая, поскольку там больше данных, значит, она каким-то образом должна быть точнее. Но как это квантифицировать? Как показать, насколько точно ваша оценка измерена? Именно это делается с помощью доверительных интервалов. Доверительным интервалом называется такая пара статистик CL, CU, что вероятность того, что Θ лежит между этих двух статистик, не меньше, чем 1 − α. Θ здесь — это параметр, который вы оцениваете с помощью интервала. (1 − α) называется уровнем доверия, а CL и CU — соответственно нижним и верхним доверительными пределами. Как интерпретируется доверительный интервал? Если представить, что вы повторяете эксперимент по построению интервала бесконечно, то в (1 − α) умножить на 100 % случаев этот интервал будет накрывать истинное значение параметра Θ. Вот так. Давайте теперь вернемся к нашему примеру и построим доверительные интервалы для узнаваемости продукта. Оценки узнаваемости, как мы видели раньше, это, по сути, выборочные средние. А значит, центральная предельная теорема говорит нам, что их распределение может быть с той или иной точностью описано нормальным. Прекрасно. Мы имеем дело с выборкой из распределения Бернулли. Открыв Википедию, мы можем без труда узнать, что математическое ожидание такого распределения равно p, а дисперсия — p умноженное на (1 − p). Подставим эти выражения в правую часть утверждения центральной предельной теоремы. Следующая проблема. Сейчас в правой части стоит параметр p, который нам неизвестен. Что делать? Лучшее, что мы знаем о параметре p, это оценка p с чертой. Просто подставим p с чертой вместо p в правую часть. Теперь распределение, которое стоит в правой части, полностью определено. А дальше нам на помощь приходит правило двух сигм. Если помните, оно утверждает, что 95 % вероятностной массы нормального распределения лежит в интервале от μ − 2ς до μ + 2ς, где ς, напомню, это корень из дисперсии. Правило двух сигм очень хорошо подходит для построения 95-процентных доверительных интервалов. Именно такой интервал мы и хотим построить. Собственно, все. Применив правило двух сигм, мы получаем выражение для доверительного интервала. Оно выглядит вот так. Подставим теперь наши данные в это выражение и получим оценки. Согласно опросу номер 1, узнаваемость составляет 0,6, однако интервальная оценка узнаваемости от 0,29 до 0,91. Эта оценка имеет довольно большую дисперсию, то есть ее точность достаточно низкая. Во втором опросе, где мы опросили в 10 раз больше людей, оценка узнаваемости составила 0,44, а 95-процентный доверительный интервал для узнаваемости — от 0,34 до 0,54. Обратите внимание, как сильно повысилась точность оценки. Доверительный интервал — это прекрасный способ донести степень вашей неуверенности в оценке, которую вы построили, потому что любая оценка, построенная по выборке, по определению неточна. Нужно всегда явным образом утверждать, насколько она неточна, по вашему мнению. Вообще говоря, доверительные интервалы необязательно строить с помощью центральной предельной теоремы. Если вы имеете дело с выборками из каких-то конкретных хорошо известных распределений, как правило, можно найти и более точный способ. Например, для распределения Бернулли, из которого мы брали выборку в нашем примере, наиболее точным является метод построения доверительных интервалов Уилсона. Однако именно центральная предельная теорема дает нам универсальное средство построения доверительных интервалов, которое работает независимо от того, из какого распределения взята исходная выборка. Это здорово. Итак, в этом видео мы узнали, что такое доверительный интервал, и разобрали, как его можно строить с помощью центральной предельной теоремы. Далее вас ждет домашняя работа на применение центральной предельной теоремы, а это последнее видео в этом уроке и одно из последних видео в нашем курсе. Я надеюсь, вам понравилось иметь дело с теорией вероятностей и математической статистикой. Разумеется, это огромные и сложные науки, и то, что мы узнали на протяжении этих двух уроков, это всего лишь первые шаги, но прекрасно то, что уже с использованием этих двух первых шагов, с использованием этого достаточно небогатого теоретического аппарата, который мы освоили, можно делать практически важные вещи. Это одно из прекрасных свойств статистики. Подробней о других статистических методах мы будем говорить в курсе о построении выводов по данным.