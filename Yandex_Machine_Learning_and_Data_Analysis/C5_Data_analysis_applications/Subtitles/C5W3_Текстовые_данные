[БЕЗ_ЗВУКА] Мы начинаем неделю, посвященную работе с текстами. Мы обсудим, в чем особенность таких данных, как извлекать из них признаки, какие бывают постановки задач на текстах, и разберем несколько прикладных кейсов. Текстовые данные встречаются повсюду. По сути интернет — это огромная неструктурированная коллекция, состоящая из текстов в разных форматах. С текстами можно делать много чего. Например, можно пытаться по тексту записи в блоге предсказать, какой рейтинг получит эта запись, сколько лайков ей поставят. И получается, что вы можете оценить успешность записи в блоге еще до того, как опубликуете ее. Или, например, по тексту можно пытаться определить его эмоциональный окрас, то есть понять, он положительный или отрицательный. Это может понадобиться, если вы анализируете отзывы на банк, которые приходят от клиентов. Если вы обнаруживаете отзыв, который отрицательный, то можете быстро предпринять какие-то действия и повысить лояльность этого клиента. Или, скажем, можно пытаться определять тематику научной статьи, которая загружается в электронную библиотеку, и на основе текста этой статьи понять, к какой же предметной области она относится. И либо автоматически отнести ее в эту категорию, либо предложить автору уже готовые варианты выбора. Можно решать и задачи кластеризации. Например, можно пытаться группировать тексты новостей по сюжету. Об одном и том же сюжете пишут разные новостные сайты, и было бы здорово группировать эти новости, если они на самом деле об одном и том же. Также можно пытаться, скажем, искать похожие слова. Допустим, есть слова «кресло» и «стул». Они немножко разные по смыслу, но говорят примерно об одном и том же. Для компьютера это просто набор символов, и вы не можете с помощью простых алгоритмов понять, что эти слова — практически синонимы. Но при этом хотелось бы уметь понимать это как-то на основе данных. О таких задачах мы тоже будем говорить. Есть и более сложные постановки задач на текстах. Например, можно пытаться выделять все имена, которые находятся в тексте. Это называется задачей выделений именованных сущностей. Или например, по большому тексту построить краткую аннотацию, которая покрывает ключевые моменты, обсуждаемые в этом тексте. Еще одна постановка — это вопросно-ответные системы, которые сейчас становятся все более и более популярны. Цель состоит в том, чтобы построить некоторую модель, которая принимает на вход вопрос в произвольной форме и пытается найти на него ответ, причем ответ она строит по какому-то большому неструктурированному корпусу, например, по Википедии. Еще одна задача — это генерация текстов. Было бы очень интересно брать, допустим, собрание сочинений Льва Толстого, и по нему построить некую модель, которая будет генерировать новые тексты, которые похожи на рассказы Толстого. Эту задачу уже сейчас пытаются решать, и мы тоже немножко упомянем, как это можно делать. Итак, нашей основной задачей будет научиться использовать текстовые данные в алгоритмах машинного обучения, в алгоритмах, которые мы уже умеем использовать: в линейных моделях, решающих деревьях, нейросетях. Основная проблема в том, что текст — это по сути некоторый набор данных, набор букв переменной длины. У каждого объекта... каждый объект может описываться разным количеством символов. И при этом хочется как-то преобразовать этот текст, этот набор символов в фиксированное количество признаков. Как это можно делать? Давайте разберем пример. Есть датасет, который называется 20newsgroups, в котором задача состоит в том, чтобы классифицировать тексты по 20 тематикам. Давайте посмотрим вот на этот текст. В нем можно увидеть названия марок автомобилей, и это сразу же наводит нас на мысль о том, что этот текст — про машины. При этом на самом деле порядок слов не важен. Нам важно, что мы увидели некоторые ключевые слова в этом тексте, и даже не важно, где именно в нем они расположены. А вот еще один пример. В этом тексте мы видим некие упоминания политических партий, собраний, и легко догадаться, что этот текст — про политику. Опять же, нам не важна структура этого документа. Важно лишь, какие слова входят в него, и этого нам хватает, чтобы определить его тематику. В этом и заключается подход, который называется «мешок слов». В нем мы не обращаем внимания на порядок слов, и просто кодируем наш текст, как количество вхождений каждого слова из словаря в этот документ. Мы будем подробнее обсуждать этот подход в следующих видео. Оказывается, такой подход очень хорошо себя зарекомендовал и часто используется в задачах машинного обучения. Итак, мы с вами обсудили, какие бывают задачи на текстах, сказали, что главная проблема здесь — это закодировать текст произвольной длины с помощью некоторого фиксированного количества признаков, и немножко обсудили подход, который называется «мешок слов», который не обращает внимания на порядок слов и смотрит лишь на то, какие именно слова входили в документ. В следующем видео мы поговорим о том, как предобрабатывать тексты перед тем, как извлекать из них признаки, в частности, о том, как разбивать тексты на слова и как приводить каждое слово к его начальной форме.

Давайте обсудим, какие преобразования имеет смысл совершать с текстом, перед тем как извлекать из него признаки и строить какие-то модели. В частности, мы поговорим о двух важных этапах обработки текста: о токенизации и нормализации. Суть токенизации в том, чтобы разбить текст, то есть просто длинную строчку, на отдельные слова, с которыми уже можно работать. А нормализация — это приведение каждого отдельного слова к его начальной форме. Зачем это нужно, мы поговорим чуть позже в этом видео. Начнем с токенизации. Как я уже сказал, смысл токенизации в том, чтобы разбить строку, непрерывную строку, на отдельные слова. Давайте разберем это на простом примере. Есть некоторый текст из Википедии, который объясняет, что такое текст. Видно, что он состоит не только из букв, но при этом из каких-то символов, скобок, кавычек, есть некое тире. Кажется логичным удалить все эти символы, удалить все, что не является буквами или цифрами. Благодаря этому, если мы заменим все эти символы на пробелы, то дальше все слова, которые отделены пробелами, можно уже объявить отдельными токенами. Но при этом, как всегда, возникает много нюансов. Например, есть слово «каком-либо». Если мы заменим дефис на пробел, то окажется, что у нас есть два слова: «каком» и «либо». Но на самом деле это одно слово, и было бы правильней не разделять их. Итак, токенизация состоит из нескольких этапов. В первую очередь, как правило, текст приводится к нижнему регистру, но при этом опять же мы можем потерять из-за этого некоторую информацию, например три буквы «О», если они написаны в верхнем регистре, то это, скорее всего, сокращение — «общество с ограниченной ответственностью». Если это три маленькие буквы «о», то, возможно, это просто некое выражение эмоций. И непонятно, как здесь правильно поступать: приводить все к нижнему регистру или нет. Следующий этап — это замена всех знаков препинания на пробелы. И опять же есть много нюансов. Во-первых, как мы уже обсудили, если есть сложные составные слова, то заменять в них дефис на пробел не очень разумно — из-за этого может потеряться смысл этого слова. Также удалять знаки препинания может быть плохо, потому что мы потеряем все смайлики, но они играют очень большую роль. Например, если мы анализируем твиты или записи в соцсетях, смайлы в них очень информативны — по ним можно, например, определить эмоциональный окрас этого текста. После того как мы каким-то образом разделили слова, мы объявляем каждое слово, которое отделено пробелами, отдельным словом. Но при этом опять же здесь есть тонкости, например, названия городов: «Нижний Новгород». Было бы логично рассматривать эти два слова как одно. Или, например, сокращения: «кандидат технических наук» сокращается как «к.т.н.». И если мы объявим эти три буквы отдельными словами, то знание о том, что в тексте есть буква «к», буква «т» и буква «н», совершенно не поможет нам решить задачу. Но если же мы будем рассматривать их вместе, то это вполне полезный термин. Также стоит отметить, что токенизация не всегда устроена так просто. В некоторых языках, например в китайском, пробелы ставить не принято, предложение — это непрерывный набор иероглифов. И чтобы разделить такое предложение на отдельные слова, нужно применять специальные алгоритмы — так называемые алгоритмы сегментации текста, которые по некоторым правилам находят разделители в этом тексте. Следующий этап, после того как мы разбили текст на отдельные слова — это нормализация слов, то есть приведение каждого слова к его начальной форме. Например, слово «машинное» нужно привести к форме «машинный», слово «шел» к форме «идти», к его начальной форме. Обычно это нужно из-за того, что то, какая именно форма у слова, не несет в себе большой роли: «машинная» или «машинный» — и так, и так смысл понятен. Но при этом нередко встречаются задачи на текстах, в которых данных не очень много, и хотелось бы, чтобы признаков было как можно меньше. Поскольку число признаков, как правило, напрямую зависит от количества различных слов, было бы здорово сократить это самое количество различных слов. И приведение слов к нормальной форме хорошо позволяет это сделать. К нормализации есть два основных подхода: это стемминг и лемматизация. Давайте поговорим о них подробнее. Самый простой подход — это стемминг, и его суть состоит в том, что слова «стригутся». По некоторым правилам от каждого слова отрезается его окончание. К сожалению, этот подход не всегда работает — например, некоторые слова, когда меняется форма, меняются целиком, скажем у слова «есть» его прошедшее и будущее время — это «был» и «будет». Понятно, что сколько бы букв с конца вы не отрезали, вы не получите из этих трех слов одно и то же в результате стемминга. Поэтому иногда пользуются более сложным, более грамотным подходом, который называется лемматизацией. Лемматизация работает следующим образом: в его основе лежит словарь, в котором уже записано большое количество слов и их форм, и в первую очередь слово проверяется по словарю. И если оно там есть, то понятно, в какой начальной форме оно приводится. Если слова нет, то по определенному алгоритму выводится способ изменения данного слова, и на основе этого способа уже делаются выводы, какие начальные формы могут быть у этого слова. Этот подход работает лучше, и он подходит для новых, неизвестных слов, но при этом, поскольку он сложнее, он работает гораздо медленнее, чем стемминг. Итак, мы обсудили, что предобработка текстов состоит из токенизации — разбиения текста на слова, и нормализации — приведения слов к начальной форме. При токенизации может быть много нюансов и всегда имеет смысл учитывать особенности задачи, и на основе постановки уже принимать решение: приводить слова к нижнему регистру или нет, удалять знаки препинания или нет и так далее. К нормализации мы обсудили два различных подхода — стемминг и лемматизацию — и сказали, что лемматизация работает лучше, но при этом гораздо медленнее. А в следующем видео мы поговорим о том, как извлекать признаки из предобработанного текста.

[БЕЗ_ЗВУКА] В этом видео мы поговорим об извлечении признаков из предобработанного текста. Как мы уже отмечали, очень неплохо на текстах работает подход, называемый «мешок слов», в котором мы не обращаем внимания на то, в каком порядке идут слова, и рассматриваем текст как лишь неупорядоченный набор каких-то слов. Если говорить более формально, мы извлекаем признаки следующим образом. Допустим, в нашей выборке всего имеется N различных слов, которые мы обозначаем как w1, w2, ..., wN. В этом случае мы кодируем каждый текст с помощью N признаков. При этом j-й признак из этих N показывает долю вхождения слова wj среди всех вхождений слов в данном тексте. Давайте разберем это на примере. Допустим, наша выборка состоит из двух предложений, из двух текстов. Первый — это «текст состоит из слов», второй — «вхождения данного слова среди всех слов». Сразу замечу, что мы выбросим отсюда не очень важные слова, в частности «из» и «среди». Это так называемые стоп-слова, о которых мы поговорим чуть позже. Получаем, что первый текст состоит из трех слов, и в этом случае мы кодируем его вектором признаков, которые показаны в этой таблице. Первый признак, второй и третий будут иметь значение по 0,33. В сумме это как раз единица. Следующий текст состоит из слов «вхождения», «данный», «слово», «все» и еще раз «слово». Всего в нем пять слов, при этом «слово» входит два раза, поэтому значение признака для него будет равно 0,4. Все остальные слова входят по одному разу, поэтому для них значения признаков равны 0,2. При таком подходе имеет смысл обращать внимание на два нюанса. Первой — это стоп-слова, популярные слова, которые встречаются в каждом тексте, например, какие-то предлоги или союзы, и которые не несут в себе никакой информации, а лишь засоряют наши признаки. Такие слова имеет смысл удалять еще на этапе предобработки, например, на этапе токенизации. Также имеет смысл удалять редкие слова. Если у нас есть какое-то слово, которое входит всего в один или в два текста, то, скорее всего, мы просто не сможем значимо учесть его в модели, мы не сможем значимо оценить, какой вклад это слово имеет в целевой переменной. Поэтому такие редкие слова обычно тоже удаляют. Есть чуть более сложный подход к формированию вектора признаков, который называется TF-IDF. Его идея состоит в следующем. Безусловно, как мы это уже делали в предыдущем подходе, если слово часто упоминается в данном тексте и если это не стоп-слово, то, скорее всего, оно важно. Скорее всего, раз его часто используют, оно играет большую роль в этом тексте. Но также есть и вторая тонкость. Если какое-то слово редко упоминается в остальных документах, а в данном документе оно встречается, то оно, скорее всего, тоже важно для данного документа, потому что по этому слову можно отличить данный текст от всех остальных. Если учитывать эти два соображения, то как раз и получится подход, который называется TF-IDF. Идея в следующем. Признак для слова w и текста x вычисляется по следующей формуле. Она состоит из двух частей. Первое слагаемое показывает как раз-таки долю вхождений данного слова w среди всех вхождений слов в данном тексте. Если это слово много раз входит в текст, то оно важно для данного документа, значение признака будет выше. Второй множитель — это как раз то, что называется IDF, inverse document frequency, обратная документная частота. Она вычисляется как отношение общего количества документов l к количеству документов в нашей выборке, в которых слово w встречается хотя бы раз. Если это отношение большое, то есть данное слово редко встречается в других документах, числитель в этой дроби константный, знаменатель чем меньше, тем больше будет дробь. Если слово редко встречается в других документах, этот множитель будет большим, и, по сути, значение признаков у данного слова будет большим. Если же данное слово встречается в каждом документе, то мы получим логарифм единицы — это ноль, и значение признака будет нулевым. Если слово встречается в каждом документе, скорее всего, оно не очень важно при решении задачи. Итак, мы обсудили подход, который называется «мешок слов», и, по сути, поговорили о двух его вариациях: счетчиках слов — это подход, в котором каждое слово кодируется его частотой в документе, и TF-IDG, который также учитывает, как часто это слово встречается в других документах выборки. А также поговорили о том, что имеет смысл удалять стоп-слова и удалять редкие слова из выборки. В следующем видео мы поговорим о некоторых расширениях этого подхода, в частности, об N-граммах и Skip-граммах, которые позволяют учитывать не только слова, но и словосочетания.

В этом видео мы продолжим разговор о том, как извлекать признаки из текстов и обсудим продвинутые методы, основанные на n-граммах и skip-граммах. «Мешок слов», о котором мы говорили раньше, никак не учитывает порядок слов в документе. Он лишь считает, сколько раз какое слово встретилось в тексте. Понятно, что этот подход не очень хорош. Например, если у нас есть фраза «нравится» и фраза «не нравится», у них противоположный смысл, но при этом «мешок слов» никак не поможет поймать нам эти противоположные смыслы. В «мешке слов» мы будем лишь знать, что в тексте есть слово «нравится» и где-то есть частица «не». Понятно, что в этом не очень много информации. Более того, если бы мы учитывали не только слова, но и словосочетания в тексте, то понятно, что наше признаковое пространство было бы более богатым, более обширным. И вы могли бы находить более сложные закономерности, используя те же самые модели, например, как известно, если добавить новые признаки, то линейные модели смогут находить более сложные разделяющие поверхности или функции. Итак, самый простой подход, который позволяет учитывать порядок слов — это n-граммы. По сути, n-грамма — это просто набор из n подряд идущих слов в тексте. Например, если взять предложение «Наборы подряд идущих токенов», то ее униграммы, то есть n-граммы для n = 1, — это наборы, подряд, идущих, токенов — 4 униграммы; биграммы, то есть пары слов — это наборы подряд, подряд идущих и идущих токенов. А триграммы — это наборы подряд идущих и подряд идущих токенов. После того как мы посчитали все нужные нам n-граммы по тексту, мы пользуемся теми же самыми подходами, как и в случае с «мешком слов». Подсчитываем счетчики для этих n-грамм, вычисляем TF-IDF или делаем что-то еще. Понятно, что чем для большего количество n мы будем подсчитывать n-граммы, тем больше различных признаков у нас будет получаться, тем более богатым будет наше признаковое пространство. При этом, разумеется, значение n — это гиперпараметр. Чем больше n мы берем, тем выше шанс переобучиться. Если мы возьмем n, равное максимальной длине текста в нашей выборке, то у нас у каждого текста будет уникальный признак, и мы сможем идеально подогнаться под обучающую выборку. Но понятно, что такой алгоритм будет переобученным. Кстати, n-граммы можно использовать не только на словах. Понятно, что в качестве токенов можно использовать также и отдельные символы в предложениях. И после того как мы найдем такие токены, для них тоже вычислять n-граммы, так называемые буквенные n-граммы. Этот подход имеет много преимуществ. Например, он позволяет учитывать смайлики, встречающиеся в тексте, или же учитывать более-менее известные слова, но в незнакомых формах, но у которых те же самые корни, что и у знакомых слов. Часто буквенные n-граммы используют вместе с n-граммами по словам. Чуть более расширенный подход к подсчету n-грамм — это skip-граммы или, как они называются более точно, k-skip-n-граммы. k-skip-n-граммы — это такие наборы из n токенов, что между соседними токенами расстояние не более чем k. Если взять наш пример, предложение «Наборы подряд идущих токенов», то, как мы помним, биграммы для него были: наборы подряд, подряд идущих и идущих токенов. Если же мы добавляем один skip биграммы, то есть разрешаем расстояние 1 между соседними словами, то также получаем биграммы: наборы идущих, между словами «наборы идущих» расстояние равно как раз одному, между ними вставлено 1 слово, и «подряд токенов». Между словами «подряд» и «токенов» тоже есть одно слово. Еще один подход, который часто используется при вычислении признаков для текстов — это хэширование. Допустим, у нас есть некоторая функция h(x), которая принимает на вход слово и выдает некоторый хэш. Причем таких хэшей у нее на выходе может быть всего 2 в степени n, где n — некоторое небольшое число. Мы можем заменить все слова в тексте на их хэши, то есть перейти от слова x к его хэшу h(x), и дальше использовать уже эти хэши как токены, и для них вычислять счетчики TF-IDF или что-то еще. Этот подход очень простой — переход от слов к их хэшам, но имеет ряд важных преимуществ. Во-первых, он позволяет легко сократить количество признаков, вы можете не задумываться о том, как именно сгруппировать слова, а просто объединить в одно слово те исходные слова, которые имеют одинаковые хэши. Понятно, что этот подход не очень умный, вы могли бы попытаться группировать слова по их смыслу и объединять похожие слова, но подход с хэшированием тоже работает неплохо и не требует никаких усилий. Но самое главное преимущество, что если вы используете хэширование, то вам не нужно запоминать соответствие между исходными словами в тексте и номерами признаков. Вам не нужно помнить, что слово «токен» — это десятый признак в вашем признаковом пространстве. Нужно лишь посчитать хэш, и значение хэша и будет индексом этого слова. Это существенно позволяет упростить хранение моделей. Хранение соответствия между словами и индексами — это довольно массивная информация. Итак, мы обсудили такие подходы к расширению признакового пространства как n-граммы и skip-граммы и поговорили о том, как можно использовать хэширование при вычислении признаков текстов. А в следующем видео поговорим о том, как обучать модели на текстовых данных.

[БЕЗ_ЗВУКА] В этом видео мы обсудим некоторые особенности обучения моделей машинного обучения на текстовых данных. Как вы уже знаете, прежде чем обучать какие-то модели, нужно тексты подготовить, нужно сформировать выборку и вычислить признаки. Для этого, как правило, сначала из текстов удаляются слишком редкие и слишком популярные слова, или стоп-слова, как их еще называют. И после этого вычисляют признаки. Это могут быть n-граммы, skip-граммы, на которых вычисляются счетчики, количество их вхождений или TF-IDF, например. После того как вы найдете таким образом признаковое пространство, как правило, его размерность окажется довольно большой. После вычисления, например, униграмм, вы можете получить тысячи и десятки тысяч признаков. Если вы добавите биграммы и триграммы, количество признаков может достигать миллионов и десятков миллионов, если корпус довольно большой. Понятно, что это очень много. Поэтому можно, например, пробовать делать отбор признаков, находить только те униграммы, биграммы и так далее, которые важны, которые как-то коррелируют с целевой переменной. Или, например, можно пытаться понижать размерность, например, применить метод главных компонент и найти некоторое подпространство, в которое вы можете спроецировать данные и все еще иметь хорошее признаковое описание. Тем не менее, даже если вы попытаетесь понизить размерность, вряд ли у вас получится радикально ее уменьшить. Все равно признаков будет очень много. Давайте подумаем, насколько хорошо подойдут различные методы машинного обучения в такой ситуации. Если вы попробуете применять случайный лес, то в нем, как вы помните, используются деревья большой глубины. Деревья, которые строятся до тех пор, пока в каждом листе не окажется очень мало объектов. Этот подход не очень хорошо работает, если признаков очень много. Вы будете строить очень глубокие деревья. Это будет занимать очень много времени и из-за этого может оказаться, что вы просто не можете дождаться построения случайного леса достаточного размера. Если вы решите применять градиентный бустинг, там деревья имеют гораздо более маленькую глубину. Но в этом тоже их проблема. Каждое отдельное дерево может учесть лишь очень небольшое подмножество признаков, в то время как зачастую ответ зависит от комбинации большого количества слов в документе. И поэтому, чтобы градиентный бустинг работал хорошо, вам может понадобиться очень и очень много деревьев. Но даже в этом случае не факт, что вы получите приемлемое качество. Байесовские методы, в частности, наивный байесовский классификатор, обычно довольно хорошо показывают себя на текстах. Вы даже уже знаете о примере его применения для задачи фильтрации спама. А чаще всего, на текстовых данных используются линейные модели. Они очень хорошо масштабируются. Они могут работать с огромным количеством признаков и на очень больших выборках. И поэтому часто именно их выбирают, чтобы делать предсказания на выборках с текстовыми данными. Более того, как вы помните, линейные модели позволяют считывать объекты с диска по одному и для каждого отдельного объекта делать градиентный шаг. Это то, что называется стохастическим градиентным спуском. Более того, если вы используете хеширование, такой подход запрограммировать совсем просто. Вы просто читаете все тексты по одному с диска, для каждого текста идете по словам и вычисляете хеши этих слов. После того как хеш слова вычислен, вы знаете, какому признаку он соответствует, и просто именно по этому признаку делаете градиентный шаг. Это очень простая процедура и она очень хорошо и быстро работает на текстовых выборках. Итак, мы обсудили, что признаки для текстов, как правило, вычисляются с помощью n-грамм, после чего на них вычисляются TF-IDF или счетчики. А затем, как правило, используются линейные модели, потому что они довольно простые. И они легко масштабируются как по количеству признаков, так и по количеству объектов.