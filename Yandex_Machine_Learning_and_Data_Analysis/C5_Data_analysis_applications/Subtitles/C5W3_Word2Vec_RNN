[БЕЗ_ЗВУКА] Мы начинаем урок, посвященный продвинутым методам анализа текстов, и начнем его с того, что поговорим о таком подходе как word2vec, который позволяет представить каждое слово с помощью некоторого числового вектора. Но вначале поговорим о другой проблеме. Представьте, что есть два синонима: «идти» и «шагать». Эти слова имеют примерно одинаковый смысл, но для компьютера это просто строки, причем не очень похожие строки. Они имеют разную длину, у них встречаются разные буквы, и компьютер просто по этим двум строкам не сможем сказать, что они имеют одинаковый смысл. Но при этом при анализе текстов, разумеется, хотелось бы оценивать, насколько похожи два тех или иных слова. Как это можно сделать? Разумеется, по данным. Оказывается, что если два слова имеют схожий смысл, то они часто встречаются вместе с одними и теми же словами. Или, как говорят в науке, у них одинаковые контексты. Используя эту информацию, информацию о том, что у похожих по смыслу слов похожие контексты, можно как раз разработать метод, который называется word2vec. Итак, мы хотим описать каждое слово w с помощью числового вектора w с черточкой. Этот числовой вектор имеет размерность d, то есть состоит из d компонент. При этом мы хотели бы, чтобы d было не очень большим, мы хотим иметь компактные представления слов. Далее мы хотели бы, чтобы похожие слова имели близкие векторы, например, по евклидовой или косинусной метрике. Наконец, если мы каждое слово описываем числовым вектором, то эти векторы можно расскладывать, вычитать, умножать на коэффициент. И хотелось бы, чтобы эти арифметические операции имели некоторый смысл. И как мы видим, у них действительно будет очень хороший физический смысл. Итак, мы пытаемся найти векторные представления слов w с черточкой и при этом будем работать с вероятностями p от wi-тое при условии wj-тое. Эта функция показывает, насколько вероятно встретить слово wi-тое в контексте слова wj-тое, то есть рядом со словом wj-тое. Мы будем настраивать, мы будем искать векторные представления слов w с черточкой такими, чтобы вероятности встретить слова, которые находятся в одном контексте, были высокими. В функционале мы идем по всем словам в нашем тексте, берем k слов до него и k слов после него, то есть слова, которые идут рядом. И максимизируем вероятности встретить слова из контекста рядом с данным словом. Этот функционал можно оптимизировать обычным стохастическим градиентным спуском. Если мы обучим его на довольно большом корпусе текстов, то окажется, что векторы действительно имеют много интересных свойств. В частности, если слова имеют похожие смыслы, то их векторы близки по косинусной метрике. Далее, что касается арифметических операций. Если мы возьмем слово, например, «король», вычтем из него слово «мужчина», добавим слово «женщина», то мы получим вектор, который очень близок к вектору слова «королева». Далее, если мы возьмем слово «Москва», вектор слова «Москва», вычтем из него вектор слова «Россия» и прибавим вектор слова «Англия», то опять же получим вектор, который очень близок к вектору слова «Лондон». Более того, оказывается, что этот подход можно использовать и для перевода слов. Если обучить векторы одновременно на корпусе английских и испанских текстов, то если мы возьмем вектор от слова one, вычтем из него вектор слова uno и прибавим вектор слова four, то мы получим вектор, который близок к вектору quarto — «четыре» на испанском. Конечно же, эти подходы проигрывают тем подходам, которые специально разработаны для машинного перевода, но тем не менее с их помощью удается получать довольно интересные результаты. Наконец, поскольку мы научились использовать представления только для отдельных слов, мы можем попытаться перейти к представлениям для текстов. Для этого мы просто берем текст, векторы всех слов, которые в него входят, и усредняем их, после этого мы получаем признаковые описания уже целого текста, и с ним тоже можно работать. Как мы уже обсуждали, проблема мешка слов в том, что он дает очень большие, очень обширные признаковые описания, в которых могут быть десятки и сотни тысяч признаков. Если же мы воспользуемся подходом word2vec, в котором, например, размер вектора равен 100, то мы получим признаковые описания текста, размер которого тоже равен 100. И уже на этих признаках можно обучать любые методы машинного обучения, например, случайные леса или градиентный бустинг, а не только линейные модели. Таким образом, используя данный подход, можно существенно сжимать представления текстов и обучать сложные методы машинного обучения. Итак, мы выяснили, что такое word2vec, что это подход, который позволяет описать каждое слово числовым вектором, и при этом эти векторы будут иметь много интересных свойств. Например, похожие слова будут иметь очень близкие векторы. И мы обсудили, как использовать word2vec, чтобы вычислять признаки для текстов. Для этого мы просто усредняем векторы всех слов, которые в данном тексте имеются.

[БЕЗ_ЗВУКА] В этом видео мы обсудим рекуррентные сети — еще один способ работы с текстами. Мы уже изучили два подхода к анализу текстов. Первый основан на мешке слов, когда мы каждое слово считаем отдельным признаком, и при этом они независимы. Мы никак не учитываем порядок, мы считаем, что текст генерируется следующим образом: у нас есть некоторое, например, распределение на словах, и при этом мы выбираем слово случайно из этого распределения, и добавляем в текст. Могут быть более сложные порождающие процессы, но идея сохраняется — порядок слов никак не учитывается при таком анализе. Мы можем переходить к n-граммам, к skip-граммам, которые хоть немножко учитывают связи между соседними словами, но при этом все равно общий порядок никак не учитывается. Второй подход — это word2vec, обучение о представлении слов. В нем, когда мы обучаем представление отдельного слова, мы учитываем контекст этого слова, то есть те слова, с которыми оно часто идет рядом. То есть уже некоторый порядок появляется. Но при этом, когда мы word2vec применяем, когда мы вычисляем представления для каждого отдельного слова в новом тексте и потом как-то агрегируем их, чтобы получить признаковое описание всего текста, например, усредняем, мы опять никак не учитываем порядок слов. При усреднении мы никак не используем то, что слова шли одно за другим. Возникает вопрос: а есть ли какой-то подход, который действительно учитывает текст именно в таком виде, в каком мы его анализируем, в каком мы его читаем? Есть ли способы, которые учитывают порядок слов? Ответ положительный, и один из подходов к такому анализу — это рекуррентные сети. Это нейронная сеть, которая устроена немножко необычным образом. Она получает слова на вход последовательно. При этом у нее есть некоторое скрытое состояние h, некоторый скрытый слой, который обновляется после прихода каждого нового слова или токена какого-то, например, буквы. Когда приходит новый токен x, мы обновляем скрытое состояние h, при этом при обновлении учитывается как предыдущее значение этого скрытого состояния, так и вход x. После того как скрытое состояние обновлено, мы на его основе генерируем выход y для данного токена. Процесс повторяется до тех пор, пока мы не подадим на вход все токены из нашего текста. Если говорить более строго, то данная сеть описывается двумя формулами. Первая формула — это обновление скрытого состояния. Оно обновляется путем своего рода усреднения предыдущего значения и нового входа. Предыдущее значение скрытого состояния h t – 1 в предыдущий момент времени умножается на матрицу Whh, вход xt в данный момент времени умножается на матрицу Wxh, они складываются, и применяется некоторая нелинейная функция активации f. После этого, чтобы получить выход, мы умножаем скрытое состояние ht, уже обновленное на данном шаге, на некоторую матрицу Why и снова применяем некоторую функцию активации. В результате мы получаем очередной выход. Такое представление очень понятное, но при этом не очень удобное. Можно отобразить нейронную сеть в более привычном для нас виде — если мы ее развернем во времени. Тогда ее структура будет выглядеть вот так. Это будет обычная сеть прямого распространения, длина которой зависит от количества токенов, которые мы подаем на вход, и при этом параметры матрицы Wxh, Whh, Why — общие. Они не меняются от шага к шагу. Меняются лишь скрытое состояние ht, входы xt и выходы yt. Этот вид нужен в первую очередь для того, чтобы обучать данную сеть. Обучать ее можно также с помощью обратного распространения ошибки. При этом оно становится несколько более сложным, поскольку у нас общие параметры между разными итерациями, между разными моментами времени, и это нужно учитывать при дифференцировании. При этом возникают тоже некоторые особенности при обучении, связанные с тем, что мы распространяем градиенты очень далеко. Если у нас нейронная сеть глубокая, если у нас много токенов в тексте, при этом может возникнуть затухание или, наоборот, взрыв градиентов, но сейчас мы не будем говорить об этих проблемах, будем считать, что они уже решены в готовых пакетах для обучения нейросетей. Для чего можно это использовать? Примеров очень много. Поговорим о двух из них. Наверное, самый популярный пример использования рекуррентов нейронных сетей — это порождение текстов. Если на вход рекуррентной сети подается слово, а выход — это вектор вероятностей, в котором элементов столько, сколько у нас токенов в словаре, например, это может быть количество слов в словаре или количество символов, если мы на символьном уровне пытаемся генерировать тексты. И таким образом, когда мы подаем на вход какое-то слово, на выходе мы получаем распределение на словах или на токенах. И если выбрать токен с наибольшей, например, вероятностью, мы сгенерируем следующий токен. После этого его можно подать на вход, сгенерировать еще один токен и так далее. Таким образом, получится генерация текста. Если обучить такую нейросеть на некотором корпусе, то мы сможем генерировать похожие тексты. Вот простой пример. Допустим, мы обучим нашу нейросеть на всех статьях из «Википедии», причем обучать мы будем на разметке этих текстов. Если мы сгенерируем новый текст для «Википедии», мы получим что-то похожее на это. Обратите внимание, что при этом корректно воспроизводится вся разметка, правильно выставляются все ссылки с квадратными скобками, генерируются какие-то ссылки на веб-страницы и так далее. То есть нейросеть очень хорошо улавливает структуру этого текста и элементы, которые там встречаются. Еще один пример использования рекуррентных сетей — это, разумеется, генерация признаков для обучения с учителем. Есть несколько подходов к тому, как извлекать из таких сетей признаки. Подход первый — это прогнать весь текст через рекуррентную сеть и взять из нее состояние скрытого слоя, скрытый вектор, после того как весь текст уже пройден. Поскольку скрытое состояние запоминает в себе информацию обо всех словах в прошлом, если грамотно обучены матрицы переходов, то мы получим некоторое представление, которое учитывает весь текст. Можно поступить и по-другому. Можно извлекать скрытый вектор после каждой итерации, после подачи каждого нового токена на вход рекуррентной сети, и после этого как-то сагрегировать все эти векторы, например, усреднить или применить какую-то более сложную операцию. Итак, мы обсудили рекуррентные нейронные сети — еще один подход к обработке текстов, который учитывает порядок слов, который имеет место в тексте. Основная особенность рекуррентных сетей — это вектор скрытого состояния внутри этой сети, который обновляется после каждого нового слова, и если он обновляется правильно, то он может сохранять информацию о словах, которые были очень давно, он может хранить в себе информацию обо всем тексте, который мы уже увидели. Мы обсудили несколько примеров, а именно генерацию текстов и обучение с учителем с помощью признаков, которые генерируются данной сетью.