[БЕЗ _ЗВУКА] В этом видео мы поговорим о выделении коллокаций. Ну, во-первых, о том, что такое коллокации, зачем их выделять, о том, как их можно выделять с помощью взаимной информации, о том, как можно комбинировать это с другими подходами, о том, какие еще существуют статистические подходы, и закончим всё некоторой простой эвристикой. Итак, коллокация — это, просто говоря, устойчивое словосочетание. При этом это могут быть как достаточно обычные для речи вещи, ну, например, фразы «ставить условие», «назначить встречу», так и какие-нибудь названия, например, крейсер «Аврора». И возникает вопрос — зачем же нам выделять коллокации? Коллокации интересно выделять, во-первых, в случае, если вы хотите сконструировать какие-то более качественные признаки, хотя здесь ваши ожидания могут быть слегка завышены. Дело в том, что использование коллокаций вместо обычных биграмм не то чтобы очень часто дает существенный прирост качества. Ну а во-вторых, можно использовать для визуализации текстовых данных. Это уже куда более осмысленное применение. Дело в том, что иногда нам хочется посмотреть на тексты и понять, о чем же они. И один из способов это сделать — это посмотреть на характерные словосочетания из этих текстов. Как вариант, это можно использовать в тематическом моделировании или же в кластеризации текстов, для того чтобы визуализировать кластеры. Познакомимся для этого с таким понятием, как взаимная информация. Взаимная информация — это некоторая мера того, насколько часто событие происходит вместе. Действительно, давайте рассмотрим вероятность пронаблюдать два события x и y вместе и рассмотрим вероятность пронаблюдать их по отдельности — p(x) и p(y). Если эти события возникают независимо, то вероятность p(x, y) будет просто произведением p(x) * p(y). Если же события возникают не независимо, и, наоборот, даже очень часто эти события происходят вместе, то отношение p (x, y) / p(x) * p(y) будет больше единицы. Ну и, соответственно, логарифм от этого отношения будет больше нуля. Если PMI принимает достаточно большое значение, то эти события часто происходят вместе. Ну какие в нашем случае могут быть события? Например, ситуация, когда мы встречаем в одной биграмме какие-то два конкретных слова. Как же можно выделить коллокации в этом случае? Можно посчитать PMI для встречной пары слов и проверить, что PMI получился больше некоторого порога. Откуда мы возьмем порог? Дело в том, что для разных датасетов порог будет получаться специфичный. Поэтому самый разумный способ — это просто посмотреть на PMI для разных словосочетаний и подобрать порог некоторым визуальным способом. Как вариант, можно взять топ слов по PMI и выбрать достаточное количество. С другой стороны, можно это сочетать с частотами, с которыми биграммы встречаются в текстах. Ну, например, можно отобрать биграммы по PMI, который должен быть больше некоторого порога, и взять из них топ по частотам. Или другой вариант. Выбрать топ по PMI, выбрать топ по частотам и пересечь его. Таким образом получаются наиболее удачные словосочетания. С другой стороны, PMI — это не единственный способ на основе какой-то статистики предположить, что пара слов образует некоторое устойчивое словосочетание. Существуют и другие методы. Ну, во-первых, можно просто посмотреть на позицию одного слова в разных текстах, позицию другого слова в разных текстах и посмотреть на разность этих позиций. Если эти слова склонны встречаться вместе часто, то матожидание этой разности будет близко к нулю, а дисперсия будет не очень большой. Таким образом, на основе оценки матожидания и на основе оценки дисперсии можно получить некоторый эвристический способ опять-таки выделять такие устойчивые словосочетания. Кроме того, можно использовать другие статистические методы, например, t-тест, χ²-тест, или же смотреть на отношение правдоподобий. Мы не будем подробно останавливаться на этих различных методах в данном видео, однако мне хочется еще успеть с вами поделиться такой интересной эвристикой. Оказывается, если у вас достаточно большая выборка, в ней достаточно много раз встречаются одни и те же слова, сильно переусложнять решение задачи выделения коллокаций не стоит. Дело в том, что может быть достаточно просто выбрать наиболее часто встречающиеся биграммы. Или же другой вариант. Выбрать биграммы, которые имеют наибольшую документную частоту, то есть большое количество документов, в которых встретились эти биграммы. Стоит понимать, что в этом случае мы выделяем просто часто употребимые биграммы. Мы не выделяем такие пары слов, которые употребляются только вместе или почти только вместе. Среди выделенных таким образом биграмм могут оказаться и пары достаточно общеупотребительных слов, которые все-таки часто встречаются вместе. Подведем итог. Мы с вами познакомились с понятием коллокаций, выяснили, как можно выделять коллокации с помощью взаимной информации, упомянули другие критерии, упомянули возможность сочетать разные критерии при выделении коллокаций и обсудили простые эвристики.

[БЕЗ_ЗВУКА] В этом видео мы поговорим про языковые модели. Сначала мы обсудим постановку задач в самом общем виде, совершенно неформальном. Затем рассмотрим частный пример постановки задачи, он нас приведет к N-граммным языковым моделям. И затем мы обсудим возможное применение языковых моделей. Итак, в самом общем виде языковые модели каким-то образом моделируют распределение вероятностей последовательностей слов так, как они идут в тексте. При этом мы можем конкретизировать, каким образом мы хотим это моделировать. Например, мы можем хотеть уметь оценивать вероятность слова при условии известных предыдущих слов. Зная такую вероятность, мы могли бы сказать, насколько ожидаемо, что какое-то конкретное слово будет после заданной последовательности в качестве продолжения текста. В частности, в N-граммных языковых моделях рассматривается вероятность встретить последовательность слов как произведение таких вот вероятностей слова при условии известных предыдущих. Здесь можно предположить, что на самом деле важны только сколько-то предыдущих слов, не все до начала текста. Например, N слов. Тогда мы можем произведение этих вероятностей примерно приблизить произведением вероятностей при условии N предыдущих слов. Такие вероятности не так-то сложно оценить, можно просто воспользоваться частотной оценкой. Рассмотрим количество ситуаций, когда слово было после именно этого набора предыдущих слов, и рассмотрим количество ситуаций, в которых был именно такой набор предыдущих слов. Ну и частное даст нам оценку этой условной вероятности. Обычно используются биграммные языковые модели или триграммные языковые модели, то есть N = 2 или N = 3. Дело в том, что языковые модели, основанные на N-граммах, имеют ряд недостатков. Прежде всего, для больших N действительно мало наблюдений. У нас слишком мало статистики, чтобы получить хорошие оценки вероятностей и хорошую модель как следствие. С другой стороны, малые N приводят к тому, что мы учитываем слишком мало слов, Слова, связанные по смыслу, бывают достаточно далеко в тексте друг от друга. Таким образом, если мы попробуем использовать языковую модель для N = 2, скорее всего, она не будет в полной мере отражать то, как строятся осмысленные тексты, даже не будет это делать достаточно похоже на правду. Ну и, кроме того, для тех слов, для которых мало статистики, такая ситуация может быть не со всеми словами, не со всеми наборами слов, идущих подряд, но для каких-то может быть такая проблема, статистики может быть мало. Вот для них получатся плохие оценки вероятностей. Но эта проблема частично решается сглаживанием, которое мы в частности обсуждали, когда говорили про байесовский классификатор. Языковые модели можно использовать для того, чтобы оценивать вероятность какого-то текста, то есть то, насколько мы ожидаем, что такой текст, в принципе, может возникнуть. Можно использовать для того, чтобы генерировать тексты. В самом деле, если у нас уже сгенерирован какой-то набор слов, то следующее слово мы просто генерируем из распределения, которое оценивается в N-граммных языковых моделях. Например, сейчас вы видите пример такого сгенерированного автоматически текста. Он действительно может быть очень похож на настоящий текст местами, но в то же время часто получается совершенно бессмысленным. Итак, мы с вами поговорили о неформальной постановке задачи языкового моделирования, узнали, что, в частности, для построения языковых моделей могут использоваться N-граммные модели, и обсудили возможное применение языковых моделей.

[БЕЗ_ЗВУКА] В этом видео мы поговорим про анализ тональности текста, или сентимент-анализ. Сначала мы рассмотрим примеры, обсудим трудности, которые возникают у этой задачи, и возможные варианты применения сентимент-анализа. После этого мы сформулируем некоторую простую постановку задачи и придумаем простой способ решения. После чего обсудим некоторые практические хитрости и особенности этой задачи. После чего поговорим о возможных других постановках задачи и о том, где взять данные. Итак, примеры. Ну, прежде всего, задача достаточно сложная. Очень часто можно столкнуться с довольно субъективными суждениями, можно столкнуться со смесью субъективных суждений и фактов. Можно столкнуться с тем, что отзыв в целом позитивный, но в какой-то момент происходит какой-то очень резкий поворот, ну например: «Я вчера купил телефон, все было хорошо, отличная батарея, отличный звук, но он сегодня перестал работать». Понятно, что отзыв, скорее, негативный, но в то же время по словам это не чувствуется до последнего предложения, и нужно уметь работать с такими ситуациями. Сентимент-анализ можно применять для анализа различных отзывов на товары, ну с точки зрения пользователя. Можно применять для анализа отзывов с точки зрения компаний, которые эти товары создают. Можно применять в политических целях для анализа мнений избирателей. И можно применять для совсем экзотичных вещей, ну например, для торгов на бирже. На эту тему тоже были некоторые публикации. Но при этом возникает ряд трудностей. С одной стороны, тексты от пользователей отличаются от текстов, которые можно увидеть в литературной речи. С другой стороны, люди используют разный набор слов в зависимости от того, в какой местности они живут, в зависимости от возраста и от многих других факторов. Поэтому если в вашей выборке есть смесь отзывов от разных людей, определенно выборка может быть несколько смещена в зависимости от того, где она собиралась и кто эти люди. Кроме того, слова могут иметь разную эмоциональную окраску в зависимости от контекста и в зависимости от предмета, который они описывают. Ну и конечно же, не стоит забывать о том, что людям свойственен сарказм. Итак, как же простым образом поставить задачу? Ну давайте просто считать, что у нас задача бинарной классификации. Бывают позитивные отзывы, бывают негативные отзывы, и нам нужно это уметь определить по тексту. Ну а в качестве признаков воспользуемся обычным мешком слов, вот и все. Тут уже сразу понятно, как решать эту задачу. Возьмем признаки из мешка слов, ну, может быть, какие-то слова отберем — те, которые наиболее редко встречаются, или стоп-слова. И после этого воспользуемся некоторым классификатором. Ну, скорее всего, нам подойдет какой-нибудь линейный классификатор, ведь мы работаем с текстами, матрица признаков будет разреженной. Ну например, логрегрессия по умолчанию с L2-регуляризатором. Но если мы захотим сделать поменьше используемых слов, например, если у нас есть чувство, что есть много слов, которые не представляют ценности для решения задачи, но почему-то учитываются моделью, то можно взять L1-регуляризатор и сделать константу регуляризации чуть побольше, чтобы отобрать лишнее. Теперь хочется рассказать немножко о практических особенностях задачи. Ну прежде всего, как и в других задачах анализа текстов, вы можете возлагать большие надежды на лемматизацию и стеммирование. Действительно, в этом случае разные по факту токены, разные признаки вашей модели будут схлопываться, если они означают примерно одно и то же. Ну то есть вы не будете страдать от того, что у слов бывают разные окончания. С другой стороны, практика показывает, что помогает это не очень сильно, и очень часто качество может даже чуточку упасть. Это может быть связано с тем, что словоформы как-то все же коррелируют с тем, какой эмоциональный окрас у текста, но на этот счет можно долго рассуждать, а факт остается фактом — на практике это не всегда полезно. Другой важный момент — это то, что слова могут встречаться с отрицаниями. И если обрабатывать эту ситуацию в модели мешка слов, вы не сможете понять, к какому слову относилось отрицание. Поэтому применяется следующий трюк: если у вас встречается частичка «не» рядом с каком-то словом, будем считать, что в большинстве случаев «не» относится именно к этому слову, хотя, конечно, так может быть не всегда. И давайте частичку «не» объединим со словом. Это будет единый токен. Ну другой интересный момент заключается в том, что люди используют разговорный язык, используют сленг и еще и допускают много ошибок. С этой ситуацией можно частично бороться, оперируя не словами, а последовательностями из букв, то есть переходя к буквенным n-граммам для достаточно большого n, ну то есть порядка четырех, пяти, шести, семи. В таком случае вы будете выхватывать какие-то смысловые части слов, но за счет того, что вы идете по буквам, окошкам ширины n, у вас будут куски слова, которые не содержат какую-то ошибку. И, может быть, таким образом получится лучше работать с отзывами, написанными людьми. Кроме того, в этой задаче очень важна пунктуация. Ну в самом деле, если человек ставит много восклицательных знаков, наверное, здесь есть какой-то сильный эмоциональный окрас. Или если человек ставит много смайликов. Да, смайлики тоже важны. И, наконец, хочется поговорить о том, какие еще могут быть постановки задачи, кроме самой простой. Ну, во-первых, можно сформулировать задачу мультиклассовой классификации и сказать, что у нас есть не только позитивный класс, негативный класс, но еще и нейтральный класс. Действительно, значительная часть отзывов не имеют яркой эмоциональной окраски, куда же их относить? Кажется, что нейтральный класс — это отличное решение, но здесь есть некоторый подвох. Дело в том, что спутать негативный пример с позитивным определенно хуже, чем спутать негативный с нейтральным. Но если мы просто решаем задачу мультиклассовой классификации, мы можем этого не учитывать. Поэтому в идеале здесь нужно еще и назначать разные веса разным ошибкам классификации. Другая идея — оценивать не принадлежность к какому-то классу, а собственно оценку, которую пользователь поставил бы товару или какому-то другому объекту, на который пишется отзыв. В этом случае, конечно, разумнее решать задачу регрессии. Ну точно не стоит классифицировать отзыв на пять классов, где каждый класс — это какая-то оценка. Понятно, что «пятерка» отличается от «четверки» не очень сильно, а от «тройки» — уже сильнее, от «двойки» — совсем сильно. И если мы будем решать задачу регрессии, это будет учитываться. Это плюс такого подхода. Минус заключается в том, что большинство методов, которые решают задачу регрессии, используют некоторые стандартные функции ошибок, то есть, скорее всего, будет минимизироваться RMSE, или абсолютное отклонение, а это уже, может быть, не совсем то, что вам нужно. Ну, прежде всего, если вы вместо «единицы» спрогнозируете «двойку», это будет такой же ошибкой, как если вы вместо «тройки» спрогнозируете «четверку». Хотя на самом деле, случай уже более-менее пограничный, если о пятибалльной системе оценки. Кроме того, даже если вы каким-то образом решили эту проблему и у вас, допустим, только три оценки, каждая из которых как раз имеет смысл — позитивный, негативный, нейтральный, но в то же время вы решаете задачу регрессии, поэтому они каким-то разумным образом упорядочены, в этом случае вы все равно сталкиваетесь с тем, что RMSE может быть не очень интерпретируем в вашей задаче. Есть и еще один вариант: давайте решать задачу бинарной классификации, но будем оставлять некоторую «серую зону», то есть область, для которой мы не уверены, позитивный класс или негативный. Таким образом, у нас, естественно, возникает нейтральный класс. При этом у нас сохраняется некоторая упорядоченность между классами, ну и в принципе, мы можем варьировать ширину этой «серой зоны», обеспечивая как можно более высокое качество на оставшихся отзывах. То есть мы придерживаемся некоторой разумной стратегии. Давайте мы будем предсказывать точно там, где уверены. А там, где не уверены, предсказывать не будем. Понятно, что таким образом мы можем прийти и к тому, чтобы вообще отказываться что-то предсказывать. Это будет не самый хороший вариант. Но мы можем рассматривать в качестве оценок качества значения метрик для той области, которая не относится к «серой зоне» и размер «серой зоны». И таким образом пытаться улучшать наше решение, уменьшая «серую зону» и повышая качество на оставшемся. Конечно, сами по себе эти вещи немного противоречат друг другу. Вы можете сделать «серую зону» поменьше, а качество в этом случае просядет. Но если вы дорабатываете ваше решение, конечно, хочется, чтобы и с тем, и с тем было хорошо. Кроме того, сентимент-анализ можно решать на разном уровне. С одной стороны, вы можете пытаться дать ответ для документа, ну то есть для отзыва — позитивный он или негативный. Но в этом случае может быть не очень понятно, насколько ваш ответ применим. Ну в самом деле, если в отзыве три раза поругали что-то, два раза похвалили, но похвалили, может быть, более активно, чем поругали, непонятно, какой же отзыв в итоге. Поэтому можно идти глубже, можно оценивать тональность не всего отзыва, а отдельных его частей, например, предложений. Это удобно и с точки зрения визуализации. Если вы просто говорите пользователю, что в отзыве содержится 66 % негативного класса, это, как правило, ни о чем не говорит. А если вы раскрашиваете отзыв в соответствии с тем, какие предложения, скорее, негативные, какие, скорее, — позитивные, это уже выглядит более похожим на правду. Ну и отдельная тема — это аспектный сентимент-анализ. В самом деле, нам может хотеться понять, отозвался ли пользователь хорошо о каких-то отдельных аспектах объекта. Например, в примере с телефоном мы знаем, что пользователь мог похвалить аккумулятор, мог похвалить звук, мог поругать дисплей, и вот эти вот отдельные части отзыва можно выделять и пытаться понять, позитивные они или негативные. Конечно, аспектный сентимент-анализ — существенно более сложная задача, чем сентимент-анализ документа в целом или каких-то его частей вроде предложений, потому что в этом случае нужно еще и выделять сами аспекты. Ну и наконец, последний вопрос, который нам осталось обсудить, — это где взять данные? На чем-то же нам нужно обучаться. Ну, прежде всего, можно просто взять готовые датасеты. Но здесь есть некоторая проблема. Дело в том, что задача сентимент-анализа очень чувствительна к тому, насколько ваши тексты похожи на то, что было в обучении. Поэтому, если вы обучитесь на каком-то готовом датасете, скорее всего, на практике это будет слабоприменимо. Отсюда возникает идея, а давайте собирать датасеты под конкретную задачу. Если нужно вам прогнозировать тональность отзыва о фильме, то соберите отзывы о фильмах. Если нужно понимать отзывы о товарах — то о товарах. Благо, существует достаточно много сайтов с такими отзывами и с оценками. Оценки будут разметкой. Однако здесь стоит понимать, что, конечно, не все сайты будут очень рады тому, что вы их будете активно парсить, поэтому это может быть сопряжено с некоторыми сложностями. Вам, может быть, нужно будет аккуратно это делать, соответственно, это займет больше времени. Но, в принципе, задача решаемая, и таким образом выборки собирают. Итак, подведем итог. Мы с вами поговорили про задачу сентимент-анализа, про примеры, трудности, возможные применения, про возможные постановки задачи и про некоторые практические особенности. Ну и наконец, обсудили, где же взять данные для решения этой задачи.

[БЕЗ_ЗВУКА] Итак, мы с вами поговорили про разные постановки задачи в сентимент-анализе. Ну, теперь давайте что-нибудь поделаем. Для начала возьмем выборку отзывов на фильмы из NLTK. К счастью, она есть уже готовая. Вот здесь мы получаем ID-шники негативных и позитивных отзывов и заодно печатаем несколько из них, чтобы посмотреть, как это все выглядит. После этого можем подготовить обучающую выборку, а именно достать тексты негативных отзывов, тексты позитивных отзывов. Здесь мы получаем слова из отзыва, но слова получаем в листе. Поэтому, так как дальше CountVectorizer из sklearn будет работать с текстами все же, мы их снова джойним через пробелы. Здесь мы получаем список текстов и список ответов. 0 будем использовать для негативного класса, 1 — для позитивного. Ну и вот мы можем посмотреть на один из текстов. Действительно это какой-то отзыв, какой-то огромный отзыв, то есть определенно человек не пожалел своего времени, чтобы его написать. Ну, похоже все верно. Далее мы импортируем нужные модули, в первую очередь это модули для построения признаков на текстах, это разные классификаторы, линейные классификаторы, в частности линейный svm. Это модуль cross_validation для того, чтобы мы могли оценить качество, и модуль Pipeline, так будет несколько удобнее. Дальше мы пишем свою простенькую функцию, которая фактически возвращает Pipeline для текстовой классификации. В принципе, можно было бы ее и не писать, везде писать вот такую конструкцию, но в какой-то степени так будет удобнее, короче. Что будет происходить? Сначала будет применяться Vectorizer, затем будет применяться Transformer, а затем — Classifier. Ну и давайте посмотрим, как это выглядит. Просто сравним разные классификаторы на этом датасете. Ну вот здесь вот уже есть результаты. Ну давайте на всякий случай перезапустим, сначала у нас используется CountVectorizer, затем TfidfTransformer, то есть мы на основе частот подсчитываем Tfidf-ы. После этого используем классификатор — тот, до которого дошла очередь в цикле. Ну, кстати говоря, нам не обязательно было разносить CountVectorizer и TfidfTransformer. Мы могли бы сразу использовать TfidfVectorizer, он тоже есть в sklearn. Но в таком виде будет несколько удобнее, если мы захотим здесь вместо TfidfTransformer использовать какой-то другой, ну например, как-нибудь понижать размерность. Ну вот мы видим, результаты примерно такие же, при этом победил линейный svm, что в целом было не вполне ожидаемо. Ну и дальше остается только подготовить классификатор, обученный на всех текстах. Это совсем уж просто. Мы просто создаем pipeline из Vectorizer и Classifier. Сразу будем использовать TfidfVectorizer. И дальше остается обучить. Давайте еще раз запустим, посмотрим, что получится. Обучается довольно быстро. Ну и можно как-то его потестировать, например, поиграться с разными отзывами. Ну вот здесь вот были придуманы пара достаточно простых отзывов, они действительно очень ярко позитивные и негативные. Первый — позитивный, второй — негативный. Ну и как вы видите, результаты на них как раз такие, какие следовало ожидать. Но не стоит обольщаться: если получается придумать несколько хороших примеров, это отнюдь не значит, что классификатор хорош. Нужно всегда смотреть на качество. В следующем видео мы попробуем как-нибудь преобразовать признаки и построить на новых, преобразованных признаках другие классификаторы.

[БЕЗ_ЗВУКА] В этом видео мы попробуем доработать наш простой baseline, добавив некоторое преобразование признаков, попробовав встроить ансамбли деревьев. И в конечном счете убедимся в том, что baseline не такой уж и плохой. Итак, начнем. Для начала попробуем применить какие-нибудь матричные разложения. Ну, вот посмотрим, за сколько они работают. NMF, как вы уже видите, строится за десять секунд для десяти компонент, это неотрицательное матричное разложение. А SVD сейчас увидим. SVD работает чуть быстрее, меньше двух секунд. Так или иначе, попробуем посмотреть на качество и SVD, и NMF. Мы, как и ранее, делаем pipeline для текстовой классификации. Сначала получаем частоты слов, затем выполняем преобразования. Раньше это было TF-IDF-преобразование, теперь это SVD и NMF. И опять же берем всего десять компонент и на этом строим линейный SVM. Как вы видите, результаты не очень впечатляют. Они существенно ниже, чем те результаты, которые у нас были ранее. Ну, кстати, вот у NMF немножко получше. И здесь можно предположить, что что-то мы делаем неправильно. Можно попробовать сделать больше компонент. Давайте выставим количество компонент, равным тысяче, и посмотрим на результат. Вот результат уже здесь есть. И как вы помните, примерно такой результат был и у исходного baseline. То есть у нас получилось сделать такое преобразование, которое по крайней мере не испортило наш классификатор. Как правило, если преобразование признаков с помощью матричных разложений действительно позволяет как-то поднять качество, то количество компонент должно быть не таким большим, чтобы качество было совсем такое же, как изначально, и не таким маленьким, чтобы качество уже начало «проседать». Но в то же время, далеко не всегда такое количество компонент есть. Не всегда с помощью матричных разложений можно действительно улучшить качество. И не стоит в этом месте поддаваться иллюзиям, что если сделать что-то «умное» с признаками, то обязательно это поможет и повысит качество. С другой стороны, понизив размерность пространства признаков, мы можем построить более сложные классификаторы. Например, композиции деревьев. Ну, давайте попробуем посмотреть, что получается с RandomForest. Опять-таки мы считаем частоты, дальше делаем SVD-преобразование и затем запускаем обучение RandomForest. И как вы видите, результаты тоже достаточно плачевные. Конечно, это лучше, чем то, что мы видели до этого на десяти компонентах. Но здесь и компонент побольше, и как вы видите, лучше не стало. Можно предположить, что дело в количестве компонент. Надо сделать побольше компонент. Но оказывается, что больше компонент не всегда хорошо. Смотрите, сделали мы 1000 компонент, сделали и деревьев чуть побольше, и качество у нас еще дополнительно просело, то есть так тоже бывает. Ну, можно предположить, что проблема в том, что мы используем частоты слов, а если мы будем использовать, например, TF-IDF, получится более удачный классификатор. Это предположение в какой-то степени оправдывается, но, конечно, это, скорее, шутливый результат. Мы действительно чуть-чуть улучшили качество, но оно по-прежнему совершенно неприемлемое. Можно предположить, если не получается построить хороший классификатор на признаках, которые получаются после SVD-разложения, то, может быть, можно совместить эти признаки с теми признаками, которые были до этого. И уж тогда, по крайней мере, должно получиться не хуже. Ну, давайте посмотрим, что получается из этого. Давайте добавим просто одну компоненту из SVD-разложения. Мы воспользуемся FeatureUnion из sklearn.pipeline, которая позволит нам совместить эти преобразования и получать единое множество признаков. И вот давайте опять оценим качество. И, как вы видите, качество получается тоже не очень хорошее. Уж по крайней мере заметно ниже, чем без добавления одной компоненты из SVD. Таким образом, как вы видите наш baseline был действительно не так уж плох. И просто поделать какие-то логичные, какие-то достаточно сложные, с точки зрения математики, которая стоит за ними, вещи и поднять качество, довольно затруднительно.

[БЕЗ_ЗВУКА] Одна из задач анализа текстов — это задача аннотирования текстов. Давайте поговорим о том, какие постановки задачи в данном случае возможны, затем о том, какие бывают методы решений, и обсудим несколько простых бейзлайнов. Итак, во-первых, вы можете задаться целью сократить текст. Получить текст меньшей длины, который будет передавать основную идею исходного текста. Но это задача довольно сложная: ведь вам нужно еще добиться согласованности предложений, возможно, как-то их переформулировать, поэтому чаще решают более простую задачу, а именно, выделяют наиболее важные для данного текста предложения. Здесь нужно заметить, что понятие важности, конечно, довольно субъективно, и вполне может быть, что разные люди будут выделять разные предложения, если попросить это сделать человека. Ну и другой вариант — это решать задачу аннотирования не для одного документа, а сразу для коллекции документов. В этом случае нужно позаботиться о том, чтобы, с одной стороны, идеи из каждого документа были представлены в итоговой аннотации, а с другой стороны, чтобы не было повторов. Ведь в принципе, предложения могут даже существенно различаться, но по сути доносить одну и ту же мысль. Мы с вами будем разбирать самый простой случай — случай, в котором мы хотим выделить какие-то наиболее важные предложения в тексте. Ну во-первых, можно попробовать применить методы обучения с учителем, но в этом случае нужна будет разметка. То есть нужно будет каким-то образом получить от людей ответы на вопросы «Какие предложения самые важные в данном тексте?», причем для большого количества текстов. В этом месте вы непременно столкнетесь с тем, что люди действительно отвечают по-разному, ну а кроме того, будет проблема с тем, что эта задача действительно сложная для человека, и довольно затруднительно сразу понять, какие предложения самые важные. Очень много вариантов ответа, приходится выбирать, какой вариант лучше, и таким образом разметку вы будете собирать не очень быстро. Можно поддаться соблазну помочь людям, которые делают разметку, и выдавать некоторый ответ. Ну например, ответ некоторого простого алгоритма или случайный ответ. В этом месте нужно быть очень внимательным, потому что люди склонны соглашаться. Если вы попробуете в качестве исходной разметки предлагать разметку от какого-то алгоритма, скорее всего, по такой разметке этот алгоритм будет оказываться самым лучшим по сравнению с другими. Поэтому будьте очень аккуратны, собирая разметку. В качестве объектов в данном случае будут выступать предложения. Для них можно решать задачу классификации: войдет предложение в аннотацию или не войдет. Конечно, можно было бы прогнозировать и какое-то действительное число, которое выражает значимость этого предложения. Но собрать разметку с действительными числами от людей — совсем уж смелая затея, поэтому проще смотреть на это как на задачу классификации, а если нужна некая важность предложений, то можно взять в качестве нее вероятность принадлежности предложения к классу «оставить в аннотации». В качестве признаков подойдут такие логичные и простые вещи, как длина предложения, количество в нем слов с большой буквы, слов, написанных на другом языке, различных терминов, если вы можете их выделять, встречаемость слов из предложения в остальном тексте, ну и на этом всем можно запустить любой адекватный классификатор. Однако очень часто получить разметку крайне затруднительно, поэтому особенно популярны методы обучения без учителя. Они в основном используют идею подсчитать значимость предложений на основе их содержания, по каким-нибудь эвристикам или на основе какой-то простой логичной модели, либо выделить группу предложений, которые развивают какую-то одну идею. Самый простой бейзлайн выглядит следующим образом. Давайте рассмотрим корпус документов, построенный так: пусть каждое предложение из текста будет отдельным документом. И давайте добавим в этот корпус еще и сам документ. Теперь давайте посмотрим на расстояния между документом и отдельными предложениями. И те предложения, которые наиболее похожи на документ, например, можно использовать cosine similarity, вот эти вот самые предложения будут наиболее важными. В этом лежит очень простая идея. Вы выбираете те предложения, в которых распределения слов очень похожи на распределение слов в документе. Таким образом, если у вас есть какое-то предложение, подводящее итог, и использующее слова, которые часто встречаются в документе, которые представляют собой основные объекты, которые описываются в документе, то это предложение, скорее всего, окажется самым важным. Вы можете выбирать для аннотации предложения, для которых cosine similarity получилась больше какого-то порога, можете выбирать просто предложения с наиболее высокой cosine similarity, ну, например, задаться целью выбрать три предложения или пять предложений из текста. Другой вариант — это кластеризовать предложения, опять же, в представлении мешком слов, то есть снова у нас каждое предложение будет описываться частотами слов в нем. Кластеризовать можно чем-нибудь простым, например, k-Means. А в качестве предложений, которые мы оставляем в аннотации, взять предложения, наиболее близкие к центрам кластеров. Также вы можете задуматься о том, что можно преобразовать признаки в предыдущих методах, и попробовать сделать всё то же самое с преобразованными признаками. Ну так действительно делать можно, можно делать какие-то матричные разложения, можно делать какие-то нелинейные преобразования. Более сложные методы аннотирования — это TextRank, основанный на идее из PageRank, алгоритма, придуманного для ранжирования поисковой выдачи, и алгоритмы, основанные на нейросетях. В основном они эксплуатируют идею использования предложений, от которых получается наибольший отклик при решении какой-то еще задачи анализа текста. Ну, например, задачи классификации текстов. Подведем итог. Мы обсудили с вами возможные постановки задачи аннотирования, поговорили о том, какие бывают методы, и обсудили пару простых бейзлайнов в случае методов обучения без учителя. Таким образом, мы познакомились с задачей аннотирования текстов.